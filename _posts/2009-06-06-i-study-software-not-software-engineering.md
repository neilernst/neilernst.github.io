---

date: 2009-06-06 04:45:04+00:00
layout: post
title: I study software, not software engineering
tags:
- nasa
- rup
- software
---

Software research has always been a strange creature. Born in the 60s as computer pioneers realized the immense challenge software posed (where previously most efforts had centered on hardware engineering and design), software covers such a broad scope of domains and possibilities that categorizing it is like categorizing transportation conveyances. Are you building an ocean tanker? A cheap, self-propelled city vehicle? Something to destroy buildings?

That's why most arguments about software tend to feature wagon-circling by members of particular domains. Are we engineers or craftspeople? The guy who builds websites for local veterinarians is appalled by the demands RUP places on his development process. He finds Java and Tomcat ridiculously over-engineered. At the same time, the multinational IT support person can't fathom how anyone would develop without a fully elicited requirements model or class diagram. She looks at Ruby and sees a newfangled bastardization of functional programming and Perl.

These aren't the only camps, either. It often seems to break down over project size, potential harm, and importance. NASA has very few defects in its code, but spends billions of dollars to get to that point. Not something 37 Signals is interested in doing.

This is why I find the term 'software engineering' misleading. Sure, back in the 60s, 70s, 80s, venues like ICSE were largely about giant defence projects, things like Star Wars (see here for a great rant about how futile that effort was, though). Software could be engineered, right? Throw peons at the project, a bunch of money, and with a healthy management team the project would succeed.

Since the dawn of the open-source movement, I would say, and the rise of personal software in the late 80s, research on large-scale software was increasingly less useful to a large number of practitioners. This led to a number of non-academics proposing truly innovative and successful methodologies -- Scrum, FDD, XP, Lean/Kanban -- because, in my view, the researchers just weren't able to see the change coming. Partly this is because as a professor, you need secure funding, and small firms cannot provide this. Inevitably you end up working on Oracle instead of MySQL, for example.

The exciting things is that doing research in software fascinates me precisely because of this diversity. But that research must be targeted appropriately. Don't insist on the primacy of UML for every possible project -- it just doesn't make sense. Don't see statements like "people over process" and [assume that process is irrelevant](http://martinfowler.com/bliki/ComparativeValues.html).

I think in order to more fully embrace this awesome scope, to make research (more) relevant, we ought to stop pretending we all do engineering the way they had hoped -- although [not all of them](http://catenary.wordpress.com/2008/05/14/masterpiece-engineering/) -- at the NATO conference in 1968. We need to more carefully identify the relevance of humans in the software loop. We need to accept the inevitability of [change and inconsistency in software and organizations](http://www.slideshare.net/nashjain/agile-is-the-new-waterfall?src=embed). We need to figure out what's useful for the ten-person company, as well as the thousand person company. We ought to just call ourselves software scientists, and go from there.

Currently, it seems to me that practitioners of all stripes get their information thusly: first, via colleagues or anecdote; secondly, from websites like Slashdot or Digg; thirdly, from vendors and PR agencies; fourthly, from industrial research firms like Gartner or Forrester; and finally, lastly, from academic research papers. I would really like to move academic research up this list.
