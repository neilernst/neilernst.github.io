---

date: 2010-07-23 15:52:57+00:00
layout: post
title: The relevance of CS research
tags:
- academia
- cs research
- quora
---

I came across [a post by Seb Paquet on Quora.com](http://www.quora.com/Do-computer-scientists-pay-enough-attention-to-innovations-taking-place-in-the-commercial-and-non-profit-worlds) about the relevance of CS research(ers). [Seb's](http://twitter.com/sebpaquet) position seems to be that academics are doomed to failure when it comes to innovation. I think he comes up with some good reasons why academia  will struggle to innovate, but he is comparing oranges to pomegranates. I should preface by noting I'm not enamoured with the current situation, but he _is_ unfair – I don't see CS research as being in any danger of being pushed into a corner of irrelevance.

First problem is _who _we are comparing. Most people will point to the successful companies in industry as an example of innovation, such as Apple with the iPad. Then, they will dredge up some aging prof who publishes in the same obscure journals each year as an example of academia and irrelevance. But we should really look at the most successful academics, like [Duncan Watts](http://en.wikipedia.org/wiki/Duncan_Watts) or [Jim Hendler](http://en.wikipedia.org/wiki/James_Hendler). Then, we have a survivor bias in industry –companies which don't innovate get pushed aside (although this is in support of Seb's position). And finally, I'm not convinced industry is all that innovative anyway. I mean, how many Y Combinator companies are doing some form of social media site? Often, what we mean by innovation is a combination of marketing and excellent product engineering (definitely realms to which innovation is useful, but not what we usually mean).

The second issue is _what_ we are innovating. Do we want research labs at university to compete with Google or Apple on product development? Of course not. In fact, if a lab did come up with a killer product, it would probably move straightaway into industry - witness Bumptop, OpenText, or Google itself.

What we want from research labs are the long-term innovations, things like ArpaNet, hypermedia, REST (a Ph.D. thesis that is just now being fully understood). These enable whole new areas for research and industrial innovation. And let's not discount the merits of heading down the wrong path. A lot of academic research consists of proving that something is in fact the case (beyond anecdote) or establishing what _doesn't_ work.

Finally, regarding the original question (about learning from industry), I think in fact academia is highly responsive to industrial innovations (perhaps too much so!). At our department, we have embraced Python, Subversion, Scrum, Wikis, etc. all within a few years of their development. Keep in mind that teaching needs to focus beyond what is immediately useful, unlike a career college. Professors here have worked with IBM to understand DB2 provisioning, used blogging to understand IR, and leveraged Hollywood to create new animation algorithms. There's a new academic conference on "Xtremely Large Databases" to keep up with Google-scale problems. I think the key is that the tools produced are not necessarily the most useable - and it is this last mile that industry is great at.

If there is one thing that concerns me as a researcher it would be access to data. In the earlier days of computing, a researcher could claim to be working on similar problems to industry, because universities sank millions of dollars into computing infrastructure to maintain this parity. Today, though, it seems as though universities are falling behind when it comes to 'real-world' data to work with. Unless you are privileged to work with Google, you will have a very hard time duplicating that scale of problem. I'm not sure what the last multi-million dollar investment by a university in cutting-edge computing infrastructure was, perhaps next generation cloud systems like [SHARCNET](https://www.sharcnet.ca/my/front/).
