[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The PI as COO\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\nThe Scientific Method 2021 edition\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\nOn Bots in Software Engineering\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2021\n\n\nNeil Ernst\n\n\n\n\n\n\n  \n\n\n\n\nREFSQ Panel session on Open Data and RE Education\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2021\n\n\n\n\n\n\n  \n\n\n\n\nThe Triumvirate of Teaching and Work Life Balance\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\nRunning a Mining Challenge Using Kaggle\n\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\nAcademic Job Searches—A Canadian Perspective\n\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2019\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Hierarchical Modeling in Software Engineering\n\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2018\n\n\n\n\n\n\n  \n\n\n\n\nSeven Principles of Effective Documentation\n\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2017\n\n\n\n\n\n\n  \n\n\n\n\nMoving to UVic\n\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2017\n\n\n\n\n\n\n  \n\n\n\n\nVisual Abstract attempt\n\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2017\n\n\n\n\n\n\n  \n\n\n\n\nOn Active Learning in Software Engineering\n\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2017\n\n\n\n\n\n\n  \n\n\n\n\nThoughts on Amy Ko’s “PL as …” keynote\n\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2016\n\n\n\n\n\n\n  \n\n\n\n\nDay Hikes\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2016\n\n\n\n\n\n\n  \n\n\n\n\nColumbus’s Heilmeyer Catechism\n\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2016\n\n\n\n\n\n\n  \n\n\n\n\nOn SCAM’s new “Engineering Track”\n\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2016\n\n\n\n\n\n\n  \n\n\n\n\nOn Using Open Data in Software Engineering\n\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2016\n\n\n\n\n\n\n  \n\n\n\n\nThe Marginal Utility of Testing/Refactoring/Thinking\n\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2016\n\n\n\n\n\n\n  \n\n\n\n\nA Model of Software Quality Checks\n\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2015\n\n\n\n\n\n\n  \n\n\n\n\nRequirements, Agile, and Finding Errors\n\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2015\n\n\n\n\n\n\n  \n\n\n\n\nHow Writing Code is Like Making Steel\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2015\n\n\n\n\n\n\n  \n\n\n\n\nGarbage In, Garbage Out\n\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2015\n\n\n\n\n\n\n  \n\n\n\n\nRunning a “Critical Research Review” at #RE15\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2015\n\n\n\n\n\n\n  \n\n\n\n\nA Field Study of Technical Debt\n\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2015\n\n\n\n\n\n\n  \n\n\n\n\nThoughts from a CodeFest\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2015\n\n\n\n\n\n\n  \n\n\n\n\nFrameworks, libraries, and dependencies\n\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2015\n\n\n\n\n\n\n  \n\n\n\n\nThe Gap Between User Requirements and Software Capabilities as Technical Debt\n\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2015\n\n\n\n\n\n\n  \n\n\n\n\nMeasuring programmer productivity is futile.\n\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2015\n\n\n\n\n\n\n  \n\n\n\n\nCults of Personality and Software Process Improvement\n\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2014\n\n\n\n\n\n\n  \n\n\n\n\nSoftware research shouldn’t be about the tools\n\n\n\n\n\n\n\ncomplexity\n\n\nIT\n\n\nsemantic web\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2014\n\n\n\n\n\n\n  \n\n\n\n\nEvidence in Software Engineering\n\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2013\n\n\n\n\n\n\n  \n\n\n\n\nConfiguring SONAR with Maven on Mac\n\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2013\n\n\n\n\n\n\n  \n\n\n\n\nThe Circle, a novel\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2013\n\n\n\n\n\n\n  \n\n\n\n\n13 Great Software Architecture Papers\n\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2013\n\n\n\n\n\n\n  \n\n\n\n\nVirtual Conferences\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2013\n\n\n\n\n\n\n  \n\n\n\n\nKnowledge and complexity\n\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2013\n\n\n\n\n\n\n  \n\n\n\n\nSome Advice on Doing a PostDoc in Software Engineering\n\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2013\n\n\n\n\n\n\n  \n\n\n\n\nThe fuzzy notion of “business value”\n\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2013\n\n\n\n\n\n\n  \n\n\n\n\nObtaining a Pennsylvania Driver’s Licence with an H1-B\n\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2013\n\n\n\n\n\n\n  \n\n\n\n\nTeaching Advanced Software Engineering\n\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2013\n\n\n\n\n\n\n  \n\n\n\n\nA stitch in time…\n\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2012\n\n\n\n\n\n\n  \n\n\n\n\nUsing GitHub for 3rd Year Software Engineering\n\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2012\n\n\n\n\n\n\n  \n\n\n\n\nWhat I learned at UofT\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2011\n\n\n\n\n\n\n  \n\n\n\n\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n\n\n\n\n\n\n\nmmd3\n\n\nscrivener\n\n\nthesis\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2011\n\n\n\n\n\n\n  \n\n\n\n\nShould we care about evidence-based software engineering?\n\n\n\n\n\n\n\ntest\n\n\nevidence\n\n\nsoftware\n\n\ntheory\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2010\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "students.html",
    "href": "students.html",
    "title": "Students",
    "section": "",
    "text": "One of the best parts of being a prof is working with talented and passionate graduate and undergraduate students. We have a research group page at the Octera Github site. I collaborate with my own students, but also with other students, with other groups and faculty (like CHISEL and SEGAL), and industry.\nProspective graduate students see here.\nCurrent students should read through the onboarding documents."
  },
  {
    "objectID": "students.html#students",
    "href": "students.html#students",
    "title": "Students",
    "section": "Students",
    "text": "Students\nSee the up to date list at Github"
  },
  {
    "objectID": "old-comments.html",
    "href": "old-comments.html",
    "title": "Old Comments",
    "section": "",
    "text": "I exported the comments from the Wordpress version of this blog as best I could. Perhaps sometime later I will link back to the original post.\n\n\n\nAuthor\nComment\nIn Response To\nSubmitted On\n\n\n\n\nJorge Aranda\nIn reply to Neil Ernst. As a likely quack, I find this fine line comforting\nColumbus’s Heilmeyer Catechism\n2016/07/20 at 12:55 am\n\n\nNeil Ernst\nIn reply to Jorge Aranda. Quack … or visionary? That’s a fine line.\nColumbus’s Heilmeyer Catechism\n2016/07/19 at 8:34 pm\n\n\nJorge Aranda\nColumbus did get his proposal “reviewed,” by Portuguese, Genovese, Venetian, and Spanish experts, and it was turned down every time, on the grounds that his math was pretty wrong and the Earth was bigger than he wished. They knew (as most literate people then) that the Earth was round, and they knew roughly its size—Columbus must’ve seen to them a bit of a quack. He got funded pretty much on a whim by the Spanish crown, and they just happened to get lucky: https://en.wikipedia.org/wiki/Christopher_Columbus#Quest_for_financial_support_for_a_voyage\nColumbus’s Heilmeyer Catechism\n2016/07/19 at 6:34 pm\n\n\nAdi Prasetyo\nHello thanks\n13 Great Software Architecture Papers\n2016/04/11 at 10:19 am\n\n\nStefan Wagner\nI think the first one is solved in principle. There are so many archiving options out there now that also give you a DOI. I specifically like ZENODO which is able to archive specific versions from GitHub.  The second one is really a problem. I guess we should have an opt-in mechanism in GitHub and similar platforms for participating in such studies.\nOn Using Open Data in Software Engineering\n2016/03/08 at 5:26 am\n\n\nNeil Ernst\nIn reply to Jorge Aranda. Yeah, the point here is to add the right tests, so that one uncovering (important) bugs should have been closer to the first one added. (you write buggy code?)\nThe Marginal Utility of Testing/Refactoring/Thinking\n2016/01/21 at 3:12 pm\n\n\nJorge Aranda\nThere are diminishing returns on extra tests for sure—but still, sometimes I find that a test I previously thought almost pointless uncovers bugs, and that just reinforces my need to test as heavily as I can.\nThe Marginal Utility of Testing/Refactoring/Thinking\n2016/01/21 at 2:41 pm\n\n\nNeil Ernst\nIn reply to fabianodalpiaz. I guess it falls under ML tools … but you’re right. I really like Garm’s work though.\nRequirements, Agile, and Finding Errors\n2015/12/22 at 12:56 pm\n\n\nfabianodalpiaz\nNice post! It is curious that you don’t mention NLP explicitly. In the PhD work of Garm Lucassen (e.g., http://www.staff.science.uu.nl/~dalpi001/papers/luca-dalp-werf-brin-15-re.pdf), we use simple NLP techniques to improve the quality of user stories (… we don’t impose new notations ).Dan Berry’s dumb tools paper (The Case for Dumb Requirements Engineering Tools, REFSQ’12) inspired our work: a useful automated tool for RE is one that achieves (close to?) 100% recall; the cost may be to sacrifice precision, but at least the analyst doesn’t have to recheck all the requirements.Our tool is deployed as a service and integrates with Jira/Pivotal. Integration with SonarQube as you suggest is definitely interesting!\nRequirements, Agile, and Finding Errors\n2015/12/08 at 5:39 am\n\n\nJorge Aranda\nIn reply to Neil Ernst. That’s true. But in your example I would say that this is what web frameworks are giving us—they are painful to set up, but on the whole worth it and an improvement over the status quo of a few years ago. However going from that to systems “generated automatically by algorithms based on well specified requirements and test cases” is a big qualitative jump, and that’s the one that makes me very sceptical.\nHow Writing Code is Like Making Steel\n2015/11/12 at 1:41 pm\n\n\nNeil Ernst\nIn reply to Jorge Aranda. I think we’re still in agreement. I just think the level of abstraction where the discernment is needed will be higher. It’s a familiar argument: we almost never hand-code assembly anymore. Why should we hand-code fairly repetitive Javascript to make a page do something a thousand others do?\nHow Writing Code is Like Making Steel\n2015/10/29 at 8:56 am\n\n\nJorge Aranda\nThis is the odd instance where we disagree completely. I don’t discount search-based software development, but I don’t see how it can lead to the qualitative jumps in discernment that are routinely needed in software work. Funnily, when I read the title of your post, I was expecting the steel making analogy to go elsewhere: something about hardening and tempering based on the interaction of the system and the rest of the world, and I was fully prepared to agree with that analogy\nHow Writing Code is Like Making Steel\n2015/10/29 at 1:06 am\n\n\njuli1\nWow – super interesting point of view. I think this is definitively an optimist point of view. Two to three years ago, I would have made fun of you. But now, considering the changes I saw over the last years, I am less skeptical. This future is exciting and scary at the same time, not really sure how to consider it.\nHow Writing Code is Like Making Steel\n2015/10/27 at 12:21 pm\n\n\nNeil Ernst\nIn reply to C. Albert. Thanks! My colleague insists this is a google/jquery bug.\nThoughts from a CodeFest\n2015/02/24 at 11:15 pm\n\n\nC. Albert\nI love the map you made! I can only see the top left corner. But for only 24 hours that is some pretty good work.\nThoughts from a CodeFest\n2015/02/24 at 11:13 pm\n\n\nBen Burton (@bjburton)\nWe did the same challenge! https://grub-up.herokuapp.com/\nThoughts from a CodeFest\n2015/02/24 at 4:19 pm\n\n\nSteve Easterbrook\nIs the Comic Sans really that much of a giveaway? Surely I’m not the only one who uses it to annoy the font nazis?\nThe Gap Between User Requirements and Software Capabilities as Technical Debt\n2015/01/20 at 11:00 pm\n\n\njuli1\nThere are some guidelines for using a library in the “Hacker’s Guide to Python” (very good book by the way). The general idea is: if the framework makes a ton of work for you and is well supported by a BigCo, go for it! If you are just using a function in a library, implement it yourself and avoid the headaches of compatibility and maintenance. Example: wanna make a dynamic mobile app? Use jquery – this is supported by Microsoft, google, etc … the big players of the web! So you have a good probability it will be maintained AND supportedExample 2: want to write excel documents in Java do not take jexcelapi (http://jexcelapi.sourceforge.net/), this is done by a single guy and the last version is 4 years old! Take Apache POI that is active and still under development (and supported by a foundation).My 2 cents: use common sense to architecture your software and weight pros/cons to rely on somebody else.\nFrameworks, libraries, and dependencies\n2015/01/20 at 8:18 am\n\n\nNeil Ernst\nIn reply to fede_luppi. It’s been a while, so things may have changed, but if header levels don’t work you might want to try a bulk search and replace in the resulting Latex file (e.g. s/chapter/section/ & s/section/subsection.\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2014/09/24 at 7:45 am\n\n\nfede_luppi\nWhen I compile following this workflow, my resulting pdf is structured as chapters. I want a research paper, so I do not want my first levels to be named as chapter, but rather just with numbers (I, II, III,…). I tried with different base header levels in the meta-data, with no result. What else can I do? Thanks\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2014/09/17 at 1:05 pm\n\n\nmircealungu\nThe text is interrupted after: “Partly is this because…”. Is this an example of CMS failure?\nSoftware research shouldn’t be about the tools\n2014/06/13 at 10:38 am\n\n\nAle\nI found out here http://tex.stackexchange.com/questions/91522/how-can-i-get-my-latex-set-up-on-emacs-auctex-to-use-the-file-line-error-option that it is possible to modify directly the command inside emacs and it works!\nForcing AucTex to properly show error messages\n2013/06/07 at 7:45 am\n\n\nirwinhkwan\nThis is a very good list and one that aligns primarily with my experience as well, though in my case I didn’t continue with my Ph.D work extensively and instead extended and picked up much of the work being done in the new group that I was working with. I think this depends highly on the group you are joining.\nSome Advice on Doing a PostDoc in Software Engineering\n2013/06/03 at 5:06 pm\n\n\nJulius Davies\nInformative and reasonable!\nSome Advice on Doing a PostDoc in Software Engineering\n2013/05/23 at 4:30 pm\n\n\nNeil\nI think it is probably a challenge on every project, agile or not – certainly that is what we hear from various clients we have at SEI. Prioritization is poorly understood – some people in the Lean world call it waste, but at some point you have to decide what you are going to implement, and that to me is prioritization. Maya Daneva does interesting work on prioritization.\nThe fuzzy notion of “business value”\n2013/03/14 at 5:40 am\n\n\nEric Knauss\nI especially like 5: In my opinion, a motivated team, being passionate about what they are doing, ranges among the top success factors. Do you have any information why (2-5) should be easier to make visible in non-agile projects? Just curious. For example, on the one hand I assume that (3) could be difficult, because you would not implement for future technical challenges (YAGNI). On the other hand, plan for change enables organizations to move quickly, when a new opportunity arises. Any thoughts?\nThe fuzzy notion of “business value”\n2013/03/13 at 8:06 pm\n\n\nChris Parnin (@chrisparnin)\nNext semester, the first homework assignment is to add a new feature to last year’s projects.\nTeaching Advanced Software Engineering\n2013/01/25 at 3:19 pm\n\n\nEbioman\nIn reply to Ebioman. Nevermind found it somewhere else: https://github.com/neilernst/misc/blob/master/abbrvnat-nourl.bst\nMore helpful LaTeX tips\n2013/01/09 at 2:03 am\n\n\nEbioman\nA shame that the file is gone – was really looking for something similar …\nMore helpful LaTeX tips\n2013/01/09 at 2:01 am\n\n\nArber Borici (@ArberBorix)\nThanks for pointing it out. This is particularly true for two other classes of problems PhD researchers encounter: undecidable and hard. I’m often puzzled to hear of PhD students trying to come up with solutions to problems which the Halting problem is reducible to. Also, instead of spending a few hours thinking whether another problem is hard or not, they immediately jump into finding a solution. Which turns out to be frustrating after a few months of failed attempts :)…\nA stitch in time…\n2013/01/06 at 2:33 pm\n\n\nअहंनास्मि (Ahannāsmi)\nI was searching Google for this problem, and yours was one of the first pages that came up. Thank for put up the solution. I used your solution (with the fix suggested by Marius), and it worked. Also, as you suggested, there is in fact a faster way to reload the configuration change than running a full update. Just run the program mktexlsr (with root permissions if texlive is installed by root on your system). I am just putting it up there for the next person who might find this page looking about this problem. Thanks again for posting this solution, and please feel free to delete this comment if it does not agree with your policies.\nForcing AucTex to properly show error messages\n2012/09/28 at 6:40 pm\n\n\nAsfahaan Mirza\nWoW this looks pretty complicated. can you please create a screencast with these steps? I am using Scrivener and Mendeley. Struggling to make it work.\nSome notes on integrating Mendeley, Scrivener, MultiMarkdown and (Xe)Latex\n2012/09/19 at 6:24 am\n\n\nNeil\nIn reply to JLawrence. The best way is probably to use the comment notation to pass the image code through as plain Latex (ie. uninterpreted by Markdown compiler). To be honest, once the bulk of the text is written and converted, I often found it simpler to use Latexian or Emacs etc. to do these finicky edits.\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2012/08/14 at 9:57 am\n\n\nJLawrence\nOne thing that is not clear for me is how to insert figures and get them properly numbered/captioned (incl. after several revisions). I tried this workflow once, but it was so hard to understand, especially when I had to use many packages like mchem, etc. Can a master here provide a step-by-step guide to improve Scrivener-Latex workflow ? My Scrivener 2 has been biting the dust since I bought Pages and Latexian…\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2012/08/13 at 7:43 pm\n\n\nNeil\nIn reply to Dan Griffin. Thanks for visiting, Dan. I have tried out JazzHub, and it is a good option. I still think, though, that RTC is just too much tool for students to get used to. These course projects often only last maybe 8-10 weeks.\nUsing GitHub for 3rd Year Software Engineering\n2012/08/13 at 2:20 pm\n\n\nDan Griffin\nHi Neil. I just ran across your blog. Thanks for your comments about RTC. I’m on the marketing team for IBM Rational and work quite a bit with universities. Are you aware of our JazzHub offering (hub.jazz.net)? JazzHub removes the requirement for a locally installed server and allows the students to quickly setup projects in the cloud. I’d love to talk to you about it and show you a demo. Also, you will be happy to know that one of the new features we added in RTC 4.0 is windows shell integration. This would allow your students to use whatever IDE they want — with no RTC integration required. Then they could use windows explorer to sync their files, much like you describe with Git above. Here is a blog discussing that feature: https://jazz.net/blog/index.php/2012/02/06/introducing-the-rational-team-concert-shell-integration-for-windows-explorer/I’d also be very curious to hear more of the comments you get on your course feedback — as those are exactly the types of things we need to know.–Dan\nUsing GitHub for 3rd Year Software Engineering\n2012/08/13 at 1:05 pm\n\n\nJorge Aranda\nI’ve been hearing about Sassen’s keynote a lot, but I hadn’t found anyone who could explain it to me, or even what it was roughly about. So thanks. It sounds like it was actually quite interesting, though it’s also obvious from other people’s comments that Sassen did not make it easy for this community to understand her.\nICSE 2012 Thoughts (1): Saskia Sassen Keynote\n2012/06/25 at 3:45 pm\n\n\nNeil\nIn reply to Adrian. This is the Cisco client from the UBC site. I tried the Mac native client but no luck on my end.\nUsing iCloud and Cisco VPN\n2012/06/21 at 4:23 pm\n\n\nKambria\nShe made reference to a phenomenon she called “barefoot engineers”, people who, post-Communism, set up rudimentary technologies like utilities outside of the traditional structures.…In health care we call these people “positive deviants” I really like this approach to solving problems: http://www.positivedeviance.org/\nICSE 2012 Thoughts (1): Saskia Sassen Keynote\n2012/06/21 at 2:35 pm\n\n\nAdrian\nAre you using the Cisco client or the built in VPN client of OSX?\nUsing iCloud and Cisco VPN\n2012/06/20 at 4:16 pm\n\n\nJohn Hunter\nWhen metrics are used to aid learning they can be beneficial. When they are used to set goals tied to bonuses or indirectly tied to money via performance appraisals… they mess things up. The focus turns to meeting the number not doing the job. &lt;http://management. curiouscatblog.net/2004/08/29/dangers-of-forgetting-proxy-nature-of-data/&gt; Even when metrics are used for learning they can lead to all sorts of trouble when there is not an understanding of variation. Thankfully software developers are more likely to have a basic idea of variation than MBAs, but it is still a big problem. http://management.curiouscatblog.net/2006/05/09/understanding-data/ I very much like “Let us stipulate that there are endless examples of low-maturity teams out there whom no technique will help.”\nDeMarco and “Cannot control what you cannot measure”\n2012/04/28 at 8:56 pm\n\n\nNeil\nIn reply to planetpolly. No, no data to hand, although the standard approach (Google web search comparison) shows 245M results for Git and 3M for “IBM RTC” (yeah, 7 years of PhD and this is the best I can do. I feel the same as you re: OSS. In general, my hunch is that employers would like someone who at least understands version control principles. Specific tool practices can be taught on the job, I think.\nUsing GitHub for 3rd Year Software Engineering\n2012/04/26 at 2:35 pm\n\n\nplanetpolly\nOur software team transitioned about a year ago from SVN to Git / Github. Thinking about it from a teaching perspective, I would way rather have a separate version control tool, outside the IDE, to start with – it makes it clear what the tool is doing, and then when a student is later shown IDE integration, they ‘get’ that is is making their lives easier, but they better understand the underlying mechanism. Otherwise it’s easy to memorize the ‘steps’ to get code checked in without really understanding the underlying version control model…Do you have any info on the industry adoption of these tools? Teaching students how to use git and the underlying models seems like a valuable skill. In general, I support teaching students using open-source tools and leaving the big expensive tools for when / if they get that kind of firepower in a workplace\nUsing GitHub for 3rd Year Software Engineering\n2012/04/26 at 12:50 pm\n\n\nKambria\nIsn’t the difficult thing for humans, the unknown? Can you look at the evolution of code and see what percentage changes or is add/deleted in coming releases to identify a baseline of expectations. If people understood and could factor in how much the anticipated cost will be, knowing that there will be changes that could be huge.\nRequirements tools and tasks\n2012/03/28 at 2:27 pm\n\n\nNeil\nIn reply to Weldon Bonnell. I passed them through with comment tags. But if you use the Markdown reference syntax, I think it will translate into the correct Latex: see http://daringfireball.net/projects/markdown/syntax#link.\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2012/03/15 at 12:17 pm\n\n\nWeldon Bonnell\nThanks for writing about your experience. Still confused about the use of the command. Is there some syntax for MMD3 to generate the in LaTeX automatically or after all this are you still passing them through with the ⟨!– –⟩ ?\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2012/03/15 at 12:06 pm\n\n\nTim Brandes\nThanks for the article. Thanks jan gerben for the MMD3 explanation. Based on this, I’ve written an article covering the topic, too: http://timbrandes.com/blog/2012/02/28/howto-write-your-thesis-in-latex-using-scrivener-2-multimarkdown-3-and-bibdesk/\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2012/02/28 at 2:51 pm\n\n\nNeil\nIn reply to Jorge Aranda. Thanks Jorge. I agree with your critique. I suspect at least one reason is simply information overload. I find it difficult enough to stay on top of my own field; trying to determine how other disciplines might be of use seems overwhelming. Doesn’t it come back to the fact that true inter/trans-disciplinarity is incredibly difficult to organize and succeed at?\nCase studies and grounded theory in software engineering\n2012/02/23 at 8:40 pm\n\n\nJorge Aranda\nThis is a very nice post, Neil. Incidentally, one of the problems I have with grounded theory is its disregard for current and valid theory. You’re supposed to start with a blank slate, and let a theory arise from your data. But for most of the questions we’re interested in, there’s already plenty of theories (sociological, psychological, organizational) that, with slight modifications, should be applicable in our field. Why ignore them?\nCase studies and grounded theory in software engineering\n2012/02/23 at 8:02 pm\n\n\nMarius Hofert\nokay, it works, I had to use “file_line_error_style=t” so with ending _style and with underscores.\nForcing AucTex to properly show error messages\n2012/02/17 at 11:55 am\n\n\nMarius Hofert\nThis problem is also mentioned here: http://stackoverflow.com/questions/7885853/emacs-latexmk-function-throws-me-into-an-empty-buffer I followed this advice and also tried your solution but it did not work for me (with Gnu Emacs 24 on Mac OS X 10.7.3 and with AUCTeX 11.86)\n\n\n\n\nForcing AucTex to properly show error messages\n2012/02/17 at 11:03 am\n\n\n\n\nChristine\nIn reply to Sam. HAHAHA! This will reoanste with me. The “deathtrap” is the perfect term for that situation. And you’re right, it’s scary as hell without brakes!\nPointless: Bike lanes downtown\n2012/02/10 at 8:06 am\n\n\ngerben jan\nhi,For installation of MMD3 in scrivener you should add that one needs to install the MultiMarkDown-Support package! This changes the (on mac) ~/Library/Application Support/Multimarkdown/bin just figured this out so thought I should mention it\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2012/01/19 at 10:22 am\n\n\nJorge Aranda\nI’m glad you wrote to the President, Neil. The comments in his blog post are worth reading, too, and quite an embarrassment for the ACM’s position.\nThe Research Works Act\n2012/01/16 at 5:10 pm\n\n\nNeil\nIn reply to Stephan Lewandowsky. Exactly. Not ideal but it works. I don’t know about Windows, but I wouldn’t be surprised, since there isn’t anything Windows specific involved.\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/11/16 at 4:10 pm\n\n\nStephan Lewandowsky\nrighto, thanks, that clarifies it. so i presume the commands are likewise just embedded in the scrivener text so you can refer to figures and so on? final question: do you know if mmd3.2 for windows integrates equally seamlessly with scrivener?\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/11/16 at 3:52 pm\n\n\nNeil\nIn reply to Stephan Lewandowsky. Yes, you type MMD syntax into Scrivener. E.g., [#ernst11re;]. Then the Scrivener “compile” menu choose MMD-&gt;Latex. You can make tables with MMD, but since a lot of my thesis was already in Latex, I would surround the Latex syntax for the figures and tables with HTML comments, which are then translated verbatim during the compile process. The compile command generates a .tex file, which you then must compile using pdflatex or what have you. I believe you can skip this and go straight from MMD to PDF, but I need more customization.\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/11/16 at 9:17 am\n\n\nPaul Gvildys\nIn reply to Neil. The code is similar enough for the most part (I’m on enough maintenance release code reviews that this holds true for the most part).The difficult thing is not just the switching, it’s the “being pulled away from mainline dev” and so having a hard time getting the mainline dev stuff done. There’s also the case where some maintenance is so old that it runs on different platforms, resulting in having several Virtualized OSes lying about.\nMy new gig at UBC\n2011/11/16 at 7:58 am\n\n\nStephan Lewandowsky\nVery nice post, very helpful and succinct. What I cannot figure out is what you actually type into scrivener to get the commands into MMD and ultimately LaTex. That is, the [#Jarke..] and [^foot1] commands in your mmd file; are they typed into Scrivener exactly like that? And how do you insert figures and tables with references to it. I noted that your sample .tex file contained a with a : How does this start out in scrivener? I am also unclear on what the final output of the scrivener compile command is: Is it a LaTeX file which you then need to process via standard LaTeX compilation or is it a complete PDF (which means scrivener somehow pulls in miktex or whatever to do the job).\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/11/16 at 2:55 am\n\n\nNeil\nDo you find it difficult to context-switch from maintenance to mainline? Or is the code similar enough?\nMy new gig at UBC\n2011/11/15 at 10:58 pm\n\n\nPaul Gvildys\nAlthough I am more in the practical than the research, I am interested in the last two topics you presented.The maintenance one mainly because I work on mainline development, but often assist the maintenance team on maintenance branches, and if they are all on vacation, sometimes get pulled in to fix maintenance branches.The last one I find interesting as that’s exactly how I program: I theorize and build everything in my head – software takes a physical form in my head with mechanical parts and everything – and then I go about implementing what I built in my head. It’s also how I debug bugs.\nMy new gig at UBC\n2011/11/15 at 6:06 pm\n\n\nNeil\nIn reply to David. Isn’t unpredictability somewhat axiomatic in defining ULS? It seems like Agile approaches, although hardly tested at scale, have it right when they simply accept change as inevitable.\nUltra-large-scale systems: fundamentally different?\n2011/11/15 at 12:11 pm\n\n\nDavid\nThe key challenge for LSCITS and ULS style initatives is to develop a science and engineering of software systems that enables the prediction, identification, management and troubleshooting of emergent behaviour. The fundamental problem is that current software engineering techniques do not tend to cope well with emergence. In otherwords we are not as good as industry would like us to be at the predicting and coping with non-linear interactions. Example problem 1: When ‘Company A’ scales up a distributed system from 10,000 to 100,000 nodes and it behaves in ways that their state-of-the-art models and simulations did not predict. Example problem 2: When ‘Company B’ deployed a system at their Manchester office the users benefited from its functionality and it worked as expected. However when ‘Company B’ deployed the same system at several of other sites it was resisted and workers claimed that it did not fit the way their sites worked despite them having the same business processes as at Manchester.\nUltra-large-scale systems: fundamentally different?\n2011/11/15 at 11:37 am\n\n\nJorge Aranda\nWhat, no knowledge of piñatas after all those years sitting next to me? (BTW for a moment I was thinking that part of the sh*t that 2006-2008 Neils went through was our seating arrangement, but then I realized we moved into the lab precisely after that period! So: after sitting next to me, sh*t transformed into gold. You’re welcome!)But seriously, I’m happy for you and for the next stage in your life. Congratulations!\nWhat I learned at UofT\n2011/10/25 at 12:08 am\n\n\nNeil\nIn reply to Fabiano. Someone should put a measurable theory in place then, and we can test it to see whether what you postulate is true. So far ULSS seems like a bunch of hand-waving and unchallenged assumptions. This is all too prevalent in academic research.\nUltra-large-scale systems: fundamentally different?\n2011/10/11 at 8:59 am\n\n\nFabiano\nIn reply to Neil. The challenge is not in size. I guess the fundamental difference is in real agent-orientation. What that means is that every agent is a locus of control, you don’t have a (hierarchical) centralized control and what actually matters is interaction among these “agents”. Therefore, traditional algorithms/design methodologies result conceptually inadequate.\nUltra-large-scale systems: fundamentally different?\n2011/10/11 at 4:53 am\n\n\nNeil\nIn reply to iandol. Todonotes works well, but to be honest tools like Word’s track changes are much more useful. But then you’d have to use Word\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/10/06 at 12:40 pm\n\n\niandol\nHow does scrivener &gt; MMD &gt; tex handle comments? Does that link in to todonotes in your preamble?\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/10/06 at 11:38 am\n\n\nNeil\nIn reply to Jorge Aranda. I don’t agree that this increasing complexity is anything fundamentally new. Arguably, someone from the 80s would be incredibly surprised at the complexity of software today – e.g., a fly-by-wire system in an airplane, even a portable phone that can direct me to the nearest Starbucks. I’m a software optimist: I think by and large software has dealt with some enormously challenging systems very capably.\nUltra-large-scale systems: fundamentally different?\n2011/09/30 at 5:52 pm\n\n\nJorge Aranda\nI think the issue is not so much that these systems are “ultra” large (whatever “ultra” means), but that they are increasingly complex, and systems change in their behaviour and understandability with complexity. So—the argument goes—it’s not that we’re dealing with instructions that are orders of magnitude more numerous, but that they are supposed to serve a function in sociotechnical systems that are orders of magnitude more complex.\nUltra-large-scale systems: fundamentally different?\n2011/09/30 at 4:36 pm\n\n\nJeremy Gibbs\nIn reply to Jeremy Gibbs. Nevermind, I have it figured out. It turns out the LaTeX compile option in Scrivener has a Meta-Data option. It was populated with a blank author and title, so it was printing them out regardless of my Meta-Data file. Thanks again. This cements my dissertation workflow.\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/09/18 at 12:38 pm\n\n\nJeremy Gibbs\nThanks for the succinct post. I have everything working okay, except that when I compile the Scrivener paper, the resulting .tex file has a couple of extra things on top that I can’t seem to get to go away. They are empty mytitle and myauthor tags, regardless of whether I have defined them in the Meta-Data file. Any ideas?\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/09/18 at 10:38 am\n\n\nfrerin\nSomehow this is not working for me, likely I do not understand what to put there and how to define all the stuff.\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/09/17 at 6:16 am\n\n\nNeil\nIn reply to Carlos. You’ll need to stick your .tex files somewhere the latex parser can find it, obviously. I don’t totally understand the tex tree on Mac, but I have my header and footer in ~/Library/texmf/tex/latex/mmd/, alongside the MMD3 samples. Then the class file is in ~/Library/texmf/tex/latex/ (ut-thesis.cls).If you post the latex that was generated on a gist site, I can give more feedback.\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/08/03 at 7:55 pm\n\n\nCarlos\nHi, nice post, but I’m at lost. I’ve installed the two packages. Then I’ve create a new document in Scrivener and I’ve add a new text with the meta-data. And then…? Can you explain or suggest me a website to understand how can I define a new latex-class in MMD3, let’s say one of the IEEE templates? Thank you.\nWriting Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3\n2011/08/03 at 6:54 pm\n\n\njulio leite\nCongrats. Nice post.\nIT failure statistics\n2011/04/21 at 8:09 am\n\n\nNeil\nIn reply to Patrik Björklund. I’m not as familiar with the business side of things, but there is good research from Scandinavia on business requirements, software development, and maintenance. For example, Bente Anda’s paper on four companies building the same system. In general studying these things is challenging because of the complexity. Controlling for all the variables is essentially impossible. So most of the research is case study format, and its poorer cousin, anecdote. I too find it strange though that so much empirical software research was done in the 80s and then died out. Perhaps the lesson is that the research didn’t generalize very well to other settings (e.g., the Basili studies on NASA are a pretty unique environment).\nIT failure statistics\n2011/01/29 at 11:15 pm\n\n\nPatrik Björklund\nHi Neil! It’s an interesting subject you bring up which made me wonder if there is any academic studies done recently on the current situation of IT project failure? The sources I see referred to all the time are kind of old. Thanks. (I actually got here from the mendeley/scrivener post which was also pretty useful)\nIT failure statistics\n2011/01/29 at 1:48 pm\n\n\nNeil\nIn reply to microbe. My supervisor makes comments on the PDF I create, and I then fix things up in the Scrivener file. That way I can save a “snapshot” of my work prior to making his changes. So the Scrivener version is always canonical. I would say to pick one version and make it the main one, otherwise you’re right, it’s a nightmare. If your supervisor makes changes in Latex (lucky!) I’d just use version control (e.g., Git) and stick with Latex. For safety I have my latex export in a Dropbox folder, and save Scrivener backups (zip files) to the same folder.\nSome notes on integrating Mendeley, Scrivener, MultiMarkdown and (Xe)Latex\n2011/01/28 at 10:40 am\n\n\nmicrobe\nI’m wondering how you use those tools during revision process. I use Mendeley to manage my references and LaTeX for writing. I was trying Scrivener and it was great for drafting and smooth to move to LaTeX for further processing. Then my draft is going back and forth between my supervisor and me. In this step, I use mainly LaTeX but I’m worried a little bit the draft in Scrivener because it becomes outdated from the current version. Do you somehow sync back or just leave the draft as draft?\nSome notes on integrating Mendeley, Scrivener, MultiMarkdown and (Xe)Latex\n2011/01/28 at 10:16 am\n\n\nEvan Cofsky\nIsn’t it 37signals? yes, sorry, fixed it.\nThe industrial fallacy in software research\n2011/01/25 at 11:27 pm\n\n\nLean Education\nSoftware was developed for dedicated purposes for dedicated machines until the concept of object-oriented programming began to become popular , making repeatable solutions possible for the software industry. Dedicated systems could be adapted to other uses thanks to component-based software engineering. Companies quickly understood the relative ease of use that software programming had over hardware circuitry..\nManagement and software projects\n2011/01/19 at 1:39 am\n\n\nNeil\nIn reply to Keith. Markdown doesn’t use typefaces but rather HTML tags. So the only options available are &lt;strong&gt; and &lt;em&gt;. I suspect something in the Kindle conversion is translating &lt;em&gt; into underlines. I’d look for the intermediate files – the XSLT for the Kindle conversion.\nSome notes on integrating Mendeley, Scrivener, MultiMarkdown and (Xe)Latex\n2010/11/11 at 3:17 pm\n\n\nKeith\nDo you know of any way to keep italics in my text from being converted to underlines when I compile the document. I’m using OSX and converting the document to a Kindle format. Every time I compile my italicized text gets underlined. It’s driving me a little crazy\nSome notes on integrating Mendeley, Scrivener, MultiMarkdown and (Xe)Latex\n2010/11/11 at 3:00 pm\n\n\nMuhammad AbuBakar\nHello FriendsI all all are find shine . Please can some one share that while listening to an audio book, can we mark/make several bookmarks ? E.G few in the chapter one, few in chapter four etc etc. Please reply. Keep Smiling\nITunes bookmarks\n2010/10/20 at 5:41 am\n\n\nAlberto Bacchelli\nThanks Neil, this is a very interesting post!\nIT failure statistics\n2010/08/11 at 3:06 am\n\n\nNeil\nI forgot to mention that REFSQ2011 will have a revised timeline. The conference will be in Essen in March 2011, and the abstract deadline is October 8th, 2010.\nREFSQ summary\n2010/07/23 at 10:20 am\n\n\nMarcel vdL\nNice summary of the conference and I liked being there (and meeting up).I’m not too sure I understood the plenary statements as you phrased them here, however. But let’s not get into the debate again right here… Being ‘from the industry’, I agree with your observation that the quality of the end-product should be the focus, and that the academia must be aware of this. In all fairness, I think the general trend is moving in that direction, which I was glad to see. A little less “research for research’s sake”, please. The conference was a good way to get ‘industry’ and ‘academia’ together to talk and exchange experiences. I hope to see you again next year!\nREFSQ summary\n2010/07/05 at 3:49 am\n\n\nNeil\nIn reply to Jorge Aranda. Well one way is to tie your theory into the broader perspective … i.e. “Small companies dispense with RUP because it hurts quality” etc.\nREFSQ summary\n2010/07/03 at 3:40 am\n\n\nJorge Aranda\nNice. It sounds like it was quite fun and thought-provoking; I’m sad I missed it.“We seem to focus so much on “making RE better” that we lose sight of the ultimate goal, which is to make better (software) products.”Yes, I agree. But isn’t this the bane of all specializations? Is there really a way out of it?\nREFSQ summary\n2010/07/02 at 9:18 pm\n\n\nNeilfink08\nIn reply to Alecia. Thanks Alecia!\nWhat does it mean to have a baby?\n2010/03/20 at 9:43 am\n\n\nNeilfink08\nIn reply to Jorge. Piñata! Haven’t done that since I was a kid. Daytum is pretty simple, but it was enough for what I wanted. You could get better visualizations from excel entry (I think you had a similar thing from Seattle?).\nWhat does it mean to have a baby?\n2010/03/20 at 9:43 am\n\n\nJorge\nYou missed the breaking of the pinata at the lab! What is this daytum thing? Is it just data entry & visualization, or does it have some way to track time that I’m missing?\nWhat does it mean to have a baby?\n2010/03/20 at 9:03 am\n\n\nAlecia\nAwww that’s great Neil! It’s a little late, but congratulations nonetheless\nWhat does it mean to have a baby?\n2010/03/19 at 11:50 pm\n\n\nNeilfink08.wordpress.comx\nIn reply to Bill Conniff. The files I had did not have CR/LF characters (which are just whitespace elements in XML). And the question is not “will VIM show it” but rather “how long do you want to wait”. I certainly wouldn’t want to edit that size file using XML mode.\nMSR Challenge: large files revisited\n2010/03/01 at 9:59 am\n\n\nBill Conniff\nWhat if a bad character occurs in a file with no carriage returns or line feeds (common in xml messaging)?Will vim show a 3GB file in a tree?\nMSR Challenge: large files revisited\n2010/02/28 at 4:17 pm\n\n\nNeilfink08.wordpress.comx\nIn reply to Steve Easterbrook. But then we get into the challenge of educating scientists about parallelization and optimization and discretization. Topics even experienced programmers don’t understand very well. Science was easier when we just had slide rules and log tables.\nOpen science and workflows\n2010/02/02 at 12:06 pm\n\n\nVítor Souza\nHey Neil,Good points. I’ve recently bought an Android phone and I’m enjoying it. But you gotta have a reason to make this kind of upgrade, otherwise it’s just wasting money. Cheers from Trento! Vítor\niPhone? Am iMissing something?\n2010/02/02 at 5:11 am\n\n\nSteve Easterbrook\nI think one of the biggest challenges is to get design choices about parallelization and algorithm optimization up there in the “language of science” representation, so that these are no longer an afterthought.\nOpen science and workflows\n2010/02/01 at 12:46 pm\n\n\nMr. Gunn\nNeil, I like how you’ve got your collection feed going into Friendfeed. I’ve got one set up that way, too. A “just bookmarked” feed should be coming soon, too.\nThoughts on open notebooks for software scientists\n2010/01/29 at 12:35 am\n\n\nNeilfink08.wordpress.comx\nIn reply to rogerthesurf. Now, what I want is, facts. Teach these boys and girls nothing but Facts. Facts alone are wanted in life. Plant nothing else, and root out everything else. You can only form the minds of reasoning animals upon Facts: nothing else will ever be of any service to them. This is the principle on which I bring up my own children, and this is the principle on which I bring up these children. Stick to Facts, sir!— Dickens\nUnderstanding climate change with anecdote\n2010/01/28 at 4:48 pm\n\n\nrogerthesurf\nRight on Neil, Are you afraid that your faith in Global Warming is caused by CO2 might be threatened? Return to facts for once in your life! If you can find logical and well referenced facts to refute what is on my blog, I am all ears, and I always allow ALL comments there as well. Cheers Roger\nUnderstanding climate change with anecdote\n2010/01/28 at 4:45 pm\n\n\nNeilfink08.wordpress.comx\nIn reply to Jorge. Somehow I haven’t got around to reading his blog…\nUnderstanding climate change with anecdote\n2010/01/28 at 3:46 pm\n\n\nJorge\nLooks like you got your own climate denial troll, Neil, I’m envious!\nUnderstanding climate change with anecdote\n2010/01/28 at 3:44 pm\n\n\nrogerthesurf\nNeil,I trust you read my blog then? I agree wholeheartedly that our planet needs attention but what I am saying is that chasing after the life giving gas CO2 as the culprit, will simply divert resources away from the real problems that need attention such as heavy metal pollution, genuinely poisonous gases and chemical waste etc. AND if the IPPCC have their way, meeting the carbon emission targets and the proposed transfers to third world countries they propose will simply break the world economy. In the pipeline for my blog is a page that will show how the proposed targets if implimented will cause general economic collapse and likely starvation for you, me and our children. All this for an unproven discredited hypothesis? Please watch my blog Cheers Roger http://rogerfromnewzealand.wordpress.com\nUnderstanding climate change with anecdote\n2010/01/20 at 6:12 pm\n\n\nNeilfink08.wordpress.comx\nIn reply to rogerthesurf. Roger, you can absolutely do something about it. Start with driving your car less, eating more vegetarian foods, reducing home power consumption, composting, etc. No matter what your beliefs on the global conspiracy, I fail to see how these actions will harm you – and they may even do some good. Good luck!\nUnderstanding climate change with anecdote\n2010/01/20 at 10:49 am\n\n\nrogerthesurf\nThere might be global warming or cooling but the important issue is whether we, as a human race, can do anything about it. There are a host of porkies and not very much truth barraging us everyday so its difficult to know what to believe. I think I have simplified the issue in an entertaining way on my blog which includes some issues connected with climategate and “embarrassing” evidence. In the pipeline is an analysis of the economic effects of the proposed emission reductions. Watch this space or should I say Blog http://www.rogerfromnewzealand.wordpress.com Please feel welcome to visit and leave a comment. Cheers Roger PS The term “porky” is listed in the Australian Dictionary of Slang.( So I’m told.)\nUnderstanding climate change with anecdote\n2010/01/20 at 5:20 am\n\n\nCameron Neylon\nI think its a really interesting question what the threshold is for different purposes. I mean that there is no reason not to record everything, because it is “easy” and comprehensive. But when you’re presenting that for some specific person or system for a specific purpose you will want to summarize it in some way. The choices you make seem to me to depend on what the purpose of your communication is and what/who the target is. Trivial example, if you want to show to a software developer a problem with their system you want a different kind of summary than the cleaned up and streamlined version that you might submit with a paper. But there are a lots of subtleties here. What do you think about the ideas I suggested about capturing the relationships between the objects you created? Does that work in your context or is there too much command line work between the creation of the relevant objects?\nA better scientific notebook\n2010/01/16 at 6:17 am\n\n\nAnonymous\nI think you have to believe in intellectual equality in order to buy David’s argument. This is also making a lot of assumptions about rationality. Given that 60% of the adult population has problems with formal reasoning I’m not sure I can care that much about that long tail, if 60% could be bunk. Look at politics, look at the US. In the southern US you had 80% of white males agree on something. Is this the long tail? Collective intelligence has reared its head in markets, especially mortages and real estate. It didn’t work out either. I remain highly skeptical of this version of “long tail” intelligence.\nThe Long Tail and expertise\n2010/01/15 at 11:05 am\n\n\nNeilfink08.wordpress.comx\nIn reply to Anonymous. Yes, those ideas are good – I use TiddlyWiki and Git for my work. However, re-reading these notes or recreating workflows from commits done months in the past is not quite what I was getting at. What you really need to do is document why certain constants are used, why you are excluding things below a certain threshold, and so on. Interestingly I think these small but sometimes very important decisions are very hard to pick up in peer review.\nA better scientific notebook\n2010/01/11 at 8:03 pm\n\n\nAnonymous\nI recommend a tool that can tagged and timestamped notes. Sometimes these lab notes help quite a bit. I also recommend using version control for everything you’re doing and carefully documenting each commit. This commit document is your research logThe tools are there all that is needed is the will power on your part.\nA better scientific notebook\n2010/01/11 at 5:24 pm\n\n\nAnonymous\nHere’s how you solve a problem on wikipedia or the Internet at large, watch and learn: http://www.google.ca/search?hl=en&source=hp&q=perl+sucks&btnG=Google+Search&meta=&aq=&oq=perl+sucks 552,000 for perl sucks http://www.google.ca/search?hl=en&safe=off&q=C+sucks&btnG=Search&meta=&aq=f&oq= 28,100,000 for C sucks http://www.google.ca/search?hl=en&safe=off&q=Python+sucks&btnG=Search&meta=&aq=&oq=Python+suck 1,100,000 for Python sucks Obviously Python sucks 100% more than Perl sucks. It must be due to Python’s lack of lexical scope (note how a conclusion is made with a complete lack of evidence). While C sucks 28X more than Python.\nData and science in enterprise computing\n2010/01/06 at 7:44 pm\n\n\nNeil\nIn reply to Greg Wilson. Googling “Perl sucks” led me to this: http://rs79.vrx.net/opinions/computers/languages/PerlC/. Not one to engage on substantive issues, are you?\nData and science in enterprise computing\n2010/01/06 at 2:18 pm\n\n\nGreg Wilson\nYou say, “here is a trend in the software blogosphere to use one or two data points as solid evidence that “C sucks” or “Perl is unreadable””, but don’t provide a citation.\nData and science in enterprise computing\n2010/01/06 at 2:10 pm\n\n\nRui Curado\nYou may want to keep an eye on ABSE (http://www.abse.info). ABSE is a code-generation and model-driven software development methodology that is completely agnostic in terms of platform and language, so you wouldn’t have any trouble applying CBSE or any other approach you would like. The big plus is that you can generate code exactly the way you want. The downside is that you may have more work to do at first to build your templates. But this is a common scenario in all model-based approaches. After all, it’s “model-driven”, so you’ll have to build the models! ABSE allows you to capture your domain knowledge into “Atoms”, which are basically fragments of larger models you can build. ABSE is both declarative and executable. The model is able to generate code by your specification and incorporate custom code at the model level.Unfortunately, ABSE is still work in progress and an Integrated Development Environment (named AtomWeaver) is still in the making. Anyway a CTP release of the generator is scheduled for Jan-Feb 2010, so we’re already close to it.\nMy experience with model-driven development\n2010/01/01 at 7:22 am\n\n\nanonymous\nInvestigate the Common Lisp loop macro to see how hard it is to do simple things. Now scale up to MDA\nMy experience with model-driven development\n2009/12/30 at 9:59 pm\n\n\nFrank\nYou saved me!!!\nITunes bookmarks\n2009/12/26 at 6:13 pm\n\n\nmartin\nIt is a shame that common sense like “certain things are useful in certain contexts” don’t get twittered, redditted, blogged, digged, slashdotted\nI study software, not software engineering\n2009/12/26 at 3:23 am\n\n\nPrabhakar Karve\nI came to this blog by chance, but found lot of sense in what you are saying. While building and maintaining software, we need to take care of two diagramatically opposite systems, namely the production system and the innovation system. The production system needs to be predictable, repeatable, measurable, deterministic, hierarchial and low risk. On the other hand, the innovation systems are uncertain, exploratory, judgemental, ambiguous, cross-functional and high risk. By whatever name we call it, software engineering MUST help us balance these two systems in the most approrpiate way for a given situation.\nI study software, not software engineering\n2009/12/14 at 3:10 am\n\n\nWyatt\n@ManuelWeb design is not the same as developing for the Web. I’m not sure what the point of conflating the two is.\nI study software, not software engineering\n2009/11/30 at 2:20 pm\n\n\nNeil\nThe math is a little beyond the time I have, but … this seems to address the space issue — won’t a generator do the same thing? — but I don’t see how it handles the time issue. For N items I still need to do 2^N operations, don’t I? I really need to filter the subsets of a given solution, so I never need to operate on them. Thanks for the links.\nMy sample Google/Microsoft interview question\n2009/11/14 at 4:25 pm\n\n\nAnonymous\nKnuth’s Art of Computer Volume 4 on Combinatorics, Permutations and Combinations would give you the answer.As long as you are allowed to check each subset once and in order you can do this with the storage limit of a single job. How? Iterate through all combinations, one at a time, and then operate on each of those subsets. So as long as all you need to do is iterate and test, you’re good, you don’t need crazy memory. Donald E. Knuth, The Art of Computer Programming, Volume 4, Fascicle2: Generating All Tuples and Permutations. Addison Wesley Professional,2005. ISBN 0201853930. Donald E. Knuth, The Art of Computer Programming, Volume 4, Fascicle3: Generating All Combinations and Partitions. Addison WesleyProfessional, 2005. ISBN 0201853949. Michael Orlov, Efficient Generation of Set Partitions, http://www.informatik.uni-ulm.de/ni/Lehre/WS03/DMM/Software/partitions.pdf. also if you like perl: http://search.cpan.org/~fxn/Algorithm-Combinatorics/Combinatorics.pm\nMy sample Google/Microsoft interview question\n2009/11/14 at 4:04 pm\n\n\nNeil\nIn reply to Jon P. S is a set in P, I should fix that. The trouble is that f(1) = True and f(2) = True does not imply F([1,2]) = True. What I’m dealing with are requirements, so P is a powerset of requirements, and requirements might interact, e.g., requirement 1 makes it impossible to achieve requirement 2. We want to maximize the set of requirements we can achieve (where f() tells us if we can achieve that set). So we just remove subsets of it without knowing or caring what their individual results are. thanks for the comment!\nMy sample Google/Microsoft interview question\n2009/11/14 at 1:23 pm\n\n\nJon P\nHey Neil, I probably don’t understand the problem exactly because the following solution seems trivial. First off, I assume that f takes an element of P (the powerset)… that is, f takes a set. In your post you say it takes a subset, but that would mean f takes a set of sets, no? I assume not. Anyhow, if f returns true on set S then you say the we must remove all of the elements in the power set of S from P. Presumably f also returns true for all the elements of the powerset of S if it returns true for S. If so, then it returns true for each of the original set items (the ones that which P is the powerset of) that appear in any S which f(S) returns true on. So solution: just run f on every element in the original set of items and filter out the powerset of the set of items for which f returns true. E.g. if f returns true for f(1), f(3), and f(5), subtract powerset([1,3,5]) from P and you’re done. What am I missing?\nMy sample Google/Microsoft interview question\n2009/11/14 at 12:37 pm\n\n\nSteve\nWhile this reduces the hiss is also increases the resistance in the headphones so you will loose some high frequency detail in the output, people make expensive low impedance headphones for good reason. Another solution is to use an external (USB) sound out like the griffin imic. Apple should have grounded their sound card and isolated the output better, hopefully the next gen of MBP will be better designed in this respect.\nQuick Tip: Macbook hissing in headphones\n2009/11/10 at 3:20 pm\n\n\nNeil\nIn reply to Jorge. Yes, I’ll take a look at them soon. It does seem like the best way to plan is to conduct a meta-analysis of all the estimates to derive a best guess.\nDe-referencing climate claims\n2009/08/26 at 8:19 am\n\n\nJorge\nExcellent exercise, Neil, thanks for posting. I wonder if the figures in the other studies are much smaller due to economic considerations – for instance, dismissing deep offshore entirely.\nDe-referencing climate claims\n2009/08/26 at 7:57 am\n\n\nanon_anon\nYou may want to use VTD-XML (http://vtd-xml.sf.net) it has an extended version that supports documents up to 256 GB\nMSR: parsing large XML files\n2009/08/21 at 9:40 pm\n\n\nMike Malone\nDude. Freaking genius. I’ve been trying to find a solution to this problem for a while now, and this worked perfectly. Wish Apple would fix their shit, but this is a great hack. Thanks!\nQuick Tip: Macbook hissing in headphones\n2009/08/13 at 1:47 pm\n\n\nNeil\nAran, I guess my question boils down to whether one needs Big Design Up-Front (and if so, when). My bias is to say that it is almost never necessary.\nAre we missing something?\n2009/07/07 at 1:07 pm\n\n\nkarthik’s Software Guide\nwell,information is good. Is it applicable in real time application?\nSome thoughts on lean software development\n2009/07/07 at 3:33 am\n\n\nAran Donohue\nI don’t think it is a fair comparison. Your examples of large enterprisey things are designed in a certain way for good reasons. J2EE is big because it needs to satisfy every use case. They ask, “What technologies do we need to solve all these problems?” The agile world comes from the opposite direction. They ask, “What problems can we solve with these elegant, simple technologies?” Then they develop new simple technologies that fit nicely on that curve. As for processes, I think that canned processes have limited utility when applied to large projects. Agile needs you to have a “customer”who runs acceptance tests. A large project has dozens of different customer types. All the iterative processes fundamentally rely on short iterations. This becomes nonsensical when individual “features” are way larger than any reasonable iteration speed. As for a PhD to establish the benefits of the agile world, I think a better question would be to ask, “How do all these (smart) people NOT using these modern fancy tools manage to succeed nonetheless?”\nAre we missing something?\n2009/07/06 at 9:01 pm\n\n\nNeil\nIn reply to Aran Donohue. Personally, no. The blogs I linked to mention some examples: Microsoft, Sprint, various corporate IT shops. IBM has an ’embrace and extend’ lean initiative: http://www.ibm.com/developerworks/blogs/page/ambler?entry=lean_development_governance\nSome thoughts on lean software development\n2009/07/06 at 2:38 pm\n\n\nAran Donohue\nNice summary. Know anyone who uses lean for software?\nSome thoughts on lean software development\n2009/07/06 at 2:29 pm\n\n\nNeil\nA follow-up: Tom DeMarco seems to agree that the notion of ‘engineering’ is outdated, in a recent articleon IEEE Software. He thinks we should be focusing more on delivering value, where we’ve seen some amazing success, rather than precision and meeting deliverables.\nI study software, not software engineering\n2009/07/02 at 11:38 am\n\n\nJorge\nI don’t think this can be established without a detailed analysis of the context of the organization. It is easy to argue that either agile or sturdy (as I prefer to call it) approaches are better for some situation or other; I don’t think we even need more empirical evidence of this by now. The question becomes what should my organization do, given its particular context, and this is still a very open research question. Also note: we shouldn’t confuse the sturdy, large enterprise projects that often use J2EE, SOA, and so on, with the stodgy version of waterfall that is still widespread in academia, but nowhere in industry.\nAre we missing something?\n2009/07/02 at 7:54 am\n\n\nanonymous\nSearch for tool adoption. Process adoption has the same problems.\nAre we missing something?\n2009/07/01 at 10:14 pm\n\n\nSam\nHere in Manchester (UK) there are rather a lot of similar schemes to the one you mention in Toronto with cycle-strips down the side of a number of major roads in and out of the city. And it’s infuriating. There are the parked cars (which I was shocked to discover is actually legal according to local byelaws), the strips stop and start randomly at very short intervals, and worst of all much of the time the cycle provision is to share a lane with busses and taxis. Being cut up by irate bus drivers is insanely dangerous and I’ve had a number of hairy moments trying to get past on both the inside and the outside. By contrast, a recent trip to Paris revealed (for the most part) a much better engineered city, curb-separated lanes and a clear sense of priority for bikes. Your idea of dedicating a thoroughfare for cycle transit and Steve’s tales of Montreal’s bi-directional separated lanes sound like even better solutions yet. I always berate other cyclists who jump lights they are such fools.\nPointless: Bike lanes downtown\n2009/06/25 at 10:03 am\n\n\nManuel\nThe problem with the bike lanes in Montreal is that we pedestrians only see them when we are already on them, and then a cyclist will run over us. I think more enforcement is needed on cars invading bike paths, but I once read that Vancouver is getting ride of their bike lanes and actually making the bikes safer…\nPointless: Bike lanes downtown\n2009/06/10 at 10:42 am\n\n\nManuel\nI do not think a web designer is a software engineer, since he is using an application, the same way that a painter is not a chemical engineer since he is using a chemical product. I do agree in the term software engineer because you are building up an application, no matter how complex (or simple) this may be. A civil engineer may be designing the next CN Tower or just paving a driveway, that do not means that he can just become an “craftman” for the later task and do not apply his knowledge and discipline in the required amounts. Not all developers are scientifics, and change and inconsistency exists in many engineers fields, not only software!\nI study software, not software engineering\n2009/06/10 at 9:59 am\n\n\nNeil\n@Jorge: One of the things that I think about is relevance; it doesn’t seem to bother physicists, so why do we care? I wish I could close my eyes to the problem like some in the field do. I think the issue, like the CHASE workshop mentions, is that ultimately this is a human endeavour, and so research needs to reflect that.\nI study software, not software engineering\n2009/06/06 at 2:04 pm\n\n\nanonymous\nIt is a shame that common sense like “certain things are useful in certain contexts” don’t get twittered, redditted, blogged, digged, slashdotted. Only this extremism of opinions gets any play online, see Joel Spolsky, Jeff Atwood, Uncle Bob, etc. Common sense isn’t interesting or bloggable. No one wants to hear that XP is a really bad idea if you have very strict requirements, no one wants to hear that gee if you have requirements document already made maybe SCRUM isn’t so useful. No one wants to hear “maybe SCRUM and XP are working for you because you didn’t do anything before”. No one wants to hear this, thus developers will continue to be assailed by consultants pushing the latest greatest things. BTW you suck at SCRUM and you should hire me to tell you how you do everything wrong.\nI study software, not software engineering\n2009/06/06 at 10:49 am\n\n\nJordi Cabotmodeling-languages.comx\nThe fact that “we don’t do engineering the way they had hoped” does not imply that we shouldn’t do it. What we should be able to is to find out the right amount of “engineering” for each project (depending on the size of the project, the criticality of the domain\nI study software, not software engineering\n2009/06/06 at 9:40 am\n\n\nJorge\nGood post; thanks Neil.“I would really like to move academic research up this list.” –many of us do, but not that many want to drop what they’re doing and study research questions that practitioners actually find relevant.\nI study software, not software engineering\n2009/06/06 at 8:07 am\n\n\nNeil\nIn reply to Chris Siebenmann. Steve: totally agree. A German city I visited used metal bollards to create a path between the pedestrian zone and the parked cars. There are still problems at intersections, of course (not to mention the number of cyclists who ignore traffic lights).Chris: a valid argument, but all it takes is a few accidents in the bike lane for the timid to head back to their cars.\nPointless: Bike lanes downtown\n2009/05/22 at 10:05 am\n\n\nChris Siebenmann\nThe story I’ve heard about Toronto-style bike lanes is that their real advantage (and possibly their real purpose) is that they encourage more people to go out and bike because they make those people feel safer. In turn this may increase actual biking safety due having more cyclists on the road. I find myself sympathetic to this story; if nothing else, having bike lanes (or even ‘share the lane’ markings and signs) sends a signal that biking is expected and being accommodated.\nPointless: Bike lanes downtown\n2009/05/22 at 9:57 am\n\n\nSteve Easterbrook\nI completely agree. Toronto’s bike lanes are mostly useless. The only sensible way to do this is build bike paths that motor vehicles cannot drive or park on. Eg separated from the vehicular traffic by a raised curb. I noticed some of this in Montreal – they’ve taken a whole vehicle lane, built a curb to separate it, and put a bi-directional bike path in it. European cities get this right far more often.\nPointless: Bike lanes downtown\n2009/05/21 at 7:33 pm\n\n\nAre we there yet?\nAlways climbing, never arriving.\nWhy a Ph.D. is like ice climbing\n2009/05/08 at 2:05 am\n\n\nNeil\nIn reply to anonymouse. Yes, and I suppose the other would be a power-law distribution or something similar.\nBlue-collar compensation\n2009/04/28 at 3:49 pm\n\n\nanonymouse\nLike a normal distribution?\nBlue-collar compensation\n2009/04/28 at 1:50 pm\n\n\nNeil\nIn reply to George. Cool! If/when I wrangle enough people together we’ll be in touch.\nWorldwide game day\n2009/03/24 at 8:28 am\n\n\nGeorge\nI love roleplaying games and D&D (although not as much a 4th edition fan, I prefer 3rd. I started playing in 2nd many years ago.). Recently I’ve been on a Shadowrun (4th edition) kick, but I am looking for games to run or play in. I don’t know you I don’t think, but I know Jorge. I wasn’t aware of that gamestore or, sadly, Worldwide D&D game day otherwise I would have shown up! I live within a nice comfy 10-15 walk of there I think.\nWorldwide game day\n2009/03/24 at 1:10 am\n\n\nJorge\nSure, I’m interested. It’s been over six years since I played role-playing games, even longer for D&D, and I’m not familiar with the new edition. But I’d still like to try it out.\nWorldwide game day\n2009/03/23 at 10:47 am\n\n\nSteve Easterbrook\n…and of course most users just want to make sure that the system won’t be non-functional very often.\nThere is no such thing as a non-functional requirement\n2009/03/22 at 2:04 pm\n\n\nCarlos Castro\nGood geeky post! I think the distinction exists at a higher level – when you start eliciting requirements. At this point it is easy to see that some requirements specify functionality and others specify ‘qualities’. However, as you move along in the requirements engineering process, you ultimately have to drill down and decompose the NFRs to the point where you have operationalizations for those ‘qualities’. At this lower level they are equally as functional as the Functional requirements.\nThere is no such thing as a non-functional requirement\n2009/03/20 at 4:35 pm\n\n\nno name\nI suspect a lot of AGILE successes come from the fact they didn’t bother to DO ANYTHING before. I suspect if you add process to something that process can help with (like some aspects of software development) you might actually see gains. A lot of the pro-agile stuff you often comes from people who didn’t actually do anything before. Really the only clear agile success stories come from the authors of the XP series and even their pet project C2 was scrapped. I think anything is probably better than nothing because anything implies that your self-reflecting and thinking that you need to improve whereas nothing implies an adhoc process that you hope just works.\nOrganizational maturity and software development\n2009/03/17 at 11:44 am\n\n\nNeil\nIn reply to Jakub Narębski. Well, I think this sort of makes Abram’s point: if you have to read the manual each time, maybe the tool isn’t as intuitive as it should be.\nTwo minor thoughts\n2009/02/22 at 12:02 pm\n\n\nJakub Narębski\n@Abram: “I checked out an older version of a repo”… and didn’t pay atention to the message from git, hmm…? “Eventually I found git-lost-found”… no need for such a low level tool. Ordinary “git checkout -b new branch name” should be enough, and if you lost a comit, there is always reflog: “git reflog HEAD”. In short: read the manual first, please…\nTwo minor thoughts\n2009/02/21 at 9:52 pm\n\n\nNeil\nIn reply to Abram. All true. I have a feeling that if your workflow is similar to Linux, it will work for you. If not, or you haven’t got a good sense for the workflow, you’ll be in trouble. Really, for working on my small projects, SVN will do as well.\nTwo minor thoughts\n2009/02/16 at 3:58 pm\n\n\nAbram\nMy problem with GIT is that the model is dirty and unclear. GIT is real deal software and it relies on an underlying model, but when I think I’ve learned the model I find I haven’t. Or the maintainers have not given me an interface to do so. Here’s one example. I checked out an older version of a repo and I commited. Where does that commit go? Turns out it was on an non-existent branch, so I couldn’t check it out without the exact commit ID, I couldn’t do anything with it. It was effectively lost. So then I tried to apply my knowledge of GIT, first I searched for commands which would let me query the children of a commit. Nope. Then I searched for commands which let me search for nodes with a certain parent. Nope. Eventually I found git-lost-found and recovered it. I didn’t have this problem in DARCS http://darcs.net/manual/node9.html Which was built ground up on a formal model of patching. Surprisingly the creator is not some formal models/methods buff, but a physicist. I don’t use darcs much anymore because it is rather slow.\nTwo minor thoughts\n2009/02/16 at 3:37 pm\n\n\nAugust\nI believe the subjunctive mood in English is still alive and kicking. It just seems dead because our verbs don’t have a lot of different endings like other languages. Except for the verb “to be,” the subjunctive is mostly undetectable in English. Only in the 3rd person singular (he/she/it) can you see it at work. Indicative: He GOES to a meeting. Subjunctive: I insisted he GO to a meeting. If I turned it around and said, He insisted I GO to a meeting, that would still be subjunctive, but it would be undetectable, no different from the indicative, I GO to a meeting. The verb “to be” tells the real tale, because the subjunctive verb form is “BE” for all persons and that doesn’t coincide with any of the indicative verb forms. Indicative: I AM here. You ARE here. She IS here. We (or they) ARE here. Subjunctive: He insisted I BE here. Or: They insisted we BE here, etc. Then there is always, “If I WERE a rich man.” “If she WERE a mermaid.” But again, you can’t hear a difference with “you, we, or they,” because they use WERE in either case. So, English speakers DO STILL use the subjunctive. It’s just hard to tell when we’re doing most of the time.\nThe subjunctive case and intentionality\n2009/02/12 at 10:52 pm\n\n\nChristian Muise\n“The wordle diagram is as close as most will get to actually reading it :)”A sentiment shared by most people regarding their master’s thesis — mine included :p.\nM.Sc. thesis wordle\n2009/02/10 at 10:18 pm\n\n\nNeil\nIn reply to Christian Muise. It’s been a while … “Towards Cognitive Support in Knowledge Engineering: An Adoption-Centred Customization Framework for Visual Interfaces”. The wordle diagram is as close as most will get to actually reading it\nM.Sc. thesis wordle\n2009/02/10 at 9:36 pm\n\n\nChristian Muise\nHeh. I’m impressed. What was the title of your thesis?\nM.Sc. thesis wordle\n2009/02/10 at 8:08 pm\n\n\nJorge\nGreat post. From a grad school perspective though, I think we are slow in catching up with developments in the real world. I remember the panel of a Computer Supported Cooperative Work conference where the panelists were beating themselves (and the community) down for not predicting nor reacting quickly enough to the greatest development of CSCW in history: the Internet.\nComputer science is doomed!\n2009/02/04 at 8:41 am\n\n\nJorge\nVery interesting points. I’m not sure the thalidomide analogy is appropriate. If a software company uses a disastrous ‘solution’, it goes out of business. Successful practices then replicate in a process similar to evolution. So if something has been widely used for a while, is it not reasonable to assume it works sufficiently well?\nEmpiricists vs constructionists?\n2008/10/25 at 10:37 am\n\n\nSherdim\nI join my soul cry to this post! I have searched for a couple of years for personal organizer/note clipper instrument. Although I had tried TW for a year ago I hoped to find something more semantics aware, more tunable and may be more intellectual (auto syncronizing, everything compatible etc.)Just every described step was done! Instead Tomboy I tryed WikidPadAt the end I found MGTD, based on TW, and though those organizing conception is not very suitable for me, the tagging scheme and compatibility of TW have led to the decision. I am a researcher too. So my specific needs in organizing everyday operations I hope to do step by step with JavaScript plugins which are easy for TW. About its not-standard wiki format: There is a standard plugin for RSS-export. It can be tuned so every edit will generate RSS feed prepared for publishing or importing into everything. Good luck!\nResearch note-keeping\n2008/10/14 at 10:21 am\n\n\nPM Hut\nThe 68% is highly subjective, because the definition of failure is. What is failure, is it a dead project? or is it a project behind schedule and/or overbudget and/or with lower quality/less features than scoped? or is it a internal assessment from the stakeholders? I’ve seen 30%-40%-50%…90% failure rates, almost all those stats target the IT sector, other sectors have much lower failure rates, but usually, in non-IT cases, the failure is of catastrophic proportions…\nRequirements and business project management\n2008/10/08 at 2:43 am\n\n\nVic Geemind-mapping.orgx\nI have a site giving a database of information management tools with thumbnail samples. It includes mind-mappers, concept mapping software, outliners and a number of other graphical tools. You can select to see just those for a specific OS, so you could choose just Mac to see those. If you use one of the browser-based ones, you’ll be pretty well cross platform whatever machine you want to use. Bubbl.us sounds as if it might be for you – it allows disconnected sections, a web, or hierarchical structure – depends how you feel. Not sure if it allows long enough notes for you though. An academic-slanted one is Sematik. This has a mind-mapping base but is aimed at producing finished documents, which may solve your ‘never find myself returning to those notes’ problem. Vic http://www.mind-mapping.org The master list of mind mapping &information management software\nResearch note-keeping\n2008/09/19 at 10:52 am\n\n\nSandy\nNice post! Tomboy does support tagging at the API level, and in fact Notebooks are just a special kind of tag. We found that for our users, notebooks were a more useful concept, especially considering how fast and easy note search is. That being said, plenty of people want a regular tagging UI, so don’t be surprised if an add-in shows up one of these days. We experimented a lot with it before deciding on Notebooks, so there’s even old code floating around in SVN for interested parties. You may also be interested in using Conduit to sync your Tomboy notes. Though I haven’t used it myself it seems to be a popular approach if you can’t set up your own ssh or webdav server. All that being said, TiddlyWiki is a great tool and I’m glad you’ve found something that works for you!\nResearch note-keeping\n2008/09/19 at 9:10 am\n\n\nB. Shaw\nGreat post! Another web app you didn’t mention is Springnote (www.springnote.com). It has a ton of features great for note taking and collaborative tasks. You can quickly get on Springnote, edit notes, and then log off. It’s that easy.\nResearch note-keeping\n2008/09/19 at 3:58 am\n\n\nJorge\nI think your reading of this is correct, both in that Microsoft would like to provide better abstraction mechanisms for developers, and in that UML is pretty much off the radar here. I’ve seen lots of diagrams here, actually, in the few weeks that I’ve been around. But they’re mostly used for informal communication and as a flexible abstraction tool. In comparison, UML 2.0 is stodgy and cumbersome, and MDD is as far from being an abstraction as code itself is. I hadn’t heard of the Oslo project before. I know it wasn’t even mentioned in Bill Gates’ farewell ceremony when he and Ballmer discussed the future of Microsoft.\nUML – Poised for takeoff?\n2008/07/11 at 4:14 pm\n\n\nroy\nJournals, in my opinion, have much more impact and relevance, firstly because many of the journal papers are selected from best papers presented at conferences, and secondly, because the review process is more thorough and constructive.\nRanking software engineers\n2008/05/12 at 5:41 pm\n\n\nKukladlinkx\nBut you are say, that this idead is bad?\nModel drift\n2008/05/08 at 1:09 am\n\n\nJerash\nfrom my limited (practical only) experience it was always about getting something done to give the impression of progress to clients – seems cynical but the phrase “we are 95% complete” is very misleading on its own – what are the metrics? and what are the criteria for success should be every client’s question – how you teach that is a tough problem\nOn Software Schools\n2008/04/27 at 11:39 pm\n\n\nshailly\nInteresting….keep it up.\nOn Software Schools\n2008/04/21 at 6:22 am\n\n\nJorge\nGood post. I agree with you, although I see where Fowler is coming from: the idea that there are best practices leads to certification, which leads to bureaucracy and inefficiency. I think identifying the context is key here, and you mention this in the end. For each team, customer, and software project, there is probably one best approach, or school of software, as Fowler calls them. We don’t know them yet; as scientists we hope to discover them. For now, stating that all schools might be valid for some context is the best we can do.\nOn Software Schools\n2008/04/13 at 9:58 am\n\n\nTarah Wheeler\nThank you very much! I’m a big audiobook fan, and I had no idea how to do this. If only iTunes would get a clue and add a help feature for “bookmark”! Doesn’t that sound reasonably intuitive to you??\nITunes bookmarks\n2008/04/07 at 2:47 am\n\n\nAnthony Brown\nCheers!\nITunes bookmarks\n2007/11/22 at 7:27 pm\n\n\nJerash\nSounds like a wonderful trip and you are encouraging to others who might be considering a similar endeavor.\nCamping in France: some tips\n2007/11/15 at 2:25 pm\n\n\nJorge Aranda\nComplexity is surely a factor, but I think it is only one of several. There are companies that deal with very complex projects and yet shun the systematic use of UML. Microsoft is one of them. In a paper I discussed recently, Cherubini said that in his discussions with Microsoft engineers, he found that “most of the diagrams had a transient nature because of the high cost of changing whiteboard sketches to electronic renderings. Diagrams that documented design decisions were often externalized in these temporary drawings and then subsequently lost”. (Oh and Owen produces some excellent hammers! Mostly everyone in Toronto seems to be using them\nAn explanation for UML usage statistics?\n2007/08/25 at 12:34 pm\n\n\nMama Ernst\nBetter late than never. Nice way to show some photos with captions. I assume it was faster than using blogspot, each pic took about 90 seconds to upload, and when you’re in net cafe, that can be costly!\nExperimenting with a Flickr photo browser\n2007/08/15 at 11:52 am\n\n\nTomasunitedstates4africa.web.netx\nWell done Neil – give me a break with the lack fo transparency. I would really like to check my diplomacy at the door, but will resist temptation. It would seem our man Jorge has revealed the answers to your questions and despite Harris’ protestations to the contrary, re protecting donor confidentiality (when are donors ever NOT interested in publicity, except when there is something to be hidden!), the NRSP is a sham. No surprise the NP publishes their propaganda.\nWhat are they hiding?\n2007/08/12 at 12:56 am\n\n\nJorge Aranda\nApparently you’re not the only one that can’t get this information, and it seems you never will: From Wikipedia: “The NRSP has been criticised on the basis that it is an industry-funded body which presents itself as a grassroots organization, an activity referred as Astroturfing. Harris rejects this criticism but refuses to reveal the sources of NRSP funding.”From SourceWatch: “According to an October 16, 2006, CanWest News article, journalist Peter O’Neill asked Harris about who financially backs the NRSP. O’Neill reported that, according to Harris, “a confidentiality agreement doesn’t allow him to say whether energy companies are funding his [the NRSP] group.” [25] Subsequently, Harris stated that there was no “confidentiality agreement”. He also insisted that “it is normal for non-profit entities like NRSP to protect the privacy of supporters by not publicizing contributions.”And from desmogblog.com: “Two of the three Directors on the board of the Natural Resources Stewardship Project are senior executives of the High Park Advocacy Group, a Toronto-based lobby firm that specializes in “energy, environment and ethics.” (…) Timothy Egan, is the president of the High Park Advocacy Group, and a registered lobbyist for the Canadian Gas Association and the Canadian Electricity Association.”\nWhat are they hiding?\n2007/07/09 at 11:45 pm\n\n\nAnon\nThanks, I was going nuts trying to figure this out before I found your post. ThAnkS alot!!!!!!\nITunes bookmarks\n2007/06/13 at 12:14 pm\n\n\nDavid Locke\nWhy are requirements non-deterministic? The non-determinism originates in the fact that the inputs change and that there is rarely a single source for those inputs. Given the elicitors role in asserting the elicitor’s own needs over those of the requirements source, the problem starts there. Change the elicitor, change the requirements. Then, you have the efficency focus of requirements elicitation process, which says get all the requirements sources into a room and have them decide what the requirements will be, which in turn means sacrificing requirements to utility functions, and washing away the cultural differences of the users, so the requirements can be efficently developed. This later issue injects politics and sociology into the elicitation process, thus so much for science, and hurah for requirements volitility. The software world has and continues to ignore culture, aka meaning, in the systems it develops. Silo busing, integration, and real-time data warehouses is just making it worse. Developers are much less expensive now, so lets put an end to efficent development as a goal, and let’s put a stop to generic software that fits no particular user and drives up those negative-use costs that accountants don’t account for, but every CFO feels. If you elicit requirements from one person, they stop being non-deterministic, they make that person efficent, and they are much less volatile. The elicitation process is not to blame for the non-determinism. By way of a practical application, say we are developing a cost accounting system. Which will it ultimately be: traditional, activity-based, or throughput? Throughput being the most recent, it has the least amount of adoption by older accountants, more by the fresh out of school accountants, and the least amount of adoption by management, aka the utility function. So it won’t be throughput, unless the management has a vocal early adopter amoung it. Activity-based will probably win, because it is the majority paradigm today. Traditional will probably lose, because it is basically fostered by the older accountants waiting for the age-based layoff, or retirement. Still, all three will be elicited. Then, they will be fought about with the utility function winning the day, expertise being ignored, and knowledge being destroyed. Once the development of those requirements are commissioned, the requirements politics will continue to churn the requirements. The end result will be a mess, which will require participants from all three paradigms to build Excel spreadsheets to compensate for what the system won’t do–time wasted invisibly, except on the actual, non-accounting, bottom line, the negative use costs. This more than determinism is the real problem with the requirements elicitation process.\nRepeatability in requirements elicitation\n2007/03/13 at 9:25 am\n\n\nJorge\nIt is definitely a great movie; I’m glad you liked it too! The translation indeed would be “The Labyrinth of the Faun”. The faun, which I believe in this case stands for “male fairy”, would be the character Pan, not the deity. Also, in Spanish “pan” means bread, so “El Laberinto de Pan” would have a very bizarre meaning for us –The Labyrinth of Bread. As for whether the Spanish Civil War resonates with Mexicans: during the war, many Spaniards escaped to Mexico, and were generally prosperous there. So among many middle- and high-class circles in Mexico, the Spanish Civil War felt close and is still relevant. I assume Guillermo del Toro, the director, was raised in this environment: he has one other movie set in the same period, The Devil’s Backbone. In general, Mexico has a complex relationship with Spain: To put it simply, Mexicans see Spain both as our “cultural mother” and “the thief who took away all our gold and destroyed our native civilizations” (everyone’s views are more elaborate than that, of course, but that gives you an idea).I’d also recommend another recent movie by a Mexican director: Children of Men. The best movie I’ve seen in a long time.\nMovie recommendation\n2007/03/07 at 9:19 pm\n\n\nAl\nPresident Ahmadinejad’s views are summarized on this website: ahmadinejadquotes.blogspot.com\nRationality\n2007/02/17 at 5:24 pm\n\n\nJorge\nI agree as well. Remember that we’re stuck with the ‘software engineering’ label almost by accident -because the organizers of a crucial conference decades ago felt we should be striving towards the engineering ideal. Since then people have tried to match software development to engineering processes, never satisfactorily. This is the second reccomendation I get about Cockburn’s book –I should check it out.\nCockburn on the 3 pillars of software engineering\n2006/12/13 at 5:07 pm\n\n\nMarkus\nNeil – I liked your post. Especially the aspect that agile approaches put the responsibility back to people (as opposed to processes). That might explain the advantage that agile approaches have over process-heavy ones. I think though that especially large, complex and/or safety critical software systems will continue to rely on process-heavy approaches. Beyond a certain point, I would assume that agile approaches are too unstructured, too unpredictable and too unorganized. Regarding your pole analogy (which I liked very much). You might want to use the north pole in your story though: Because it (exclusively) consists of ice (as opposed to the south pole), it is there where you are actually confronted with a moving target – which is the ice masses floating around the (north!) pole.\nCockburn on the 3 pillars of software engineering\n2006/12/08 at 10:41 pm\n\n\nNeil\nGood points about information overload. I think tagging might be useful here, esp as the arXiv categories are VERY coarse-grained. This is especially true in the multi-disciplinary era we seem to be in. My other, more cynical observation would be that your second reason is equally applicable to conference proceedings! Perhaps to a lesser degree, though.\nThe role of arXiv in information science research\n2006/11/16 at 3:26 pm\n\n\nJorge\nInteresting post, Neil. I think the first problem you talk about (just how useful is this paper?) is more important than it seems, for two reasons. First, even considering only peer-reviewed work, there are hundreds, perhaps thousands, of relevant papers produced every year, in any field. Just keeping on top of these involves a great deal of time. Without that initial filter that is peer review, we’d have four or five times as many papers to consider. The second reason is that we often can’t determine the usefulness of a paper until after we’ve read a considerable fragment of it. For example, a paper might seem helpful until when, halfway through, you spot a glaring methodological mistake that makes the paper completely unreliable. It’s unrealistic to perform this careful, critical thought on every technical report or paper that hasn’t passed the peer review test. On the other hand, it’s still a worthy initiative, especially if you’re only browsing for very specific topics –peer reviewed publications are terribly slow, and something like arXiv could speed things up quite a bit.\nThe role of arXiv in information science research\n2006/11/16 at 3:10 pm\n\n\nSotirios\nActually, the “Liaskos corollary” was a comment posted to the AI mailing list by a prof! The funniest is, I think it has been experimentally confirmed by some MIT folks.\nThe Three Laws of Academic Publishing\n2006/11/09 at 12:45 am\n\n\nJorge\nThese are great – and “Salay’s query” is devastating. You guys must have had a lot of fun when these came up.\nThe Three Laws of Academic Publishing\n2006/10/16 at 5:25 pm\n\n\nJorge\nAgreed, Neil. I’m pretty convinced that software cannot (perhaps should not) be engineered, at least not in the way we understand engineering to be. It’s a controversial idea, and considering the young age of the field, perhaps premature. The gut instinct, however, suggests it’s right.\nMy dangerous idea\n2006/01/09 at 12:31 pm\n\n\nChris Fogelklou\nCompletely, totally off topic, Neil, but I am just lettin’ you know (in case you didn’t already know) that our 10 year high-school reunion is this Saturday, Jan 14, 2006!I decided to google you because I didn’t know if you were still at UVic… Apparently not! Judging from the look of this blog, you have done rather well for yourself, at least academically (the financial part usually comes later in that case You wouldn’t be able to brag about that, unfortunately (It’s a no boasting party – teehee.)Congrats on the PhD work! Hope you can make it, but I won’t get my hopes up since you’re out in T dot. Cheers, Chris members.shaw.ca/spectrum1995reunion\nMy dangerous idea\n2006/01/08 at 10:59 pm\n\n\nAnonymous\nThanks for the info…Seem to work OK\nITunes bookmarks\n2005/12/08 at 9:19 pm\n\n\nYaroslav Bulatov\nPeople haven’t proved that there isn’t a killer algorithm for learning to predict relevant websites, but there’s something related, the No Free Lunch theorems, http://www.no-free-lunch.org/Basically an algorithm that works exceptionally well in one area, is bound to be exceptionally bad in another area. This implies that a good algorithm for google would have to be hand-taylored to it’s prediction task, and that we would have to hand-code a lot of the knowledge that it’s trying to extract from the data, into the algorithm itself. An interesting application of “no free lunch” philosophy is the idea of anti-learning. Instead of designing an algorithm to perform well on realistic data, we can design an algorithm to perform badly on random data, to the same effect.,\nPeter Norvig talks at UofT\n2005/11/23 at 2:53 pm\n\n\nStephen Fickas\nYou note the following question: Big question: is the RE goal achievable? I.e. can a sufficiently detailed analysis actually produce something perfectly in line with user expectations? Almost certainly not. This is why, I conjecture, CWA type analysis is needed, since it provides a domain model (as RIck is suggesting) that the user can use at the KBB level, to deal with unanticipated/unexpected events. If no, perhaps just use a small goal model and define a tight system boundary. We’ve started to run up against this question. Might want to look at my RE05 paper off http://www.cs.uoregon.edu/~fickas. I think the notion of a “goal attainment scale” is interesting. Steve\nGADG: Requirements monitoring\n2005/11/21 at 12:35 pm\n\n\nPiotr Kaminski\nFirst, a quibble: Reef is all about integrating code and UML with programmer interference. Automated RE systems are a failure except for very specific scenarios. However, Reef also treats developer attention as a precious resource, and tries to leverage their effort far more than other tools I’ve seen. The very simple difference between Reef and MDA is that in MDA, the model must be a complete representation of the system and therefore bears the full complexity of the implementation. In Reef, the model is an abstracted representation of the system, with designer-selected details elided to enhance high-level understandability without affecting the implementation. MDA seems to be based on the belief that, with sufficiently powerful transformation facilities, a model can be both abstracted and complete; I think this is not achievable in the short term, and in the long term is essentially equivalent to a new higher-level programming language. Which would certainly be a good thing, but history tells us that 1) it’s not likely to be graphical and 2) it is not wise to let every fool invent his own language (as MDA seems to encourage).\nTrac and me\n2005/09/11 at 1:32 am\n\n\nNeil\nInteresting work, however, I think it’s getting away from the ‘Simple’ part of RSR to suggest developers use calculus of any kind. I’m skeptical any system designer will be able to grasp that.\nRSR: implementing Really Simple Requirements\n2005/06/14 at 4:23 pm\n\n\nJon Hall\nDear Neil (?),I, too, begin with Jackson’s framework. Here at the Computing research Centre of the Open University we have a very active Problem Frames group, and that includes foundational research on ESR-tuples (we use WSR (World) or KSR (Knowledge):-).I’d like to bring your attention to some work we have done; the web-page is http://computing-reports.open.ac.uk (look for Jon G. Hall and Lucia Rapanotti), some of which addresses the issues you raise in the first paragraph. I would also be interested to see if our framework (see http://computing-reports.open.ac.uk/index.php/2005/200505) work could extend to cover problem solving in Agile methodologies. If you could be interested, please email me. Very best wishes, Jon\nRSR: implementing Really Simple Requirements\n2005/06/14 at 5:07 am\n\n\nNeil\nSure, certainly. I should really post the various hacks I’ve collected over the years somewhere. I may have to do some tracking on that one as I’ve moved computers since then.\nACSE and Portland\n2005/02/24 at 1:17 am\n\n\nTom Heath\nHi Neil,Your RDF output styles for EndNote sound really interesting and may be useful in a little project I’m working on. Would you be prepared to share the EndNote style file, or walk us through how you did it? Tom.\nACSE and Portland\n2005/02/23 at 12:07 pm\n\n\nAnonymous\nNeil, my legs are twitching just reading this. Correct your time inthe opening paragraph, you added 10 minutes! Dad and I were thinking of you as we did our runs on Sunday from 9:15 to 9:45, just about your ‘wall’ time. Too bad the telepathy didn’t work. Do you recall swimming races you did in school? I think it was with St. Michael’s. Boring to watch, but at least the spectators were warm and there was always a snack bar! Love from Mum\nThe Niagara Marathon\n2004/10/26 at 5:41 pm\n\n\nNeil\nIn reply to Alexy Khrabrov. If you use Scrivener and MMD export, anything between is passed directly through as Latex. So for complex tables or math I prefer this approach to the MMD footnotes and Unicode math symbols. e.g. *&lt;!– \\begin{table}[h] \nSome notes on integrating Mendeley, Scrivener, MultiMarkdown and (Xe)Latex\n2010/09/23 at 9:16 pm\n\n\nAlexy Khrabrov\nHow exactly do you surround LaTeX with HTML comments? An example would be great!\nSome notes on integrating Mendeley, Scrivener, MultiMarkdown and (Xe)Latex\n2010/09/23 at 2:29 pm"
  },
  {
    "objectID": "posts/2016-04-22-on-scams-new-engineering-track.html",
    "href": "posts/2016-04-22-on-scams-new-engineering-track.html",
    "title": "On SCAM’s new “Engineering Track”",
    "section": "",
    "text": "This year SCAM, the Working Conference on Source Code Analysis and Manipulation (located in Raleigh, NC, Oct 2–3 2016) includes an engineering track, as described here. The CFP is available here. This track will be co-chaired by myself and Jurgen Vinju. In this post I want to briefly explain what an engineering track is and why you should submit to it! 1"
  },
  {
    "objectID": "posts/2016-04-22-on-scams-new-engineering-track.html#footnotes",
    "href": "posts/2016-04-22-on-scams-new-engineering-track.html#footnotes",
    "title": "On SCAM’s new “Engineering Track”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIncidentally, I agree with and support the ICSME co-chairs’ statement on the anti-LGBT legislation in North Carolina.↩︎\nThat definition is from Roel Wieringa’s excellent design science book.↩︎\ncan itches be unknown? I may be mixing metaphors.↩︎\nIncidentally, I am not a big fan of the term “industry” or “industrial”. Maybe it is my location in Pittsburgh, but it conjures up steel mills and heavy machinery. The other problem is the term “industry” is used as a catch-all for a widely different set of folks, from a 2 person startup to a Fortune 500 company or DOD agency. I prefer research vs practice. Not a huge fan of “real-world” either, since we all live in the real world. Presumably.↩︎"
  },
  {
    "objectID": "posts/2013-05-23-some-advice-on-doing-a-postdoc-in-software-engineering.html",
    "href": "posts/2013-05-23-some-advice-on-doing-a-postdoc-in-software-engineering.html",
    "title": "Some Advice on Doing a PostDoc in Software Engineering",
    "section": "",
    "text": "Post-doc positions in CS are a growing part of the research landscape, as seen in this figure from the CRA:\n[caption id=“” align=“alignnone” width=“445”][](http://www.cccblog.org/wp-content/uploads/2011/05/figure21.png) CRA 2011 Research positions[/caption]\nSo if you are a senior doctoral student, should you take a post-doc offering? Herewith a few tips based on my own experience (7 year doctoral student in Toronto, 1.5 years postdoc at UBC, now with SEI as researcher).\n\nFigure out your long-term goal. Do you want a research intensive faculty post? Or a teaching-intensive job? Industrial research lab? Industry development job? I would not bother with a post-doc if I wanted a programming job (even a PHD is a hindrance here, in most cases).\nThink about networking. Most of the people who get the top jobs in the field are well-connected to the main community, via supervisor connections, industry internships, collaborations. You will need to secure 3-4 people who will write highly of you. You need to get onto the short list. I am assuming you already understand what the bar is for high quality research.\nAvoid teaching positions if you want to do research, and vice-versa. I did a dual position, and while I love teaching, research suffered. There is plenty of time to think about teaching later, and in my experience, top research schools almost never ask about teaching experience. On the other hand, if you apply to teaching universities, then demonstrable ability to manage a large class should serve you well, as will good evaluations.\nEvaluate how well the position will accommodate your existing research contributions. You simply do not have time in the standard 2 year postdoc to shift your research interests dramatically. To me this is one big difference with life science postdocs, where you get lab experience and the positions are typically for 3-4 years. Ideally, you will be able to submit to ICSE, CAV, FMCAD, PLDI etc. immediately after starting. I’m not convinced that hiring committees are at all sympathetic to any gaps in your publication record.\nBe realistic about the quality of the lab’s past research. Are they publishing in the top venues? Is there a history of collaborative work, where you might be able to tail along on a paper as you start your post?\nFinally, all the other criteria: is it a nice city? Are the people friendly? What is the salary? 50K Canadian is at the upper end for most positions.\n\nThese tips are mainly pragmatic and from a research hiring point of view, where the main reason to do a post-doc is to improve your publication record and increase your social network. That being said, one of the things I most liked about my postdoc was meeting and working with great people. That sticks with you longer than any paper submissions will.\nEDIT: I’ve been reminded of another criterion, namely, does your partner (if any) support your decision!"
  },
  {
    "objectID": "posts/2012-12-19-a-stitch-in-time.html",
    "href": "posts/2012-12-19-a-stitch-in-time.html",
    "title": "A stitch in time…",
    "section": "",
    "text": "This blog post from the excellent complexity blog Godel’s Lost Letter is on the theory behind branch and bound search. One of my favourite things about this sort of analysis is how it it can eliminate, with mathematical certainty, hours and hours of programming effort. Consider this statement:\n\nThere is an issue of being odd or even, which matters but not hugely, since pruning the bottom layer is not so valuable.\n\nI have spent many hours working on problems that might fall into the “matters, but is not so valuable” category. A few hours of analysis might well have saved me a lot of trouble."
  },
  {
    "objectID": "posts/2016-01-21-the-marginal-utility-of-testingrefactoringthinking.html",
    "href": "posts/2016-01-21-the-marginal-utility-of-testingrefactoringthinking.html",
    "title": "The Marginal Utility of Testing/Refactoring/Thinking",
    "section": "",
    "text": "Andy Zaidman had an interesting presentation about test analytics. The takeaway for me was that a) people overestimate their unit test engineering (estimate: 50%, reality, 25%). But b) the real issue is convincing a developer that this unit test will improve the quality of the code. In other words, like with technical debt, or refactoring, or commenting, the marginal utility of adding a test is perceived to be low (and of course the cost is seen as high). Each new individual test adds nothing to the immediate benefit (with some exceptions if one is following strict TDD). And yet each one requires switching from the mental model of the program to the one of Junit frameworks and test harnesses.\nThe issue is not whether testing is good or bad, but rather, which testing is most useful. It seems unlikely to me that the value of individual tests is normally distributed but rather power-law form (i.e., that there are a very few extremely high value tests). And this isn’t just about testing; indeed, most activities with delayed payoff—refactoring, documenting, architecting—likely exhibit the same problem. It is hard to convince people to invest in such activities without giving them concrete proof it is valuable. You just have to look at the default examples for Cucumber, for instance, to see that the vast majority are trivial and easily grasped without any of the tests. Similarly, “code smells are bad”, but bad might just mean they look nasty, while having little to do with the underlying effectiveness of the code. It isn’t technical debt if it never causes a problem. It isn’t a bug if it isn’t worth fixing it.\nIn new work we are starting with Tim Menzies, we are trying to understand the inflection point beyond which your decisions add little incremental value (i.e., stop adding more tests). The good news is this is easy to spot in hindsight; the challenge is to take those lessons and determine this before doing hours of pointless work. The direction we are taking is to try and capture the common patterns the key decisions share (in the testing example, perhaps this is bounds testing). Ultimately, we hope to provide advice to developers as to when the marginal utility falls below a threshold (i.e., stop testing!)\nThe other point is the over-reliance of software engineering on hoary folklore. Things like “some developers are 10x as productive”, or “80% of bugs occur in requirements”, tend to be statements that are derived from a single study, conducted in 1985, on 3 large scale defense projects, but have somehow made their way down the years to become canon. Ours is not the only field to suffer from this, of course. But when capable developers refuse to pay 200$ a year to join the IEEE Digital Library, it seems to demonstrate a firm commitment to ignorance."
  },
  {
    "objectID": "posts/2017-05-10-active-learning.html",
    "href": "posts/2017-05-10-active-learning.html",
    "title": "On Active Learning in Software Engineering",
    "section": "",
    "text": "I’ve read 2 papers recently (references) about using active learning to improve classification for software engineering.\nActive Learning is the idea that, if we have a feature space with instances, for the classification task of labeling an instance either “A” or “B”, there are clumps of points that clearly are As, and another clump that are clearly Bs. In between, however, the boundary is unclear. Some instances will be equidistant from both centers of mass, and the classifier will struggle to properly classify it. Active Learning (AL) quite simply picks what are hopefully the “most useful” (for improving the classifier) points for human labeling, which reduces the amount of (possibly) redundant labeling humans have to do (since the labeling time is the most costly part of creating a classifier).\nIt turns out that so far this active learning approach for software data is not too successful. I think there are 2 main reasons.\n1) We don’t have much data. Classifiers do better when they see more instances. In the 2 studies I read, the number of unclassified instances was measured in tens of thousands. Contrast this with most image recognition or information retrieval applications, which have orders of magnitude more training data. In the case of the ImageNet context, they also label substantially more instances (e.g. 150,000 labeled with 10 labels).\n2) More importantly, I think the task is fundamentally difficult. The Borg paper makes this clear; when human raters themselves cannot agree on a label, it probably won’t work any better with active learning. I think this is because some problems have fuzzy label boundaries for non-core feature reasons, while many software concepts are innately (ontologically) unclear. Think about labeling photos of house numbers. I’m pretty confident that any two humans would agree that instance X is a house number. We have clear and simple criteria for what a “number” is (intensionally and extensionally). The reason the classifier struggles is because of non-intensional properties of the data itself: perhaps a tree obscures the top of the 1, or a shadow is partly on the lower digits. In software data, that problem exists as well (e.g. someone talking about an old version of Rails). But for labeling an utterance as technical debt, or a performance bug, or a usability concern, there seem to be broad disagreements on the core discriminating features. If we talk about paradigms like distributed computing, is that an “architectural” discussion? What about a bug that results from not understanding an RPC service?\nWe’ve looked at some of this in our latest research on design rules. We found that while a majority of static analysis/code checker rules can be clearly distinguished as either design or not, there remains this stubborn middle tier that resist easy categorization. We think you can still make progress (after all, these rules may not even fire on your project). But it would be satisfying to have a more repeatable analysis.\n\n\n\nCategorizing rules\n\n\nThe conclusion of the Borg paper really seems useful for future work here. One, they say that the AL approach helps to pull out controversial instances, that then help build rater consensus. Two, using bootstrapping with more positive examples helps the AL improve its accuracy (in other words, there is still benefit to grinding out the labels manually – no free lunch, sorry!).\n\nReferences\n\nN. Van Houdnos, S. Moon, D. French, Brian Lindauer, P. Jansen, J. Carbonell, C. Hines, W. Casey. “Human-Computer Decision Systems for Cybersecurity”, Presentation. https://resources.sei.cmu.edu/asset_files/Presentation/2016_017_001_474277.pdf\nBorg, M., Lennerstad, I., Ros, R., Bjarnasson, E. “On Using Active Learning and Self-Training When Mining Performance Discussions on Stack Overflow”, arXiv:1705.02395v1. Preprint of paper accepted for the Proc. of the 21st International Conference on Evaluation and Assessment in Software Engineering, 2017."
  },
  {
    "objectID": "posts/2015-12-22-a-model-of-software-quality-checks.html",
    "href": "posts/2015-12-22-a-model-of-software-quality-checks.html",
    "title": "A Model of Software Quality Checks",
    "section": "",
    "text": "Software quality can be automatically checked by tools like SonarQube, CAST, FindBugs, Coverity, etc. But often these tools encompass several different classes of checks on quality. I propose the following hierarchy to organize these rules."
  },
  {
    "objectID": "posts/2015-12-22-a-model-of-software-quality-checks.html#footnotes",
    "href": "posts/2015-12-22-a-model-of-software-quality-checks.html#footnotes",
    "title": "A Model of Software Quality Checks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m not sure how to ‘noun’ this adjective …↩︎"
  },
  {
    "objectID": "posts/2011-10-25-what-i-learned-at-uoft.html",
    "href": "posts/2011-10-25-what-i-learned-at-uoft.html",
    "title": "What I learned at UofT",
    "section": "",
    "text": "My dissertation is nearing approval (touch wood) and I have started a new position as a Post-doctoral Research Fellow and lecturer at UBC. I wanted to summarize my experiences in grad school as a reflective exercise. I often found I got down on myself during the process: it is an incredible challenge to acquire a research Ph.D. at one of the top-10 computer science schools in the world. I’m extremely proud of my past selfs for persevering and allowing 2011 Neil to reap the reward, as it were. ’Cause 2006-2008 Neils put up with a lot of sh*t.\nThese are all things I knew nothing about when I arrived for my PhD in 2004:\nLanguages and Tools\n\nPython (matplotlib, numpy, networkx)\nLisp (SBCL and Clozure)\nGit and SVN\nTwitter, Facebook, Wordpress\nMendeley\nLatex + Scrivener + MMD3\nFlash / Flex\nSqlite\nRuby/RSpec/Rails/Gems\n\nFrameworks\n\nMDE with Eclipse and GMF\nLogic programming with ATMS\n\nTheories\n\nKnowledge representation\nPropositional logic\nNon-monotonic logic\nLatent Dirichlet Allocation\nAgile software development\n\nMiscellaneous\n\nA smattering of Italian (via ferrate means “iron ways”).\nThe importance of good coffee.\nHow to change a diaper at 4am without turning on the lights.\nFriends: It’s somewhat trite to say that it was the people you met who you will remember, but that’s true. I think one of the most enjoyable things about moving on to a new experience is the idea that there will be all of these people you will call friends in five years, of whom you know nothing now.\nHow to prepare and defend a 60,000 word opus starting with no knowledge of the area, no relevant background skills, and little to no published work. In that context seven years seems about right.\n\nThanks to my wife and family for getting me to this point. They found the right combination of “why are you doing this again” and “you can do it”."
  },
  {
    "objectID": "posts/2021-06-03-The-Scientific-Method-2021-edition.html",
    "href": "posts/2021-06-03-The-Scientific-Method-2021-edition.html",
    "title": "The Scientific Method 2021 edition",
    "section": "",
    "text": "The typical Science workflow is something like\nA - Related work: build a Prior about a real world problem, like pulsar formation\nB - Fieldwork: Collect data about that problem in the ‘field’\nC - Model: Build a model of the problem\nD - Posterior: Generate a posterior/data output\nE - Analyze: draw inferences about A, updating the prior if necessary.\nWe can have several types of problems - “debt” - in this process.\nNote that in A/B, we are in traditional science. This is the stuff you would learn in Astro 200 - how planets form, how to take readings, etc.\nI want to focus on C, as an issue of scientific technical debt. I think a lot of challenges have to do with understanding the tradeoff between the scientific challenges in A/B/E, and the engineering focused challenges in C and D. Ultilatemly to do good science - to design vaccines, to predict climate adaptation approaches, to better understand pulsars - we need to have all of these phases working efficiently. Since I know about C that’s where I’ll focus."
  },
  {
    "objectID": "posts/2021-06-03-The-Scientific-Method-2021-edition.html#making-models-accessible",
    "href": "posts/2021-06-03-The-Scientific-Method-2021-edition.html#making-models-accessible",
    "title": "The Scientific Method 2021 edition",
    "section": "Making Models Accessible",
    "text": "Making Models Accessible\nThere are two dimensions (or more) to this question. The first is about using models and building them; the second is about querying them."
  },
  {
    "objectID": "posts/2021-06-03-The-Scientific-Method-2021-edition.html#model-usability",
    "href": "posts/2021-06-03-The-Scientific-Method-2021-edition.html#model-usability",
    "title": "The Scientific Method 2021 edition",
    "section": "Model Usability",
    "text": "Model Usability\nWe now see tons of tools that help with modelling. Here are some illustrative examples\n\nFortran, in Global Climate models\nPython, in Ralph Evin’s building sims\nAstropy\nCommercial: Tableau, RStudio, Metabase, https://core.hash.ai/@hash/wildfires-regrowth/9.8.0 are moving from building dashboards and visualizations into more complex modelling support. But like dashboards, the challenge is less about using bars vs lines, and more about the A/B and E parts: what problems do I need to learn about.\n\nOne challenge in the move to more complex models is of course the inflection points: the places where the models fail to capture the real world. It is trivial to build a predator-prey model; to build one that accurately captures the dynamics of wolf/elk dynamics in the Mackenzie valley is entirely different, and probably the validity of the model is only understandable by maybe 100 people in the world. Increasingly the challenge in peer review is not about (or not just about) the problem’s relevance or significance, but whether the data and model support the claim. Technical debt in science is a massive concern if we are going to rely on large, difficult to verify datasets for our claims.\nHow easy it is to build the model:\n\nShow examples of RStan, Fortran, Hash as dealing with the problem of “building the model”"
  },
  {
    "objectID": "posts/2021-06-03-The-Scientific-Method-2021-edition.html#accessing-model-outputs-digital-twins",
    "href": "posts/2021-06-03-The-Scientific-Method-2021-edition.html#accessing-model-outputs-digital-twins",
    "title": "The Scientific Method 2021 edition",
    "section": "Accessing Model Outputs: Digital Twins",
    "text": "Accessing Model Outputs: Digital Twins\nBryan Lawrence blog post\nThe idea that in SKA you might not get raw data"
  },
  {
    "objectID": "posts/2016-09-08-day-hikes.html",
    "href": "posts/2016-09-08-day-hikes.html",
    "title": "Day Hikes",
    "section": "",
    "text": "A list of long, high vertical day hikes I have done and wish to do. I think looking back the most common theme to all of them was “bring more water”.\n\n\n\nHike Name\nLength\nElevation Gain\nElevation\nNotes\n\n\n\n\nBlack Tusk\n29km/18mi\n1740 m/5700’\n7600’\nHighly exposed last section up remaining volcanic core. See details\n\n\nLions\n16km/10mi\n1280m/4200’\n5427’\nI remember when we did it in 2001 or so there being little to no trail markers. Trail page\n\n\nTriple Crown (Finlayson, Work, Gowlland Range )\n? Maybe 10mi/16km\n? Probably around 1000m/3200’\n1375’\nI couldn’t find details on this, but the gist is to hike up Finlayson, go down the backside, then back up along the Gowlland Tod ridge above the inlet, then up Mt Work at the end.\n\n\nHalf Dome\n26km/16mi\n1450m/4800’\n8839’\nCables! Now need permits to go. Trail page\n\n\nMt St Helens\n16km/10mi\n1370m/4500’\n8366’\nPermit needed. Painful boulder climbing and loose scree from middle to end. Insanely exposed rim of crater. Trail page\n\n\nGolden Ears\n24km/15mi\n1500m/4900’\n5630’\nSome freaking jackrabbit passed us going up and was heading back down before we summited. Even in June had plenty of snow that made the top risky without ice axes and/or crampons. Trail page\n\n\nMt Thar\nNo idea. Took about 6 hrs.\nYak Mtn is listed as 1640’ for prominence, so I’d guess no more than 1200’ for Thar.\nYak: 6693’\nTrip report This one is in 103 Hikes in the SW BC, highly recommended.\n\n\nMonte Bondone, Trentino, IT\nAbout 12 hours.\nTrento centro is 636’, so nearly 6500’ of elevation gain (seems high to me)…\n7150’\nI started in Vela where my flat was. The Italian Alpine club chapter - S.A.T. - has a good trails site and maintains the helpful markers. You can take a cable car back to the river from Sopramonte to shave a few minutes off.\n\n\nMt San Jacinto\n30km/19mi\n1700m/5600’\n10,833’\nTBD! Trail page"
  },
  {
    "objectID": "posts/2015-01-06-measuring-programmer-productivity-is-futile.html",
    "href": "posts/2015-01-06-measuring-programmer-productivity-is-futile.html",
    "title": "Measuring programmer productivity is futile.",
    "section": "",
    "text": "(I’ve typically posted long-form entries but so infrequently … )\nThe arguments and debates about 10x productivity in “programmers” rage on (this time to defend/reject H1B visas). This debate is doomed to never be concluded. I think the reason why is nicely captured in Andrew Gelman’s post on p-values: they work best when noise is low and signal is high, something which can never be the case when we talk about productivity. As he says,\n\nIf we can’t trust p-values, does experimental science involving human variation just have to start over?\n\nGiven a random sample of (let’s say) Microsoft software developers, can you devise a test that would show the statistical differences? Are you convinced you would have high power? A big effect size? One person online (via HackerNews) says it is about tool competence. But the recent Latex/Word study leaves me doubting even that conclusion (although I have trouble with that study too, which just reinforces my overall point).\nMore importantly, I think this calls into question almost any controlled experiment in software engineering. Short of replicated results, I’m skeptical the information content is very high. Instead, I would like more qualitative research. Why do people say there is this difference? What traits are important? Can they be taught? How do we share productivity improvements? These questions seem much more important than trying to attach a p-value to whether one group is better than another."
  },
  {
    "objectID": "posts/2015-02-23-thoughts-from-a-codefest.html",
    "href": "posts/2015-02-23-thoughts-from-a-codefest.html",
    "title": "Thoughts from a CodeFest",
    "section": "",
    "text": "This past weekend was the Steel City Codefest. The idea is that community non-profits present some problem for which an “app” would help them, and coders spend 24 hours coming up with some solution. It was a lot of fun. site-urlYou can see our team’s solution at http://citipark.herokuapp.com. –&gt; Our challenge was to create an easier way for people to find the city of Pittsburgh’s GrubUp food program, which offers free lunch and breakfast at 80+ sites around the city in the summer (sadly, a lot of Pittsburgh youth are food insecure).\nWe didn’t win the challenge, but I learned a lot on the way.\nWe created tons of  technical debt : code clones, code comments, no testing, no design. It was code as fast as possible, get it working, fix the obvious user facing bugs. We shipped. But even during that 24h span the design hit us, as it became harder to change things since logic and UI were wrapped together. Even something as trivial as renaming a media folder became a massive headache. We had no tests, so any change had to be “tested” by running the app and running through a few scenarios. Error handling was likewise left for later work, so if faulty input was entered the whole thing crashed.\nIt took a long time simply to do infrastructure setup: what Github repository, what web host, what database, how do we communicate together. Part of it was this was only my 2nd time building a node application, so I was unfamiliar with its internal expectations and capabilities. Things like “don’t send headers twice” caused problems for me that a more experienced developer would not have had. In a 24h period this stuff needs to be like riding a bike, so deciding on a framework that I had little experience in was costly. It’s like going to a marathon without having trained at all.\nWe were three people: two coders and a designer/QA person. Three was the minimum, and it really wasn’t enough. There were tasks like entering data into the database (the Citiparks staff provided excel spreadsheets) that took me a few hours but had zero payoff. In a codefest, data quality is not a factor in the judging (the judges don’t come from the clients). In an enterprise situation, the data is probably as important as anything else, but here it was wasted effort, and sample data would have worked fine.\nWe had somewhat of an idea how things would work, but wireframing it beforehand, and being much clearer about what steps were necessary, would have been better (you could not write code before, but this sort of sketching was allowed). A simple design plan and backlog would have been easier to work off, and help to resist the temptation to simply start hacking away. A number of times I would push back from the table, and say to myself “do I even need to do this?”\nWriting code this way is a great way to learn these lessons. I have a number of academic publications about finding requirements, for example, but it is only when you do it yourself that you realize how much is lost between the quick IM conversations you have with teammates and the actual issue tracker. I do wonder, however, if these 24h codefests promote ‘code first’ over the value of design. For example, my sense is that a lot of what we did simply wouldn’t work in an enterprise environment: there are design guidelines, authentication, security, data integration, lifecycle maintenance concerns, none of which you have the luxury to spend much time with. The cool thing about the Steel City event, however, is that the organizers do make a series of $10k grants available, in order to take the app to a more integrated and polished version.\nIt was a great event - very well organized, with great food and volunteers. And the Citiparks staff were amazing, sending their director and deputy director to do user testing at 7pm Saturday, and bringing amazing treats for us twice during the event. It also focused on an underserved area, in my view: social justice and not-for-profits. Many have quite simple needs, that in many cases amount to adding data to a Google Map, but even that is beyond their budgets."
  },
  {
    "objectID": "posts/2021-04-15-Refsq.html",
    "href": "posts/2021-04-15-Refsq.html",
    "title": "REFSQ Panel session on Open Data and RE Education",
    "section": "",
    "text": "Together with Alessio Ferrari, I organized a panel at the well-regarded conference on Requirements Engineering: Foundation for System Quality, which is a mouthful but really nice working session on RE that has always nicely blended practice and research. It also was the first place to accept one of my papers so I will always have a soft spot for it, and for Essen, industrial city or not.\nThe purpose of the session was to encourage open data packages in the context of RE Education (the aim, I think, is to have subsequent OpenRE tracks at REFSQ change theme). We got excellent submissions and accepted three packages, which we have hosted at the existing repository of the RE Education and Training (REET) workshop.\nAfter the short talks on the packages, we turned to a panel with the theme of “RE in the age of COVID”. Our hope was to collect some experiences from the attendees (40 or so) on how they approached RE education, and RE in general, during the COVID induced shift to online learning. We definitely got that and more generally, I think it was a cathartic session to commiserate and share with others the challenges of the past few terms.\nA few lessons I drew from the discussion:\nParticipants were a bit torn on the need to completely redesign the curriculum vs sticking with the previous content. “Maybe my course was boring and remained boring!” Of course in some cases just getting online was sufficiently challenging to prevent major redesigns. In general projects worked well in both formats. There was some thought that lab exercises worked better, since it was easier to checkin—students were in a fixed location!\nLearner styles or perhaps preferences (since “styles” are not a thing) was something we didn’t have a good handle on. Some students definitely prefer online. But no one had data on who is doing better, and who is doing worse, online vs offline. For example, students seem to appreciate recorded lectures, but mostly to replay/relisten. The downside is it is harder to get questions in a recorded lecture. Even in more normal settings, students are not always sold on flipped classroom. Then there is the problem of video content: should we re-record videos? In the end it’s about making the content relatable and helping them through the struggle with it. Thus there is no substitute or tech fix for the need to demonstrate empathy, use multiple learning techniques, and I suppose the things we know work well in teaching regardless of venue.\nThere was growing recognition that—online and off—bringing some levity and enthusiasm, such as via Serious Games, was critical to keep people engaged in the Youtube and Netflix era. Dan Berry, who has a charismatic personality, suggests we think about being a comic. But of course that will not work for everyone. Even during ‘normal’ lectures, it is not uncommon for 60 students to turn into 3-4 actively participating, 15-20 in class, and some coming merely to sleep.\nIn other settings, participants acknowledged a need to maybe step away from the computer and do a lecture from outdoors, away from disruptions. The 1 year mark of the pandemic led to a let down in formality, with less emphasis on formal backgrounds and acknowledgement that it was ok for it to be weird.\nThere were a few folks who ran hybrid classes, where the university allowed some reduced subset to attend class. The popularity of this depended greatly on the perceived safety level. There were often technical challenges e.g. mic’ing students and sanitizing mic before answering a question, how to get video that made remote participants still feel engaged (e.g., eye contact).\nThe final takeaway was about student well-being. Birgit Penzenstadler, who studies this in her research, emphasized the need to meaningfully check in and get beyond the “how are you doing” question. This is, as she points out, precisely an RE elicitation problem, e.g. “what is the biggest impediment” you are currently facing. We agreed that for most of us, the reality of needing to meaningfully check-in was hitherto unappreciated, and something that is completely independent of learning modality or current global crises. Meaningful checkins are certainly something I will be including in my own teaching practice, online or off (but I hope in person!)."
  },
  {
    "objectID": "posts/2013-05-30-knowledge-and-complexity.html",
    "href": "posts/2013-05-30-knowledge-and-complexity.html",
    "title": "Knowledge and complexity",
    "section": "",
    "text": "Somewhat inspired by +Rob England, I tried a mapping of Rumsfeldian terminology to Cynefin (yes, i know this predates the SecDef!).\nKnown knowns - either a simple or complicated case. If simple, we do it routinely. E.g. landing an aircraft in good weather.\nKnown unknowns - we have a plan for accommodating it. E.g. landing an aircraft in Edmonton with high crosswinds.\nUnknown unknowns - we are in a complex domain and we have to see how things should work with experiments e.g. edge of envelope flying. One of the things that the Kennedys made clearer for me is how experiments have to be well conceived, in particular, by controlling variables properly (see their SysEng Journal paper).\nUnknown knowns - we didn’t realize we could plan for this but now that we sense it, we can delegate to existing routines, or perhaps adapt to it. That pathway is available but unused. The example would be .. looking through the manual and realizing the system can actually do this (maybe the Apollo 18 case? There they did the exaptation that Snowden often mentions.)\nSnowden would no doubt criticize my limited understanding, but there is some use in seeing how these frameworks co-exist, for me at least.\nUpdate: Dave Snowden replies with a pointer to his HBR article with Cynthia Kurtz in which “knowns” are discussed. Summary: _simple _contexts are the domain of known knowns, complicated contexts are the domain of known unknowns, since experts are required. Unknown unknowns are the domain of complex contexts. Interestingly, they categorize the Apollo 13 case as being in the complex context. In the sense that there was no clear answer, that makes sense, but to me, it also highlighted that idea of unknown knowns: that is, these skilled engineers did “know” the answer (since the astronauts survive), but not consciously. So we could perhaps characterize that as relying on the “expert within”."
  },
  {
    "objectID": "posts/2016-07-19-columbuss-heilmeyer-catechism.html",
    "href": "posts/2016-07-19-columbuss-heilmeyer-catechism.html",
    "title": "Columbus’s Heilmeyer Catechism",
    "section": "",
    "text": "I have no idea if Columbus had to have his “India Expedition” proposal peer-reviewed, but here is my interpretation of it according to the ever-popular Heilmeyer catechism.\n\n\nWhat are you trying to do\nI would like to sail to India and bring back gold and spices for the Crown of Spain.\n\n\nHow is it done today\nCurrently no one has sailed west. Everyone takes the trip east, around the Cape of Good Hope. Most of these people think the world is flat and that heading west would cause us to fall into space.\n\n\nWhat’s new in your approach\nI will head west. I’m pretty sure the Earth is round, and we can reach India from the west in less time\n\n\nWho cares?\nA faster trading route to India, monopolized by our mapping skills, would generate 1 million Real a month for the royal treasury.\n\n\nRisks\nThere is a lot unknown about the middle of the Atlantic, including rumors from the Vikings that some colder land is in between. My math may be off in calculating the circumference of the Earth. I am not a great sailor. We may encounter fierce alien tribes.\n\n\nCost and schedule\nFor 1000 Real we can outfit four boats with sailors, supplies, and weapons (note: of course Columbus would never get all he requested, either!). We plan on a quick 1 year voyage to India, and one more year back.\n\n\nCheckpoints for success\nWe plan to see India after 2000 nautical miles of sailing. While measuring distance at sea is currently impossible, after 3 months we expect to sight land. If not, we will head back."
  },
  {
    "objectID": "posts/2015-09-25-garbage-in-garbage-out.html",
    "href": "posts/2015-09-25-garbage-in-garbage-out.html",
    "title": "Garbage In, Garbage Out",
    "section": "",
    "text": "My dad had this great cup from one of his vists to COMDEX (ostensibly to keep up with the latest in the tech world, which at the time COMDEX represented). It said “Garbage in, garbage out” (GIGO), and then had the name of some failed software company.\n\n\n\nGigo Mug 2\n\n\nI read a great blog about intermediate targets and over-optimizing what you measure (Hawthorne’s law) and the unintended side effects. Then I watched a presentation on the future of data visualization.\nThe commonality to me is this undesirable focus on the simple over the complex. So a dashboard can in a glance tell you how fast your car is going, which is useful because it maps to two concerns you have as a driver: obeying the speed limit laws, and maximizing your time in the car. I should say, “maps directly”, because as an indicator for these two concerns, speed is pretty much a 1-1 mapping. But consider a car indicator with a much poorer mapping to your concern: the “distance remaining” gauge new cars have. This tells you that based on some model of past driving behavior, you can expect to travel X more miles before the fuel runs out. The problem is this indicator is no longer a simple mapping. You have a (possibly non-linear) model of past behaviour (and no idea how far back the model goes); possibly inaccurate sensors (e.g., depending on temperature, the amount of fuel actually remaining might change); and finally, it is predicting future behavior (you will continue to drive to work tomorrow, and not go on a long distance highway drive).\nIn much the same way I think this fascination with metrics and dashboards confuses construct for concern. If I’m the government CIO, my concern is the value for taxpayer money each project is generating. But the dashboards are probably showing me constructs like estimated time to completion or lines of source code. Furthermore, and this is the data/info vis piece, those constructs are being mapped into visual variables using some arbitrary function. For instance, the decision to turn something from green to red might be based on a simple threshold chosen by an intern.\nIn broad strokes, constructs like source lines of code can, I think, be useful: logarithmically, perhaps, in the sense that a system with 100 thousand lines is more complex than one with only 10 thousand.\nThis typically isn’t how dashboards work, though. Thinking about numbers seems so innately arithmetic (5 is halfway between 1 and 9, not 3) that we cannot comprehend how little the dashboard is telling us. The Japanese lean movement has a nice word that captures what i think needs to happen: genchi genbatsu, “management by walking around”. In a factory, just looking at metrics for production speed and inventory is not the whole picture, and so long ago the Toyota production system creators learned that you had to actually walk the shop floor to see for your own eyes.\nThis is perhaps harder in the non-physical world of software, but I think for most of us we have a sense of project performance innately: are meetings productive? When was the last time you saw a working piece of code? Do you get quick answer to emails? While it is possible to metricize these things, probably it won’t help much more than buttonholing someone in the hallway."
  },
  {
    "objectID": "posts/2018-06-16-satt.html",
    "href": "posts/2018-06-16-satt.html",
    "title": "Bayesian Hierarchical Modeling in Software Engineering",
    "section": "",
    "text": "At MSR18 in Gothenburg, I presented my work on using Bayesian inference to set software metrics thresholds. We want to set thresholds because for many software metrics, like coupling between objects (CBO), a single, global metric value (“all software objects with this value or below are maintainable”) is nonsensical, if only because programming language choice is important. So we want to tailor threshold values to some contextually relevant value (e.g., perhaps all Java code should be X or less). The question I answered is how we do the tailoring, given some contextual features.\nIn this case, the contextual features I was looking at were Java files categorized by architectural role in the Spring framework, derived from a paper by Mauricio Aniche and others.\nThe bottom line of this new approach is that we can use Bayesian inference and hierarchical models to perform a simple regression and get a 50% drop in root mean squared error (RMSE).\nThe more interesting conclusion from a methodology point of view is that hierarchical modeling with Bayesian inference fit software engineering data very well, and is straightforward to model given modern probabilistic programming languages. I followed a similar approach to the one detailed in this blog post on hierarchical modelling with PyStan. There are two key ideas.\n\nUse a combination of global data as regularization over the detailed, local model. In this case, the global data comes from all the different Java projects. The local model is the specific coupling metrics for one particular project. The effect is to allow each individual project’s slope and intercept values to vary by some amount dictated by the global values.\nWe model this using a Bayesian approach, which means we will condition our likelihood based on the data we are observing, and use that to estimate a posterior distribution. I really like this approach because it forces you to think about your prior distribution (what should the metrics distribution be?), and also because it produces a posterior distribution, and not a single point estimate. A distribution is much more flexible for making inferences than a point estimate (e.g., we could say “set the threshold where &lt; 75% of the probability mass lies”).\n\nThis was also a fun project to do from an open science approach. I used Jupyter as my notebook throughout the project, and my notebook and the paper/presentation are both available."
  },
  {
    "objectID": "posts/2023-05-14-bots-in-SE.html#tasks",
    "href": "posts/2023-05-14-bots-in-SE.html#tasks",
    "title": "On Bots in Software Engineering",
    "section": "Tasks",
    "text": "Tasks\n\nDetailed tasks: Mylyn style tasks that have to do with a specific problem like finding a bug, refactoring a method\nHigh level tasks: get an overview of the system in order to see how it is progressing. This bot might send a weekly update on lines of code added. Sort of exists as Github’s various visualizations.\nDesign tasks: help me understand how the software will respond to quality attribute requirements."
  },
  {
    "objectID": "posts/2023-05-14-bots-in-SE.html#tasks-bots-can-help-with",
    "href": "posts/2023-05-14-bots-in-SE.html#tasks-bots-can-help-with",
    "title": "On Bots in Software Engineering",
    "section": "Tasks bots can help with",
    "text": "Tasks bots can help with\nAre bots just “API endpoints”?\nBots are api calls plus “vocal tics” like the fridge in Silicon Valley\n\n“It’s bad enough it has to talk. Does it need fake vocal tics like ‘ah’?\n“The tics make it seem more human,” Dinesh tells Gilfoyle.\n“Humans are shit,” Gilfoyle replies. “This is addressing problems that don’t exist. It’s solutionism at its worst. We are dumbing down machines that are inherently superior.”\n\nThe challenge in these systems has always been that entry level knowledge is extremely easy to retrieve, but going deeper is way harder (kind of like self-driving). For perhaps 80% of the interactions online, the bot can manage. But it is in the details that bots get stuck and need to call for the operator to step in. We saw something like this with expert systems. Coding something that can advise people to call their doctor when they report a fever of 102 or higher is pretty simple. But the complex explanations as to what is causing the fever are fairly intractable (explanation is usually thought of as NP hard, after all). Getting the knowledge in to solve the problem (basically, all the heuristics and learning that an experienced GP would have) is very expensive - the KA bottleneck. This is probably less costly now with deep learning. But the other bottleneck is the reasoning. Even if we have that knowledge, inference to multiple competing explanations is very expensive. Recommending the most common explanation—such as viral ear infection in a toddler—is what bots basically do now, but in many cases there isn’t a clear common explanation, or there is no clear set of symptoms to diagnose."
  },
  {
    "objectID": "posts/2023-05-14-bots-in-SE.html#bots-for-td-reduction",
    "href": "posts/2023-05-14-bots-in-SE.html#bots-for-td-reduction",
    "title": "On Bots in Software Engineering",
    "section": "Bots for TD reduction",
    "text": "Bots for TD reduction\nOne area we see a lot of activity is in static code analysis to find rule violations. There are more rules than programmers could reasonably want to use; code quality checks, syntax warnings, code smells, etc. The problem in fact is these warnings annoy developers. At Google they had a scheme where the code checks would be rejected if they had more than 10% false positives which developers could vote on. These tools generate multiples more warnings than developers actually take action on.\nHow could bots help? Well, the main issue I notice is the need for interactivity. Bots could easily process the boring problems in one shot (fix all trailing commas), but more importantly the bot could be an interface to the tool, instead of the common approach which is either a dashboard with TMI, or some simple weekly report. The bot instead could be a text interface to the tool itself, and run predefined queries or make new queries, adapting on the fly as the situation demands it. So bots are good for rapid re-contextualization which you do not see with dashboards, which require sophisticated analysis to configure.\nA bot is also automatable so it could delivery weekly updates to the developer without him or her having to do anything with that info. Again though it seems like we are pushing the complexity - what reports do I really need - into another interface. The tough problem in TD presentation is to figure out exactly what context underlies the data and only show that.\nWe need to find problems and generate data\nWe need to filter and store the data\nWe need to query the data\nWe need to visualize the data.\nBots don’t make any of these easier per se…."
  },
  {
    "objectID": "posts/2016-11-04-ko-splash.html",
    "href": "posts/2016-11-04-ko-splash.html",
    "title": "Thoughts on Amy Ko’s “PL as …” keynote",
    "section": "",
    "text": "Amy Ko had a great presentation at a conference on programming languages (PL), that he also video taped for a wider audience.\nI’d always thought of PL as “things”, or material. The program was the interesting bit; the PL was the material that constructed it. But I guess as I extend that metaphor, it seems clear that it falls short. Cedar, for instance, is a material, and the building is the interesting thing. But cedar has intrinsic properties as well. You can bend it without cutting it to make a box.\n\n\n\nbentwood box\n\n\nIt weathers beautifully due to the oils it contains, so you can make shingles and siding out of it. It burns very easily. If we extend the definition of cedar to include the tree itself, we can make canoes, rope, hats, spears, and so on. Cedar was a crucial part of Northwest aboriginal culture.\nAnd so in translating that thought back to the PL world, it seems clear that PL also has this. The syntax of Java vs C in ease of learning the language. The ecosystem of Javascript vs Clojure in building apps. The culture of web programming languages vs scientific programming languages. And so on.\nThe one quibble I have—clarification, to be more accurate—is the slide on definitions, values and community weighting toward the end. The \u0003implication → goes one way for a reason. That is, because we chose to focus on PL as math, we have, as a result, a lot of focus on the value of certainty. But that isn’t to say because we value certainty, we focus on PL as math. In fact the reason for ‘valuing’ this value are complex and systemic: most CS departments started with math graduates, most CS departments still contain math-heavy disciplines like theory and machine learning, we want to show correctness and soundness, and math is the way to do it. So it isn’t to say that PL community does not value equity, or the others, but rather that equity is hard to prove, and PL academics function in a math world.\nFinally, Amy had this great list of what form PL takes, and associated research questions, which I’ve shamelessly duplicated here so people can more easily copy it.\nProgramming languages as ….\n\npower\n\nwhat responsibilities does knowing PL come with?\nhow does PL corrupt?\nshould democracies distribute it?\n\ndesign\n\nwhat tradeoffs are made?\nwhat is a “good” PL design process?\nhow can we rapidly prototype PL?\nwhat are PL aesthetics?\n\nmedia\n\nwhat message is enabled by PL?\nhow does PL facilitate expression?\n\nnotation\n\nwhat can PL not model?\nwhat info can PL not share?\nwhat makes a PL learnable?\n\ninterfaces\n\nhow can PL convey what is possible?\nhow do we make PL usable?\nwhat feedback must a PL provide?\n\nmath\n\nwhat does PL correctness mean\nhow to prove PL correct?\nwhat in PL is equivalent?\n\nlanguage\n\ndo PL have ambiguities?\ndo PL shape how we computationally think?\n\ncommunication\n\nShould PL model developer intent?\nshould PL express intent to developers?\n\nglue\n\nwhat makes a PL a good adhesive?\nwhat materials do PL adhere to?\n\nlegalese\n\nwho should interpret code legally?\n\nare programmers lawyers?\n\n\ninfrastructure\n\nhow do PL decay?\nhow should we maintain PL?\nis PL a public good?\n\npath\n\nshould gov’t create the path?\nhow do we make PL equitable?\nwho should go down this path?"
  },
  {
    "objectID": "posts/2015-01-20-frameworks-libraries-and-dependencies.html",
    "href": "posts/2015-01-20-frameworks-libraries-and-dependencies.html",
    "title": "Frameworks, libraries, and dependencies",
    "section": "",
    "text": "I’ve been doing a little thinking about frameworks lately. They fascinate me as 1) a realization of the vision of ‘pluggable software’ and reusable components desired since probably 1968; 2) what you are getting into when you rely on one. This is prompted by this great post on libraries vs frameworks.\nNow, we’ve used libraries for ages, viz. glibc etc. And the notion of ‘code that someone else wrote and maintains that I need’ was likely established in the design of Unix and pipe and filter architectures. But it really seems like the past 10 years have seen this wonderful explosion of creativity in writing ‘little libraries’ for various different systems.\nI’ll take a common example. I’ve previously used Node.js for a small visualization I did for my brother’s work on genetics (in progress!). Although an academic, I like to try to stay on top of things, so I tried out Node, the Javascript web server. Now JS itself has at least60+ frameworks and libraries, and that list doesn’t even include Node or some of the ones I’ll describe below. This is amazing considering although JS has been around a long time, only recently (would we say JQuery is the prototypical case?) has this explosion happened.\nThe trouble is that like the Cambrian explosion, some of these libraries and frameworks are doomed to extinction. If you are BigCo, that makes choosing one very tricky, in addition to the licensing and security questions you will need to ask.\nConsider. I wrote the application for the Node server, using Express as a web framework (that means it automates some of the routing and layout of files and directories for you). To get to the database I used the Node PostGreslibrary. To do UI I relied on JqueryUI and Stylus for CSS, with Jade for templating. Then I used Morgan for logging, Gulp to automate the style generation from the Stylus files, and was toying with D3 to do the display. Not to mention I need a Platform as Service from Heroku, so I have their command line tools installed as well.\nSo that gives about 10 different libraries to run this app. On the plus side, they automate a ton of code I no longer have to worry about, letting me focus on the key value-add of the app (realized in the SQL code I write and custom request handling code).\nBut I just upgraded to Express 4, and they’ve broken the back-compatibility, so I must now understand what the changes mean and how to retrofit them. Who maintains these libraries? Will he or she keep updating it? These are by no means new questions, but I think what has changed is that now it is very hard to avoid using them. And once you commit to it, re-architecting for the problems you will inevitably face with leaky abstractions seems challenging, because everything is deeply connected. You cannot just drop in a new back end server with the same libraries.\nNow imagine that multiplied times 10 years and instead of my simple app, a mission critical information system, and you start to get a sense of the problem that legacy applications can pose. Fortunately, I work at a place with lots of experience solving those problems, so give us a call if you need help!"
  },
  {
    "objectID": "posts/2014-06-13-software-research-shouldnt-be-about-the-tools.html",
    "href": "posts/2014-06-13-software-research-shouldnt-be-about-the-tools.html",
    "title": "Software research shouldn’t be about the tools",
    "section": "",
    "text": "It comes down to essential vs. accidental complexity, as outlined by Fred Brooks. What we research is new ways to ‘nibble’ at the accidental complexity: new languages (GO, Swift), new abstractions (Actors vs. functional programming in distributed systems), new methodologies (random test case generation). It’s what nearly every story on Hacker News is about.\nBut ultimately, I think most problems come down to two factors: the problem complexity itself, and the team tackling it. To me, many of the problems highlighted as software/IT failures, like the FBI registry, have nothing to do with a lack of good tools or techniques. These are ultimately management failures: scope creep, poor leadership, insufficient budget, too much budget, negative work environments, etc. It is ‘executing’ that is the problem, not the technology. How many errors have been caused by the US reliance on imperial units?\nLook at this quote by a senior VP at Oracle on failures in implementing CRM projects:\n\n[M]y comments apply to ALL CRM vendors, not just Oracle. As I perused the list, I couldn’t find any failures related to technology. They all seemed related to people or process. Now, this isn’t about finger pointing, or impugning customers. I love customers! And when they fail, WE fail.\n\nI’d be willing to say that software engineers have all the tools they need. We need some form of continuous integration and deployment, abstraction mechanisms to simplify the problem, tests to verify our solution, version control to maintain a history of changes, and some form of requirements (whiteboard, paper, spreadsheet, what have you) to keep track of what needs to be built. I don’t even think it particularly matters how you use those tools. If you have a mature organization and process then you can all into the following matrix (James Montier via Jonathan Chevreau):\n\n\n\n\n\n\n\nGood Outcome\n\n\nBad Outcome\n\n\n\n\nGood Process\n\n\nDeserved Success\n\n\nBad Break\n\n\n\n\nBad Process\n\n\nDumb Luck\n\n\nPoetic Justice\n\n\n\n\n\nBut just having the right tools, the good people, and a mature process is not enough to guarantee success, of course. You could be tackling a ‘wicked problem’. You could have a team of misfits and losers. You could have a manager who refuses to accept responsibility or make decisions. Most software research does not address those issues. I’m not convinced there is any research that addresses those issues: leadership, management, sociology… nothing can help when your team lead is having a marital crisis and can’t devote any time to product development."
  },
  {
    "objectID": "posts/2013-11-20-evidence-in-software-engineering.html",
    "href": "posts/2013-11-20-evidence-in-software-engineering.html",
    "title": "Evidence in Software Engineering",
    "section": "",
    "text": "This post is spurred by a line in a paper of Walker Royce, son of Winston Royce, he of the “waterfall model” (misunderstood model). He says\n\nwithout quantified backup data, our software estimates, proposals and plans look like long-shot propositions with no compelling evidence that we can deliver predictably or improve the status quo.\n\nBold text is mine, to emphasize this notion of evidence. The question then becomes what evidence is acceptable. And here I think we get into some hoary philosophical questions concerning truth in science (epistemology, really).\nI think one of the fundamental impedance mismatches in software engineering for large-scale systems, or software engineering in a systems engineering environment (e.g., airplanes, military software, ultra-large-scale software, safety-critical software) is that a number of people on those teams have a positivist view of evidence. They subscribe to the notion that sufficient “data” can show whether something will work or not. So if you design a missile system’s rocket engines, those engines either deliver the necessary thrust, navigability, etc. or they do not (actually, I think this is probably not the case in those so-called hard sciences either, but the point is that people from those domains think it is the case).\nAgile software delivery works much more from the falsificationist or even outright constructionist approach. I would estimate that the majority of agile practitioners believe in the test and refine approach, where you deliver an increment, test it to determine how well the ‘theory’ of that software matches reality, and then iterate. The key difference with the positivists being, of course, that there is no a priori evidence you can use to show things will work. It is hard to do simulations in Simulink or Matlab as to how well software will perform. This is why Alistair Cockburn calls software development a cooperative game. And some people, I would say, would go even further and treat software development as a post-modernist exercise in building the reality you want to see and dispensing with evidence altogether (“if it works, it is right”). Those people probably don’t get a lot of government contracts, however.\nBack to the quote: the issue remains, how easy is it to produce “compelling evidence” and what does it consist of? In Royce’s view, evidence takes the form of historical context for productivity, function points, etc. In that case we are almost measuring the team’s capability to deliver more than any particular artefact.\nAt some level, cynically one might say, there is a need to show the ‘evidence’ that will get the job or contract. People hire companies like Thoughtworks because they have a track record of getting the job done to people’s satisfaction. We don’t much care how long it took, or how many lines of code were written, if the software was valuable.\nWhich is fine, but as an engineering discipline one would like a bit more to chew on."
  },
  {
    "objectID": "posts/2016-03-07-on-using-open-data-in-software-engineering.html",
    "href": "posts/2016-03-07-on-using-open-data-in-software-engineering.html",
    "title": "On Using Open Data in Software Engineering",
    "section": "",
    "text": "I recently reviewed data showcase papers for the Mining Software Repositories Conference, and I’m co-chair of the Engineering track (subsumes datasets, tools, approaches) for the SCAM conference1. I’ve worked with a number of different datasets (both openly available and closed) for my research efforts. This caused me to do some reflection on the nature of empirical data in SE.\nWe’ve had a nice increase in the amount of data available for researchers to explore, and most recently, the amount of well-constructed, easily understandable and accessible datasets – like the GHTorrent tool – is impressive (traditionally it has been difficult to get any credit for creating these resources). I think it is a hugely beneficial effort for our efforts to create a well-grounded, empirical basis for software engineering (as opposed to pie in the sky theorizing).\nI have two concerns that threaten this idyll."
  },
  {
    "objectID": "posts/2016-03-07-on-using-open-data-in-software-engineering.html#footnotes",
    "href": "posts/2016-03-07-on-using-open-data-in-software-engineering.html#footnotes",
    "title": "On Using Open Data in Software Engineering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSubmit early, submit often!↩︎\nJorge Aranda’s ‘secret life of bugs’ paper sheds light on this.↩︎\nYou put your freaking email on Github! What did you expect!? (sigh)↩︎"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching and Materials",
    "section": "",
    "text": "Dates are planned future offerings. Where future = “as of when I last updated this page (Jun 22)”.\n\n\n\nSENG 350, Software Architecture. Fall 22\nSENG 330, OO Design.\nSENG 321, Requirements Eng. Spring 23\nCSC 497 - Interdisciplinary Project\n\nThis is the project capstone for students in any combined program. I advise students in Geomatics. This course is scheduled by completing the Directed Studies (Pro Forma) form available here. The project must combine elements of Geography (landscapes, geomorphology, human geography, climate change, etc. etc.) and Computer Science (software development, algorithms, networking, visualization, etc. etc.).\nThe format of the project is decided with the project advisor, but must include a final report and final public presentation (usually at a group meeting or department seminar). I strongly recommend weekly reports as a deliverable. The project should take 10-15 hours per week to complete. You cannot use content from other courses for this project, but building on top of those results is fine.\nPast projects have included comparing ML algorithms on remote sensing data, predicting urban growth, and mapping plankton blooms.\nThe course needs to have an advisor; students can find one on their own, or work with me on a topic. A good approach is to find a professor whose classes you have enjoyed and ask them directly. Most profs are more than happy to advise an independent research project that aligns with their ongoing research.\nThis is (supposed to be) a fun course! You get to work on ideas and problems that are interesting to you.\nFailing is possible, and usually happens when students do not keep up with the work.\n\n\n\n\n\n\nSENG 480X/CSC 586X, Data Science for SE. Fall 22\nSENG 480X/CSC 586X, Documenting and Understanding Software Systems."
  },
  {
    "objectID": "teaching.html#undergrad-courses",
    "href": "teaching.html#undergrad-courses",
    "title": "Teaching and Materials",
    "section": "",
    "text": "SENG 350, Software Architecture. Fall 22\nSENG 330, OO Design.\nSENG 321, Requirements Eng. Spring 23\nCSC 497 - Interdisciplinary Project\n\nThis is the project capstone for students in any combined program. I advise students in Geomatics. This course is scheduled by completing the Directed Studies (Pro Forma) form available here. The project must combine elements of Geography (landscapes, geomorphology, human geography, climate change, etc. etc.) and Computer Science (software development, algorithms, networking, visualization, etc. etc.).\nThe format of the project is decided with the project advisor, but must include a final report and final public presentation (usually at a group meeting or department seminar). I strongly recommend weekly reports as a deliverable. The project should take 10-15 hours per week to complete. You cannot use content from other courses for this project, but building on top of those results is fine.\nPast projects have included comparing ML algorithms on remote sensing data, predicting urban growth, and mapping plankton blooms.\nThe course needs to have an advisor; students can find one on their own, or work with me on a topic. A good approach is to find a professor whose classes you have enjoyed and ask them directly. Most profs are more than happy to advise an independent research project that aligns with their ongoing research.\nThis is (supposed to be) a fun course! You get to work on ideas and problems that are interesting to you.\nFailing is possible, and usually happens when students do not keep up with the work."
  },
  {
    "objectID": "teaching.html#combined-gradsenior-undergrad-courses",
    "href": "teaching.html#combined-gradsenior-undergrad-courses",
    "title": "Teaching and Materials",
    "section": "",
    "text": "SENG 480X/CSC 586X, Data Science for SE. Fall 22\nSENG 480X/CSC 586X, Documenting and Understanding Software Systems."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research & Publications",
    "section": "",
    "text": "My research explains the how and why of software, based on user intentions. We want to know that our software is doing what we told it. We also want to know the reasons our software does something unexpected. Finally, we want to be able to design software to meet our intentions in the first place. Some recent and ongoing projects:\n\n All Publications\nFor a complete list, or mostly complete, best to look at one of the following sites.\n\n\n DBLP\n\n\n ORCiD\n\n\n Semantic Scholar\n\n\n Google Scholar\n\n\n ArXiv\n\n\n\n Slideshare\n\n\n SEI Digital Library\n\n\n SpeakerDeck\n\n\nI encourage you to check out Impact Story’s “UnPaywall” extension.\n\n\n Technical Debt and Software Documentation\nTechnical debt is a short-term software design choice that incurs long-term costs if not dealt with. We look at technical debt in requirements, in architecture, and for emerging machine learning systems.\n\n\n\n Qualitative Research and Peer Review\nWe are conducting studies into how qualitative research is emerging as a key research strategy for software engineering, which is, after all, highly subjective and contextual. A similar project looks at how we know what we know in software engineering, specifically for reviewing papers.\n\n\n Bayesian Statistics\nWe are examining ways to improve the state of the art of software research statistical approaches, particularly by eliminating null hypothesis testing. Bayesian statistics works intuitively with the highly contextual nature of software projects, which tend to vary in size, domain, criticality, and numerous other places.\n\n\n\n Software Requirements and Analysis\nMy background is requirements analysis and modeling, and this continues to be a passion of mine. All the funky stuff we can do with programming languages, testing, design, etc. is irrelevant if we are building the wrong thing.\n\n\n\n\n\n\n\n Theses\n\nN. A. Ernst, “Software Evolution: A Requirements Engineering Approach”. Ph.D. dissertation, University of Toronto, 2011. pdf (224 pages)\nN. A. Ernst, “Towards Cognitive Support in Knowledge Engineering: An Adoption-Centred Customization Framework for Visual Interfaces”, unpublished M.Sc. thesis, University of Victoria, 2004. pdf (95 pages)\n\nIcons from NounProject"
  },
  {
    "objectID": "posts/2015-08-25-running-a-critical-research-review-at-re15.html",
    "href": "posts/2015-08-25-running-a-critical-research-review-at-re15.html",
    "title": "Running a “Critical Research Review” at #RE15",
    "section": "",
    "text": "Today we conducted our first attempt at “Critical Research Reviews” (CRR) at our workshop on empirical requirements engineering (EmpiRE) at the 2015 Requirements Engineering Conference.\nCRR was introduced to me by Mark Guzdial’s post on the same exercise at ICER last year, which was run by Colleen Lewis. The idea (as I understand it) is to have researchers present work in progress, ideally at the research design stage. The purpose of the workshop is to “leverage smart people for an hour” in improving and stress-testing your research idea and methodology.\nThe cool part about doing this at EmpiRE is that our proposers got to leverage some of the leading empirical researchers in the RE community. These are the people likely reviewing your full paper, so it makes sense to get their critique up front.\nWe had three accepted “research proposal” papers as a special category for the workshop call. In the afternoon, (2pm-5.30pm) we had the three presenters do a 15 minute plenary presentation to get everyone in the workshop (25 or so) aware of the work. I restricted any questions so this was almost entirely over in 45 minutes. After a coffee break, I introduced the CRR concept and some ground rules, as well as a list of potential questions to consider. Then, for the next 45 minutes or so, the participants were invited to join the presenter that interested them and have a (polite) discussion about the proposed research.\nFinally, I had asked each group to bring some wide-ranging thoughts back for the entire group for the last 30 minutes. My intent here was not to go into specifics on the proposals; rather, to get some other lessons that might be useful for the people who were not part of that particular group. This worked pretty well; it did tend to go into more detail than perhaps warranted, but it did stimulate some interesting discussion.\nFrom what I heard, people quite enjoyed this approach to research evaluation. It’s much more fun trying to poke holes in research approaches when the author on the other end can rebut your arguments!  Look for another edition next year.\nYou can find my slides introducing the idea here, and our proceedings, with the presenters’ research proposals, will be posted whenever IEEE gets around to it.\nLessons learned\n\nThe room was terrible: large central conference table. One group retreated to the coffee room which had large circular tables.\nNo one used the flip charts: I think the presenters were writing their own notes down on their laptops anyway.\nWe mostly had established researchers presenting. In the future we are considering perhaps restricting this to early career or Phd students, who likely need the assistance more. But I think the more senior researchers still benefited. The primary difference, I think, will be that the senior researchers will have considered more of the potential threats.\nI was main facilitator: having one group a 2 minute walk away made this harder. No group really needed help, but I can certainly see possibilities where that would be an issue. For instance, if you get too many people going to one presenter, or one person dominating the discussion, or too much negativity (the usual group dynamics, in other words)."
  },
  {
    "objectID": "posts/2013-03-12-the-fuzzy-notion-of-business-value.html",
    "href": "posts/2013-03-12-the-fuzzy-notion-of-business-value.html",
    "title": "The fuzzy notion of “business value”",
    "section": "",
    "text": "Software development is rife with references to business value, particularly in agile approaches: the Agile Manifesto declares that “Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.”\nThe trouble is that it isn’t clear what ‘valuable’ means. I’m sure that the point of this phrase, as with most of the Manifesto, is to start a discussion rather than to behave as a prescriptive methodology. I believe “value” is inherently context-dependent, so in that sense it is reasonable to leave it vague.\nOn the other hand, many people refer to business value as the Holy Grail of software development: this is what you are supposedly optimizing in Scrum. Other methodologies help focus on “impact”. Lean approaches have one remove ‘waste’ from the value stream. And yet no one has ever pinned value down, as Racheva et al. have shown [1] (in the software domain, anyway - many attempts have been made in economics).\nBusiness value does have the nice property of communicability, though. It gives developers that one number to sell the project to the business, and allows for a conversation about scope, cost of delay, and prioritization that is difficult to do with purely qualitative methods. And for the mathematically inclined, it lends itself to algorithms like linear programming for optimization.\nOne paper does try to break business value into more reasonable components, which I quite liked. It is by Heidenberg et al. [2]. They break business value into five dimensions, each of which is ranked on an ordinal scale with four possible categories:\n\nMonetary Value - this is the number calculated by, say, a business analyst.\nMarket Enabler - does delivering this feature create new market opportunity?\nTechnical Enabler - does this feature help prepare the company for other features?\nCompetence Growth - measures how much the work will improve the team’s skill.\nEmployee Satisfaction - do the developers like working on this feature?\nCustomer Satisfaction - how much will customers appreciate this work?\n\nOne of the big problems with agile planning is that making categories 2, 3, 4, 5 visible is often hard. It is comparatively easy to sell a customer a feature that ranks highly in monetary value or customer satisfaction - these are the slick and cool UI widgets, the mission-critical Word reporting function, and so on. But making architectural work (category 3) visible is very challenging. If we had this six-factor model, prioritizing important architectural work would be easier.\n[1] Z. Racheva, M. Daneva, and K. Sikkel, “Value Creation by Agile Projects: Methodology or Mystery?,” presented at Product-Focused Software Process Improvement, 2009, vol. 32, no. 12, pp. 141–155.\n[2] J. Heidenberg, M. Weijola, K. Mikkonen, and I. Porres, “A Model for Business Value in Large-Scale Agile and Lean Software Development,” presented at EUROSpi: Systems, Software and Services Process Improvement, 2012, pp. 49–60."
  },
  {
    "objectID": "posts/2015-10-27-how-writing-code-is-like-making-steel.html",
    "href": "posts/2015-10-27-how-writing-code-is-like-making-steel.html",
    "title": "How Writing Code is Like Making Steel",
    "section": "",
    "text": "I saw an interesting keynote from Mark Harman recently, on search-based software improvement. Mark’s lab at UCL also pioneered this idea of automatic code transplants using optimization techniques.\nI think if you are an engineer who does fairly standard software development you should be concerned. The ultimate vision is to be able to take some specification with thorough tests, written in a language at a high-level of abstraction (e.g., here is my corporate color palette, here are my security requirements) and automatically generate the application.\nThere are several forces at play here. One is the increasing componentization of large and complex pieces of software. We’ve always had software reuse, but it tended to be at a much smaller level - the ODBC api, or the OAuth framework. Now our frameworks reach much larger areas of concern, particularly when we look at container technology running on commodity hardware. Someone else is maintaining huge chunks of your software base, in those cases: the OS, the backend, the messaging system, etc. If you then take your Rails app and add it to that stack, how much, as a %, have you created? A decreasing amount, in any case.\nThe other force is the improvements in genetic and other optimization algorithms, combined with the inevitable scaling of computing power. That means that even though you may be really good at crafting code, and the machine generates garbage, it can improve that garbage very very quickly.\nHow different is it for me to copy and paste the sample code on the Ruby on Rails site to create a new application, than for a computer algorithm to follow those same steps? To be clear, there remain a lot of complex decisions to make, and I’m not suggesting algorithms can do so: things like distributed systems engineering, cache design, and really just the act of taking a user requirement and turning it into a test.\nSo how is this like the steel industry? I think it reflects commodification and then automation. Steel was largely hand-made for years, but the pressure of capitalism generated rapid improvements in reducing costs - largely labor costs. Process and parts became standardized, so it was possible to set up mills at much lower cost. The difference in quality between US and (say) Indian steel became small enough to not matter. But even in India, the pressures continue downward, so India’s dramatically lower labor costs still cannot compete with automation.\nSome of these pressures don’t exist in software, of course: there is still a large knowledge component to it, and there are no health and safety costs in software labor (the hazards of RSI and sitting notwithstanding). So I don’t see any big changes immediately, but the software industry is probably where the steel industry was in the 20s. In 50 years I cannot see software being written by hand at the level it is now, with the exception (like in steel) of low-quantity, high-tolerance products like embedded development. The rest will be generated automatically by algorithms based on well specified requirements and test cases. Silicon Valley will become the rust belt of technology. You realize that Pittsburgh, birthplace of the steel industry, was once the most expensive city in the US, right?\nIf you doubt this, I think we are really arguing over when, and not what. My simplest example is coding interviews. Why test people on knowledge of algorithms that are well understood, to the point where they are in textbooks and well-used library code? The computer can write the FizzBuzz program faster and more efficiently than a human can. Over the next few decades, I believe Mark Harman’s optimization approach will encompass more and more of what we now do by hand."
  },
  {
    "objectID": "posts/2013-09-24-13-great-software-architecture-papers.html",
    "href": "posts/2013-09-24-13-great-software-architecture-papers.html",
    "title": "13 Great Software Architecture Papers",
    "section": "",
    "text": "In the paper “The Past, Present and Future of Software Architecture”, the authors (Philippe Kruchten, Henk Obbink, and Judith Stafford) have a sidebar in which they list their selection of “Great Papers of Software Architecture”. I’ve tried to collect these papers and links thereto for future reading. Here is a bibtex file for full citations. I’ve also included the parenthetical comments of Kruchten et al. in italics, and my comments in bold. Unless otherwise noted I’ve linked directly to the PDFs (please let me know if a link breaks)."
  },
  {
    "objectID": "posts/2013-09-24-13-great-software-architecture-papers.html#foundations",
    "href": "posts/2013-09-24-13-great-software-architecture-papers.html#foundations",
    "title": "13 Great Software Architecture Papers",
    "section": "Foundations",
    "text": "Foundations\n\nGarlan and Shaw, “An Introduction to Software Architecture”—Shortly preceding their book, this paper brought together what we knew about software architecture in the beginning of the 1990s.\nD.E. Perry and A.L. Wolf, “Foundations for the Study of Software Architecture,”—This seminal paper will be always remembered for giving us this simple but insightful formula: {elements, form, rationale} = software architecture."
  },
  {
    "objectID": "posts/2013-09-24-13-great-software-architecture-papers.html#precursors",
    "href": "posts/2013-09-24-13-great-software-architecture-papers.html#precursors",
    "title": "13 Great Software Architecture Papers",
    "section": "Precursors",
    "text": "Precursors\n\nD.L. Parnas, “On the Criteria to Be Used in Decomposing Systems into Modules,”—Software architecture didn’t pop up out of the blue in the early 1990s. Although David Parnas didn’t use the term “architecture,” many of the underlying concepts and ideas owe much to his work. This article and the next two are the most relevant in this regard._\nD.L. Parnas, “On the Design and Development of Program Families”\nD.L. Parnas, P. Clements, and D.M. Weiss, “The Modular Structure of Complex Systems”\nF. DeRemer and H. Kron, “Programming-in-the-Large versus Programming-in-the-Small,” —Their Module Interconnection Language (MIL 75) is in effect the ancestor of all ADLs, and its design objectives are still valid today. The authors had a clear view of architecture as distinct from design and programming at the module level but also at the fuzzy, abstract, “high-level design” level."
  },
  {
    "objectID": "posts/2013-09-24-13-great-software-architecture-papers.html#architectural-views",
    "href": "posts/2013-09-24-13-great-software-architecture-papers.html#architectural-views",
    "title": "13 Great Software Architecture Papers",
    "section": "Architectural views",
    "text": "Architectural views\n\nD. Soni, R. Nord, and C. Hofmeister, “Software Architecture in Industrial Applications,” —This article introduced Siemens’ five-view model, which the authors detailed in their 1999 book Applied Software Architecture\n__P. Kruchten, “The 4+1 View Model of Architecture,” —Part of the Rational Approach—now known as the Rational Unified Process—this set of views was used by many Rational consultants on large industrial projects. Its roots are in the work done at Alcatel and Philips in the late 1980s."
  },
  {
    "objectID": "posts/2013-09-24-13-great-software-architecture-papers.html#process-and-pragmatics",
    "href": "posts/2013-09-24-13-great-software-architecture-papers.html#process-and-pragmatics",
    "title": "13 Great Software Architecture Papers",
    "section": "Process and pragmatics",
    "text": "Process and pragmatics\n\nB.W. Lampson, “Hints for Computer System Design,”—This article and the next gave one of us (Kruchten), a budding software architect in the 1980s, great inspiration. They haven’t aged and are still relevant.\nJ.A. Mills, “A Pragmatic View of the System Architect,” paywall\n****W.E. Royce and W. Royce, “Software Architecture: Integrating Process and Technology,” —This article articulates the connection between architecture and process very well—in particular, the need for an iterative process in which early iterations build and validate an architecture. Not online, sadly."
  },
  {
    "objectID": "posts/2013-09-24-13-great-software-architecture-papers.html#two-more-for-the-road",
    "href": "posts/2013-09-24-13-great-software-architecture-papers.html#two-more-for-the-road",
    "title": "13 Great Software Architecture Papers",
    "section": "Two more for the road",
    "text": "Two more for the road\n\nM. Shaw and P. Clements, “A Field Guide to Boxology: Preliminary Classification of Architectural Styles for Software Systems,”\nM. Shaw, “The Coming-of-Age of Software Architecture Research,”."
  },
  {
    "objectID": "posts/2020-09-30-kaggle-mining.html",
    "href": "posts/2020-09-30-kaggle-mining.html",
    "title": "Running a Mining Challenge Using Kaggle",
    "section": "",
    "text": "For the 2nd edition of the Dynamic Software Documentation (DysDoc) workshop, the organizing team wanted to push the boundary on how to engage the community in tool supported demos. Previously, we had asked participants to come to the workshop (co-located with ICSME) with a tool to demo, live, to the other attendees. One of the goals was a tool that worked on unseen data.\nThis year, at our organizing meeting, we wanted to try something that went beyond documentation generation, and looked at other issues with dynamic documentation fixes. A study by Walid Maalej and Martin Robillard, which looked at types of API documentation, included an interesting issue with documentation - code comments - that were uninformative.\n/** \n *  Clears the log\n */\npublic void clearLog() {\n  LogMessages.getInstance().clear();\n}\nThis comment is clearly not adding information to the code, and in fact, might even be harmful, if it were to be outdated. Thus our “Declutter” Challenge: figure out a way to identify these type of comments and (eventually) target them for removal. I was co-organizer alongside Nicole Novielli.\nWe were inspired by the success of datasets and benchmarks such as Fei Fei Li’s ImageNet contests, or the SAT competition. Both of these have been influential in driving innovation in graphics and satisfiability solving. As it turned out, these distributed competitions were also ideally suited to the new remote work paradigm that was required during the COVID pandemic.\nTo set up this competition, there was the option to have the competitors take the dataset, work on a solution, then submit their solution to the organizers for evaluation. Of course, this involved a lot of work on the part of the organizers, and being fairly lazy, I looked for an alternative approach. Immediately the Kaggle competition platform seemed the way to go: it has been hosting large-scale data science competitions since before there was data science.\nI therefore investigated how this could work. Normally, hosting Kaggle competitions require payment (commercial) or prizes (academic). Academic contests are also selected by Kaggle. Fortunately, Kaggle makes the platform available for classroom use, on Kaggle InClass. The difference, other than no support, is that competitors do not get Kaggle points for entering. Nicole and I thus decided to use Kaggle to host the Declutter challenge, which you can find here.\n\nWhat Worked\nDespite lacking support, getting the contest going on Kaggle was fairly simple. Nicole organized labeling with the rest of DysDoc’s organizers, and hosted a gold set on Github. I then used the gold set to generate the inputs Kaggle wanted. This includes the gold set, split into training and test. Test data is further split into “public leaderboard” and “private leaderboard”, since competitors can submit multiple entries, and see where they stand on the leaderboard. Only the organizers get to see the private leaderboard, which is what ultimately ranks the competitors. You can see that in practice here.\nI also had to choose between Kaggle’s available validation metrics, in this case choosing F1, and this can be a bit finicky, as you have to map between the columns in the solution CSV file and whatever Kaggle’s automation expects. Clearly at paid support levels they would make this simpler, or just do it for you.\nDespite not having a lot of labeled data, we managed to get a good set of data, and Kaggle’s infrastructure worked - as far as I know, anyway! - with no problems. Competitors download the data, run their model, and then upload a solution file with their predicted labels.\nWe managed to get 2 principal competitors, one of whom submitted several distinct entries. Both entrants published their submissions at our workshop, which can be found at the ICSME proceedings site.\n\n\nImprovements and Questions\nI was quite happy with how simple Kaggle made the process of evaluating entries. It also scales flawlessly (unlike me), and in theory, could help us dramatically expand our contest. In the COVID era, of course, it also made it pretty easy to host a remote contest, unlike our previous approach, which used in-person demos.\nIt would be nice to have more support for notebooks, or perhaps a mandatory notebook submission, so that we can see how each group approaches the problem (after it finishes of course).\nAs far as I am aware, this is one of the first challenges to be hosted on Kaggle. To me it seems like an obvious choice for running and hosting automated benchmarks, such as the various effort estimation and defect prediction datasets out there. If we could disambiguate entries, that would help with understanding who is entering.\nKaggle makes it possible to host an ongoing, never-ending contest, which is also appealing. The obvious bottleneck, unsurprisingly, is data annotation, and at this point I would say that is the main obstacle to running more such contests. However, we have tentative plans to continue the approach in future workshops."
  },
  {
    "objectID": "posts/2021-03-03-teaching-factors.html",
    "href": "posts/2021-03-03-teaching-factors.html",
    "title": "The Triumvirate of Teaching and Work Life Balance",
    "section": "",
    "text": "Most courses have a series of learning outcomes for students. Once you have done the course (and, I assume, gotten a B or some reasonably high mark), then you know how to accomplish the learning outcomes.\nSome may break those learning outcomes down to smaller units per module.\nFor instructors, it occurs to me there are three objectives to balance (at least):\n\nHow much effort it takes to teach the topic\nHow much students appreciate the topic and teaching choices\nHow much students learn after being taught\n\nNumber 2 is not ever considered in pedagogy, but is the ONLY thing that matters from a management point of view. It maps directly to the things RateMyProf and course evaluations measure. Therefore from a pragmatic point of view, a prof should only care about 1 and 2.\nNumber 3 is what pedagogy is all about: how much are students learning? This is what is referred to when we look at things governments fund us to do. They want more “skilled workers”. Students in the short-term don’t really care much about this stuff. And course evaluations don’t really test for this. Teachers who are really good at 3 often end up getting punished for 2 (learning things is hard and not fun!)\nNumber 1 is often ignored, too, but can be the difference between having a good term and a shitty term. For any given topic, there are many ways to think of teaching that topic. Take data flow diagrams:\n- we could lecture off a set of slides showing DFDs\n- We could use a textbook reference and just skim the topic\n- We could create a detailed case study and show DFDs by construction\n- And for each of these, we could come up with different teaching strategies: whiteboard, live coding/drawing, interviewing experts etc.\nAs someone with limited time, one goal has to be minimize #1. My contention is that it is easy to go for perfection in 3 and absolutely devastate yourself in #1.\nWhat I try to do is:\n\nDestroy with fire any plan that maximizes 1, but minimizes 2 and 3. If students aren’t learning, aren’t enjoying it, and it is a lot of work, you should NEVER do it. And I bet you would be surprised how often this case happens. For example, in my third year software design course I spent a ton of time (increasing 1) ingesting the Play framework, learning how it worked, so that students could use it in the project. But it was a huge pain for the students to work with (hurting 2), they wanted to use React instead (hurting 2), and most of the learning was about the Play framework itself, rather than software design concepts (hurting 3). I won’t be doing that again.\nRuthlessly assess how important #3 is for your career. I haven’t seen anyone whose teaching packet evaluates 3. And yet it seems like we all talk about how it is the only goal. I really hope researchers improve on this. For tenure, for example, I seriously doubt anyone is looking at this. The closest we come is peer evaluation, but as Mark Guzdial wrote, this is often the blind leading the blind.\nTeaching awards seem to be about 2: winners tell jokes, they dress nice, they are male, they seem knowledgeable.\nGiven what we think we know about how software is built, I am going to guess that some teachers are really effective (either for 2 or 3) for minimal effort, and others put in orders of magnitude more time on 1, but have little extra to show for 2 or 3. I believe there is a non-linear, diminishing returns model for teaching effort; you might do 10 hours of prep for a lecture and not have much more to show for (3) then if you had done 1-2 hours.\nThere is a sunk cost/amortization problem here, too. If you teach a course the first time, you may have a lot of 1 to pay off. Subsequent offerings might greatly reduce 1 and allow you to focus on 3 (or 2) to a greater extent. But I’m not sure how much this is true. Things move fast in software courses, especially in 2nd year and above, some costs simply don’t amortize (marking), and we often try to improve the course year to year. Plus, we might not get to teach that course more than a few times.\n\nI want to be clear that I am not endorsing a focus on teaching for “show” instead of long-term learning. 3 is clearly the goal. But the reward structure does not reflect this. We should figure out how to ensure that teaching is measured against outcomes on 3, and not on 2 (2 is also horribly biased!).\nMore importantly, there seems to be embarrassingly little data on how to minimize 1 and maximize 3. I think this is a problem. We have a lot of info on (for CS1 at least) how to best teach linked lists, such as using Parsons problems. But frankly, my job involves teaching 40% of my time. I may not be able to dedicate the time required to prepare Parsons problems for the course. So a “cost/benefit” (1 vs 3) analysis would be very useful to help me maximize the teaching effectiveness for unit of teaching effort."
  },
  {
    "objectID": "posts/2022-06-03-Roles-of-PI.html",
    "href": "posts/2022-06-03-Roles-of-PI.html",
    "title": "The PI as COO",
    "section": "",
    "text": "Faculty at a research university wear many different hats. One analogy might be to the executive roles in a company. Now a company tries to make profit, which is usually not the goal in academia (quite the contrary). But still, one can think of the mapping like\n\n\n\n\n\n\n\nTitle\nTasks for Faculty Member\n\n\n\n\nCEO\nStrategy, overall planning, team management\n\n\nCFO\nBudgeting, managing funds, making “payroll”\n\n\nCIO\nFiguring out IT equipment, choosing between cloud, govt infrastructure, managing equipment\n\n\nCTO\nLooking at trends and new tools (e.g., Google Colab, VS Code, Qualtrics)\n\n\nCMO\nMarketing the team and PI to the world, creating a need for the lab’s products (papers)\n\n\n\nThe one role not listed here is the one that I think in many ways is the most important: COO. Now, I don’t know much about business roles, but to my mind the Chief Operating Officer is sort of similar to the executive officer of a submarine. They are the person who makes the business work: ensuring inventory is at the right level, planning for new space as the company grows, managing supply chains, etc. Tim Cook, now CEO at Apple, made his name growing Apple’s manufacturing to the vast network it is now. That meant ensuring secrecy, getting supplies to factories, scaling to manage millions of devices being released at the same time, etc. Apple isn’t profitable and gigantic if the day-to-day operations aren’t running efficiently.\nBut the same is true in academic life!\nI think of the analogy as requiring thought and attention to the day to day management of productive work in the university. You need to make sure the ‘operations’ are smooth. One small component of this is the paper funnel: we need to ensure we have a lot of ideas in our funnel, that they get turned into data collection and analysis, and that the analysis gets written up and submitted, and eventually published. This is Arvind’s statement below about “getting sh*t done”, because it can be frustrating to think of oneself as moving papers from idea to publication. We want to pretend we are supposed to be thinking about ideas, noodling on the whiteboard, and being inspired by genius. And we are! But that’s not the COO part of the job.\n\n\nAcademics would double our productivity if we learnt some basic project management skills that are bog standard in the industry. We have this myth that scholarly success is all about brilliance and creativity, but in fact 90% of it is getting sh*t done, same as any other job.\n\n— Arvind Narayanan (@random_walker) June 2, 2022\n\n\n\nMetrics\nWe could look, like I’m sure Apple does, at operational metrics and efficiencies. For example:\n\nNumber of papers in draft/being reviewed/published (WIP)\nTime between idea and paper publication (lead time)\nStudents graduated in expected time\nNumber of important unanswered emails in Inbox\nTo the nearest thousand, how much money is in various accounts, and what the projected “burn rate” is for those accounts.\nHow long has each student been in the program, what milestones have they finished, and when they should graduate\nGrant money received\nGrants used efficiently\nReviews conducted within deadline\nTalks invited/given/follow up\nSize of industry collaboration address book\nTravel reimbursements received within 30 days of trip\nTime between equipment being needed and equipment being purchased and equipment delivered\n\nNow for some of these you might say “but someone else is blocking that!” Which is of course true of EVERYTHING and also not an excuse Steve Jobs was likely to accept. That’s all part of being efficient and operating smoothly. If you know the university takes forever to process room bookings, you need to factor that in to the operational goals.\n\n\nWhy Care About Operations\nI think operational efficiency is what separates average researchers from those seen as impactful. Sure, in some cases it might be a brilliant one-off paper, but often we reward output volume: “quantity has its own quality”. Did the project lead to a single paper, or did you harvest 3-4 papers from it? That’s an operational detail that has to do with a PI’s ability to direct students, target appropriate venues, manage meetings to keep the papers on schedule, and so on.\nI think the importance of the COO view of one’s career is that for better or worse these outputs are the easiest to turn into data, and subsequently evaluate you, and your institution. So ignoring number of students graduated, or number of publications, or grant values, will result in poor scores on these data metrics. It doesn’t matter how many brilliant ideas you have if no one gets to read them.\nThe question for this COO view of a research career is to figure out which metrics one truly cares about, and when to stop focusing on operations and think more about strategy and trajectory. Metrics, because the metrics you choose reflect your priorities (e.g., papers published vs industry collaborations nurtured), and strategy, because (hopefully) the research you pursue should reflect some higher level of understanding about what problems are important to be spending time on.\n\n\nMy approach\nFor me personally, it can be hard to remember to manage the operational details. The easiest way for me to see this concretely is when papers fail to meet a venue deadline. That’s an operational failure: we didn’t move fast enough on data analysis, the meetings were not productive and the project spun its wheels, I didn’t kill the project or value the cost of delay, I answered emails about committee work rather than spending 2 hrs editing.\nMy current management approach is to check in on each project (I have about 9-10 in various stages) weekly, using a dedicated card (using a note in Apple’s Notes tool). A Kanban board with stickies can be really helpful here too, but the important thing is not the particular system but that you use it and check it regularly.\nAnother idea I have just started to implement is reflecting on lessons learned from a project (e.g., after a paper is published). Not just the research problems, but the operational challenges. What would I do differently for project management? What worked well in moving the project along? Was this a productive collaboration? Why did it get delayed (it’s always delayed)?"
  },
  {
    "objectID": "posts/2015-07-27-a-field-study-of-technical-debt.html",
    "href": "posts/2015-07-27-a-field-study-of-technical-debt.html",
    "title": "A Field Study of Technical Debt",
    "section": "",
    "text": "Over on the SEI blog, I’ve written up our survey results on technical debt."
  },
  {
    "objectID": "posts/2017-06-25-Moving-to-UVic.html",
    "href": "posts/2017-06-25-Moving-to-UVic.html",
    "title": "Moving to UVic",
    "section": "",
    "text": "I’m excited to announce I will be taking up a position this fall as a tenure-track faculty member in the Department of Computer Science at the University of Victoria.\nThis is a great opportunity to work with some of the top software engineering faculty in the world, in one of the best cities in the world (although I’m biased, as it is my hometown :). Victoria is at the forefront of the startup scene, just a few hours from Vancouver, Seattle, and direct flights to the Valley (not Abbotsford, the other one).\nIf you are interested in doing research with me please take a look at my ‘prospective students’ page. Uvic, and Canada, welcome people of all backgrounds. See our study permit process, and the federal Express Entry program for post-graduation immigration opportunities.\nI want to thank my colleagues and co-workers at Carnegie Mellon and the Software Engineering Institute for a great four years. I’ve learned a lot about software architecture, large-scale software projects in government agencies, and more US military acronyms than I care to admit. I’ll also really miss Pittsburgh, which has been wonderfully welcoming and a pleasant surprise. It’s easy to see America through a particular perspective these days, but many—most—Americans are awesome and caring people. \nYou can continue to reach me via Twitter, @neilernst, via this web page, or via email, neil@neilernst.net."
  },
  {
    "objectID": "posts/2010-04-22-should-we-care-about-evidence-based-software-engineering.html",
    "href": "posts/2010-04-22-should-we-care-about-evidence-based-software-engineering.html",
    "title": "Should we care about evidence-based software engineering?",
    "section": "",
    "text": "Time for some contrariness. The current rage in the academic software research community is evidence-based practice. It’s in popular magazines, desirable in academic publications, and the subject of a new book.\nDoes it matter? On the face, one would say of course. Why would you make decisions ignorant of the facts?  (set aside for now the reality that almost NO decisions in the world are made based on the facts!)\nIt would be nice if software researchers were in a position to present facts to people. In climate science, for example, the facts are pretty clear, and certainly much clearer than corresponding literature in software. That’s why Al Gore, among others, probably sees debates on climate change as pointless. But the utility of model-driven development, among many others, is very much worth debating. I think there are five reasons why we shouldn’t be too concerned about evidence in software development:\n\nThe field with a long history of evidence-based practice, and the most to gain from it, medicine, often doesn’t adopt the recommended practices, or the evidence chosen is irrelevant. Despitehand-washing or checklists being shown (proven?) to be very cost-effective practices to adopt, doctors still leave washrooms without cleaning their hands, and instruments still get left in patients. And in most software projects, there isn’t anything like that sort of liability.\nPeople don’t understand statistical generalization very well. Is that new pill reducing my risk of heart disease 20% more than the other pill, or 20% more than a regimen of Big Macs? Was this experiment done with non-English speakers? There’s a lot more to it than running a few t-tests and calling it a day. See e.g. “Why most published research findings are false” or a series of critiques on fMRI studies.\nSmall results don’t say much. A lot of research is evaluated on small numbers of undergrads or focused on one particular organization (pdf). That evidence is useless to most developers. There is a paucity of in-depth, detailed case studies that generalize to meaningful theories. Personally I am in favour of a moratorium on experimentation in software research until more of these case studies are done. Unfortunately, the lure of the easy number is a Siren-call to reviewers and funding agencies.\nSEMAT to the contrary, there is no good body of software theory that would provide explanatory power to go along with results. Without a theory facts are descriptive; with a theory they can be predictive.\nIt simply isn’t that important. Individuals and organizations do many things which research suggests is downright insane – like embarking on projects without clear requirements, or maintaining 30 year old mainframes – and get by. In fact, anecdotal evidence suggests that many excellent companiesstarted with poor practices, then refactored as needed. Probably, this is because evidence-based software development is a case of premature optimization. For example, despite reams of studies suggesting model-driven development is the way of the future, industrial adoption is underwhelming. Is it because they haven’t read the studies? Or that they evaluated the technology and concluded it wasn’t necessary? As academics, we tend to undervalue the benefit of anecdote and gut feelings. Most of the time this is probably correct, but only if we have evidence to support generalization to common scenarios. Most developers were so burned by the CASE tools of the 1980s that they have no interest in repeating the experience with UML.\n\nI think my final point is that rationality is the exception, rather than the rule, in human behaviour. There’s no reason to lose any much sleep over the fact that industry isn’t following evidence-based software practices.\np.s. I’m a complete hypocrite with respect to experimentation."
  },
  {
    "objectID": "posts/2013-09-11-virtual-conferences.html",
    "href": "posts/2013-09-11-virtual-conferences.html",
    "title": "Virtual Conferences",
    "section": "",
    "text": "Virtual Conferences\nThe virtual conference has many benefits in terms of GHG emission reductions, cost savings, and accessibility. Jon Pipitone and Jorge Aranda co-organized a very small event with me in 2011 around climate change and software. We used Skype and instant messaging to connect climate researchers with software researchers, but my feeling afterwards was that virtual worlds like OpenSim were the way to go. \nIt’s cool to see Crista Lopes has managed a relatively large-scale event."
  },
  {
    "objectID": "posts/2013-01-25-teaching-advanced-software-engineering.html",
    "href": "posts/2013-01-25-teaching-advanced-software-engineering.html",
    "title": "Teaching Advanced Software Engineering",
    "section": "",
    "text": "The course covers software architecture, with a focus on quality attributes, security, and formal methods. I liked the range of material, even though my expertise is limited with formal methods. It is difficult to teach architecture to students in a 3 month time frame, so we expanded using the AOSA textbooks. Students did a presentation for five minutes as a way of exposing them to various different architectures.\nThe other large component of the class is a course project. In this semester they had to build a location-aware, social application. There were great projects including my personal fave, a zombie fighting location-based game.\nMy favorite part of this course, like the third-year course, is seeing how the students approach the project. Some are truly excellent coders and put an enormous amount of effort into the project.\nI introduced a few new lecture topics in addition to the ones pre-existing. I added a topic on Service Orientation and SOA, important trend in particular in enterprise architecture; a new topic on REST, which was well connected to the project; and a topic on agility and architecture, based in part on the book by Dean Leffingwell, Agile Requirements. I thought all of these were useful, although they tend to be less easily tested than e.g. model checking, so perhaps students are just forgetting them. I even mentioned CMMI!"
  },
  {
    "objectID": "posts/2013-01-25-teaching-advanced-software-engineering.html#material",
    "href": "posts/2013-01-25-teaching-advanced-software-engineering.html#material",
    "title": "Teaching Advanced Software Engineering",
    "section": "",
    "text": "The course covers software architecture, with a focus on quality attributes, security, and formal methods. I liked the range of material, even though my expertise is limited with formal methods. It is difficult to teach architecture to students in a 3 month time frame, so we expanded using the AOSA textbooks. Students did a presentation for five minutes as a way of exposing them to various different architectures.\nThe other large component of the class is a course project. In this semester they had to build a location-aware, social application. There were great projects including my personal fave, a zombie fighting location-based game.\nMy favorite part of this course, like the third-year course, is seeing how the students approach the project. Some are truly excellent coders and put an enormous amount of effort into the project.\nI introduced a few new lecture topics in addition to the ones pre-existing. I added a topic on Service Orientation and SOA, important trend in particular in enterprise architecture; a new topic on REST, which was well connected to the project; and a topic on agility and architecture, based in part on the book by Dean Leffingwell, Agile Requirements. I thought all of these were useful, although they tend to be less easily tested than e.g. model checking, so perhaps students are just forgetting them. I even mentioned CMMI!"
  },
  {
    "objectID": "posts/2013-01-25-teaching-advanced-software-engineering.html#overall",
    "href": "posts/2013-01-25-teaching-advanced-software-engineering.html#overall",
    "title": "Teaching Advanced Software Engineering",
    "section": "Overall",
    "text": "Overall\nI feel that these types of courses in SE should be more about reflective apprenticeship than the lecture and project model. In other words, there should be more focus on feedback about the way in which students do design, more experiential learning, and less memorization of specific techniques such as formal methods (which should really be in a separate course, in my opinion). Mark Guzdial called this reflective apprenticeship and points to the work of Donald Schön.\nA parallel might be drawn with the way law schools operate. If you want to learn about how one practices law, argues cases, etc., you hire experienced practitioners as adjunct faculty. There seems to me to be a real difference between the skills of an academic SE faculty member and a person who has spent years building high-availability, mission critical software. There are a few of the latter at UBC, such as Philippe Kruchten, but in general it is exceedingly difficult for them to be hired without academic credentials. Not to mention the increasingly large salary gap between industrial SE and academic SE.\nThe other thing I disliked is that it seemed to me that a few students hid out during the project, latching on remora-like to their more capable teammates to secure a good mark with no work. It irritates me to pass students who are not able to write code (not good code, or even mediocre code - just not write code! See Fizzbuzz). It is very difficult to (defensibly) identify these people, however. One technique which I should have used is to ask questions of the indifividual students during their final demos. This would help to identify who actually knows what the heck is going on."
  },
  {
    "objectID": "posts/2013-01-25-teaching-advanced-software-engineering.html#academic-dishonesty-and-software-engineering",
    "href": "posts/2013-01-25-teaching-advanced-software-engineering.html#academic-dishonesty-and-software-engineering",
    "title": "Teaching Advanced Software Engineering",
    "section": "Academic dishonesty and software engineering",
    "text": "Academic dishonesty and software engineering\nOn the one hand we cannot prohibit collaboration and code re-use: these are fundamental practices in software engineering. On the other hand, we need to assess the student’s actual contribution. I had a few interesting cases that suggest our pollicies in this area need more attention. 1. One group used a sample Ruby on Rails project to bootstrap their application. It came with most of the controllers they needed. They then customized the UI and logic to implement the functionality (poorly). 2. Another team hired a third-party designer to do custom artwork for the project (which looked fantastic). 3. A different team had a friend with web design expertise work on the CSS for the project. 4. Several teams used Twitter’s Bootstrap UI library or JqueryUI to simplify their efforts on the design end. 5. Many groups used third-party libraries to simplify their life, like JQuery, Rails, image libraries, etc.\nObviously, most of these are exactly what would happen in industry. On the other hand, it definitely gains one an advantage. The Twitter Bootstrap apps all looked an order of magnitude better than the custom apps.\nMy principle was the remixing and reuse was fine, as long as it was properly acknowledged. In the design case, we could try to discount that aspect of the UI in the marking. But it is almost certainly the case that some groups did NOT acknowledge their use of other people’s IP, and yet benefited from it. I don’t have a good solution to the problem of detecting code reuse. And furthermore, the burden of proof is pretty high to call something cheating, and requires more than a gut feeling or a commit to Github that touched hundreds of files at once (i.e. a bulk commit of 3rd party code)."
  },
  {
    "objectID": "posts/2013-01-25-teaching-advanced-software-engineering.html#it-role",
    "href": "posts/2013-01-25-teaching-advanced-software-engineering.html#it-role",
    "title": "Teaching Advanced Software Engineering",
    "section": "IT role",
    "text": "IT role\nOne of the things which I think will only become more prevalent is the use of third-party services to manage the course. In the past, students would use CS department machines and servers to do their assignments, a CS database server, and store code on the department subversion or IBM RTC servers.\nThis semester I don’t think we used a single department resource, save for email (and that only because I was forced to for privacy reasons) and the course webpage. Class discussions took place on Piazza.com; code and issues were managed with Github, and students nearly always have their own laptops and Android devices (there was not a single group that chose iOS, incidentally, although nearly half the class has Macbooks. I think the 99$ fee is a real stumbling block - that and Objective-C).\nI did not get any support or materiel from the department, apart from the classroom and photocopier. I could just as easily have run this course from my home. So what should the IT section do? They could manage Github for me (they were extremely reluctant to do this, and very hesitant about even installing Bugzilla, apparently). They could provide more AV services to record classes. They could manage virtual machines for me, so that each student could install the same setup – things like Puppet and Vagrant will be key in the coming years.\nFinally, the UBC wireless infrastructure is truly terrible. You get better wifi at the Starbucks. Latency between two machines in my office was 200ms! The connection is constantly dropping or extremely slow, such that even demos are affected by the web performance."
  },
  {
    "objectID": "posts/2013-01-25-teaching-advanced-software-engineering.html#student-perceptions",
    "href": "posts/2013-01-25-teaching-advanced-software-engineering.html#student-perceptions",
    "title": "Teaching Advanced Software Engineering",
    "section": "Student perceptions",
    "text": "Student perceptions\nIn an unscientific survey, I asked the following questions:\n1. How could the TAs and myself improve your experience?\nStudents were either positive (but they had names attached to their responses, so that isn’t unexpected) or asked for more help. One of the big challenges they face is sorting out silly configuration problems. They would like more advice on design choices as well. I think this is a real opportunity to make the project more like an apprenticeship model, a la Software Craftsmanship: take some senior developers, get them to do an hour of code review, an hour of design feedback, etc. And there seem to be many companies eager to help out (and recruit) for whom this might be doable.The other issue was that due to 4) below, TAs and myself often did not know much about the technology (e.g., Microsoft’s C#/Azure platforms). However, this is definitely a learning objective in the course. Admittedly in industry one would often be able to ask senior devs these questions. However, the ability to track these answers down is invaluable, I feel.\n2. Were the AOSA readings useful?\nMost students responded that they appreciated the opportunity to present to a large audience. But the overall lessons of the architecture in these systems was lost on them, because it did not have a lot of relevance to the project, which consumed the majority of the time. Asking exam questions was difficult, as there was a lot of material that would have to be studied. I think I would keep this module but be more strict about the time limit (5mins) and give some introductory examples/prep before hand.\n3. Did Github work for you?\nStudents loved Github. Egit was less good (and personally I find it less usable than the command line). A major improvement over RTC.\n4. Was the freedom to choose language good or bad?\nMost students loved this aspect as well. In the past the project, worth 40% of the course, has been in e.g. Java+Tomcat for everyone. Feedback here indicated that main problems were finding team members with similar interests (in, e.g. RoR), getting help from TAs, and a possible penalty on the final, where the code snippets are in Java.\n5. What annoyed you about the project?\nUnsurprisingly, most complaints were about the time it took - one student spent 80 hours over two weekends on what was, however, a really cool UI - and the vagaries of group work with fellow team members, some of whom get sick, abandon their teammates, or simply are not good at programming. Students would appreciate more help on scoping the project, and getting the thing started earlier. We tried to address this by insisting on an early ‘project idea’ review in the first 3 weeks, and by doing a 30 minute design review midway. However, some people have to learn the hard way, and ultimately, we are constrained by how many teams there are - 27 in this case. Multiply that by 30mins and you can see the magnitude of the challenge. I had 3 TAs to help, but that is still a ton of work. And I think students got frustrated, since they see a 1-1 interaction, not 27-1 that I see."
  },
  {
    "objectID": "posts/2013-10-24-configuring-sonar-with-maven-on-mac.html",
    "href": "posts/2013-10-24-configuring-sonar-with-maven-on-mac.html",
    "title": "Configuring SONAR with Maven on Mac",
    "section": "",
    "text": "I had this issue a few times:\n\nyou get SONAR installed and the web client working fine (e.g. you can go to http://localhost:9000 and see the dashboard).\nyou have a project to analyze with a Maven POM, to which you add the sonar target as described here.\nyou start the Maven run and it returns in short order saying:\n\nCan not execute SonarQube analysis: Unable to execute Sonar: Fail to download [http://localhost:9000/api/server].\nTurns out for some reason this is a problem with the default Ruby install on OSX. The workaround is to use JRuby instead of Ruby, best done with RVM, e.g. rvm use jruby. Someone mentioned this online, and I cannot find the post now, but thanks.\nI use Sonar with Homebrew, by the way, which has its log files at /usr/local/Cellar/sonar/&lt;version&gt;/libexec/log/sonar.log."
  },
  {
    "objectID": "posts/2013-03-02-obtaining-a-pennsylvania-drivers-licence-with-an-h1-b.html",
    "href": "posts/2013-03-02-obtaining-a-pennsylvania-drivers-licence-with-an-h1-b.html",
    "title": "Obtaining a Pennsylvania Driver’s Licence with an H1-B",
    "section": "",
    "text": "In case this helps other people:\nPennDOT rules on what paperwork is needed can be found here. In addition, keep in mind the following:\n\nWe needed a letter from my employer, I94+passports, old licences, 2 proofs of residence (lease+bills), and a rejection letter from Social Security for the SSN (for my wife) and a letter with the number on it for me (haven’t got the physical card yet).\nIf you have a H1-B, and your spouse has an H4, you will need to go with your spouse if s/he is getting a licence as well - you can’t go separately.\nPennDOT does not take cheques drawn on foreign banks, only US banks. Fortunately you can get money orders easily at grocery stores. There is a Giant Eagle that does this near the Penn Hills licence centre.\nStaff are pleasant but extremely over-worked, so be patient. Downtown Pittsburgh was less busy during the week than Penn Hills on the weekend."
  },
  {
    "objectID": "posts/2013-10-16-the-circle-a-novel.html",
    "href": "posts/2013-10-16-the-circle-a-novel.html",
    "title": "The Circle, a novel",
    "section": "",
    "text": "The Circle is a novel about the tech/social networking industry, where fictional company the Circle plays the role of Twitter, Facebook and Google combined. The topic is certainly ripe for the satirizing, but I didn’t think Eggers pulled it off very adroitly. Either he was going for the brutal, over the top parody like Jonathan Swift’s _A Modest Proposal, _or he was writing it very quickly and perhaps in anger. The characters felt a little flat and one-dimensional (Mae for example was unbelievably naive) and the conspiracy (or ‘logical extension’ perhaps) was hard to believe—Americans are fiercely proud of being independent and private, so the idea that they would willingly join in with mandatory Circle membership felt off. \nSome points I thought were well made. For example, the skewering of the “campus culture” of tech companies which privileges the already-privileged while the drivers of the busses and local inhabitants are worse off. The technocrats who think technology presents a simple solution to all problems (witness the Gates Foundation and educational testing, for example) are an unfortunate reality today. Parts of the book raised good and thought-provoking questions about understanding social media tradeoffs. Some of the reasons why privacy is important are really poorly outlined (but perhaps the question is wrong on its face—why should I justify my need for privacy?). There really are things that technology has improved dramatically—every time I use Skype with my parents I realize this. Tracking my friends on Facebook is a nice thing to do and brings them more into my life as well. \nI appreciated that aspect of the novel, and it has given me a new perspective on being a ‘user’/‘product’ of social media tools. Overall, this is a topic that cries out for a sustained and subtle approach and I felt The Circle was too facile and, for the Valley cognoscenti who most need it, too easy to dismiss. By the way, there is a certain irony in posting my thoughts about this novel online, but doing so on my own blog feels somewhat different. I hope."
  },
  {
    "objectID": "posts/2015-12-07-requirements-agile-and-finding-errors.html",
    "href": "posts/2015-12-07-requirements-agile-and-finding-errors.html",
    "title": "Requirements, Agile, and Finding Errors",
    "section": "",
    "text": "It’s a long held view in the requirements engineering (RE) community that “if only we could do RE better, software development would be cheaper”. Here ‘doing RE better’ means that your requirements document adheres to some quality standard such as IEEE 830. For example, none of the requirements are ambiguous.\nOne justification is that, based on (very few) studies in the late 80s, requirements errors cost a lot more to fix in test/production than when they are incurred. For instance, if I tell a subcontractor she has a 100 kilobyte message size limit, and I really meant 100 kilobits, fixing that problem after she has delivered the subcomponent will be expensive. This seems obvious. But two problems emerge. 1) Why does she have to wait so long to integrate the subcomponent? 2) how many of these problems are there? Granted it is cheaper to fix that particular error in the requirements/system engineering phase, how much money should we spend to find these errors at that point? [1]\nAn interesting early experiment on this is described in Davis, 1989, “Identification of errors in software requirements through use of automated requirements tools”, Information and Software Technology 31(9) p472–476. In an example of an experiment we see very rarely these days, his team were given sufficient funds to have three automated requirements quality tools applied to a large software requirements specification for the US Army (200,000 pages!). The tools were able to find several hundred errors in the spec, including errors of inconsistency. Yay, the tools worked! But….\nThe program had decided to go ahead and build their (Cobol) system before the automated analysis. The developers on the program didn’t care much about the findings. 80 of the 220 modules were not detectable in the final system (meaning, presumably, they were either merged or omitted altogether). Davis did some post-delivery follow-up, showing that the modules with greater numbers of requirements problems had a significantly greater number of post-release defects. But whether the two are causally related is hard to say (those modules may simply be more complex in general, so both requirements and code are harder to get right).\nWhat I conclude from this is that finding errors of the sort they did, e.g.,\n\nPROBLEM: the referenced table directs that PART_NO be moved from the WORK_ORDER_FILE to the WORK_TASK_FILE. Available fields in the WORK_TASK_FILE include PART_NO_FiELD_PART and PART_NO_FIELD_TASK.\nCHOICE: We assume that PART NO FIELD_TASK is the proper destination.\n\nare ultimately of zero value to document. As a result, finding problems with them, automated or otherwise, is also of no value. Of course we know all this from the past 20 years of the agile movement, but it is interesting to see it in action. I think that (in 1989 certainly) this was excusable, as the program managers had no good sense of what made software special. The level of detail the design describes, down to field names and dependencies, is better suited to the Apollo program, where they prescribe how tightly to turn bolts, label each individual bolt, etc. Which makes sense in a safety critical dynamic environment, but not a lot of sense in an office logistics tool.\n\nGoing Forward\nA term I loathe but seems better than “Future Work”. I’ve worked a lot on automated requirements tools like PSL/PSA or SREM, so where should we head with automated tooling for requirements?\nThere is a lot of empirical evidence that simple, easily integrated process patterns such as requirements goals and scenarios lead to higher quality requirements. Intel, for example, are strong believers in training staff in writing good requirements (although notice their domain is also hardware-oriented and mistakes are costly). Even in agile settings I believe there are big improvements to be gained in writing better user stories (e.g., how to create the “Magic Backlog” described in Rebecca Wirfs-Brock’s EuroPLoP 2015 paper).\nFurthermore, we are seeing more and more use of machine learning to flag requirements problems. For example, at Daimler they have simple detectors for checking requirements. And at Rolls-Royce, based on simple training exercises, they label requirements based on potential risk, combining uncertainty, change impact and cost into an index. All of these types of tools integrate will into a developer analytics approach, able to populate dashboards and flag things unobtrusively (compared with the cost of writing requirements formally).\nLike with any analytics techniques, which ones to apply is situation-specific. Small companies doing the same things in well-understood domains won’t need much, if any requirements analysis. I think there is a lot of room for intelligent augmentation of what makes a good requirement, that facilities conversations and discovery of uncertainty, that automates the repeated and boring tasks (if you cannot possibly avoid creating a 2000 page document …). And in specialized domains, we are moving to a world where more and more of the analysis can be done in models, to verify timing requirements, guarantee that software partitions hold, and so on. Here the line between ‘requirement’ and ‘design solution’ is blurry, because requirements at one level become design solutions at the next level. A mature requirements practice would leverage this to enable experimentation and prototyping in silico, as it were, finding design problems before releasing products or fabricating chips.\n\n\nFinding Defect Leakage\nA major goal for large programs is to reduce defect leakage, the number of bugs that make it to production (to put it more precisely, reduce the number of critical bugs that make it to production). It seems to me there are at least four complementary approaches to this issue:\n\nWe could do this manually, and insist on writing good requirements using checklists, training, inspection, etc.\nWe could use formal methods, on well-formed architectural models, looking for very specific rule violations (safety, security, performance);\nWe could apply machine learning tools on past artifacts and try to leverage experience to predict problems. Not every requirement is equally important (obvious but not always followed).\nWe could design a process that accepted the inevitability of change and made it not only possible, but desirable to change design and requirements in response to new knowledge.\n\nFor the automated tools, I have this quick list of principles. Much like software analytics in general:\n\nDon’t make life worse. Developers should not dread having to do this. That said, an ounce of pain is worth a pound of pleasure.\nWork with existing tools like Doors, Jira and Excel. Your Eclipse plugin does not count.\nDon’t mandate new or complex languages or tools for requirements. We can barely get engineers to write requirements in natural language as it is.\nPrefer lightweight, high value checks over complex, theoretically appealing ones. Socialize people to the value of checking anything before insisting on the complex stuff.\nIntegrate with existing dashboards like Shipshape or SonarQube. These tools have good plugin frameworks and already integrate with many build and CI servers.\nFacilitate conversations and early delivery of results. Remember that requirements engineering is the start of a conversation that gets us to a valuable solution. It is never an end in itself. In very few domains does assuming requirements won’t change get you anywhere.\n\n\n\nAnd Basili and Weiss’s 1981 study on the A7 program’s change requests and requirements suggest a power-law distribution to the most costly (e.g., &gt; 1 person-month of effort) changes.  ↩︎"
  },
  {
    "objectID": "posts/2011-07-27-writing-complex-latex-documents-with-scrivener-2-1-and-multimd-3.html",
    "href": "posts/2011-07-27-writing-complex-latex-documents-with-scrivener-2-1-and-multimd-3.html",
    "title": "Writing Complex Latex Documents with Scrivener 2.1 and MultiMarkDown 3",
    "section": "",
    "text": "I have another post that discusses my approach to writing my thesis using Scrivener. It’s out of date now because I transitioned to MultiMarkdown 3 (MMD3).\nThe Latex support in MMD3 is much simpler than the previous version. Instead of complicated XSLT transforms from the HTML formatted Markdown output, the new approach is to transition from the Markdown directly to Latex. It makes customizing the output much simpler - no more editing XSLT files. In the following, I assume you have a Scrivener document that contains the body of your work (e.g., Introduction, Related Work, Observations, Conclusions). Here’s how to get started:\n\nScrivener still ships with MMD2, so you will need to install MMD3 on your Mac. Fortunately this is straightforward. Go to the download page and download MultiMarkdown-Mac-3.0.1.pkg.zip and MultiMarkdown-Support-Mac-3.0.1.pkg.zip (as of July 2011). The support files will seamlessly integrate MMD3 with Scrivener. I’m not sure how easy it is to revert, however, so be careful  (update: pretty easy - seethis note). I do think it is ultimately easier to work with MMD3.\nNow we need to add custom metadata to Scrivener to add the ancillary files for MMD3. I’ve found the easiest approach to be adding a Meta-Data text document as the first document in your Scrivener project (right-click the top folder in the Binder, Add-&gt; New Text). Now we will tell MMD3 where to find the extra files for our Latex output. Here’s what mine looks like:\n\nBase Header Level: 2 Bibtex: IEEEabrv,../../bibtex/thesis-new Latex footer: ut-thesis-end Bibliostyle: plainnat-nourl Title: My Big Thesis Author: Neil Alexander Ernst Latex input: ut-thesis-begin\nOrder matters here. See Fletcher’s guide on metadata in MMD3.\nBase header level is telling MMD3 that we want to create Chapters for each first-level Scrivener folder (I think Base level 1 is “Part”). Bibtex is the location of the bibtex files, relative to where we will run the “latex” or “pdflatex” commands. Latex footer will be inserted at the end of the last piece of your Scrivener file using the Latex command . There is also Latex header, but as we will see that doesn’t work well. The next command, Bibliostyle, will define the bibliography style for use with Bibtex. Title and Author are obvious, and I finish with Latex input. This is the beginning Latex of my thesis document, including packages, newly defined commands, etc. Now, because MMD3 will turn the metadata entries into variables in Latex, it is important that the input come after the definition of the title and author (otherwise there is an error). This is also why I avoid the use of the Latex header metadata.\nNow it is up to you to define what document class etc. to use for your document: MMD3, nicely, will just stick whatever is in input/footer/header into the appropriate place in the Latex file. There are some nice pre-defined input sections Fletcher Penney created, that you can download as well on the Github site.\nNow in Scrivener, compile your document using the File-&gt;Compile.. and setting “Compile For…” at the bottom to “Multimarkdown-&gt;Latex”. Note that it is important to disable conversion of two hyphens to an en-dash, otherwise HTML comments don’t work, and you cannot escape the Latex properly.\nNote: to escape Latex, surround the Latex (e.g., tables, math) with HTML comments (). The most useful MMD features, for me, are lists, which are just numbers or bullets (see the Markdown syntax guide). Much simpler than the cumbersome \\begin{itemize} syntax.\nTo use citations with MMD3, you can use the [#citename;] or [#citename] syntax for or Natbib commands, respectively. You can also do MMD footnotes with [^foot1] and [^foot1]:Footnote text. I haven’t used any other advanced features of Markdown. The number one wish I have is for easy syntax, but I don’t know how to do it (edit: see the helpful post here). MMD3 automatically creates a after each section heading, based on the section name with no spaces.\nI don’t mind writing raw Latex: emacs+Auctex+refTex makes it pretty painless. But I found, for my 60,000+ word document, that it was much easier to do revision and editing in Scrivener: moving sections around, for example. It also uses Mac native spell-check which is pretty nice (Emacs’s spelling I find clunky and slow).\nMy files for reference:\n\nThesis preamble\nThesis end section\nSubset of MMD output\nSubset of Latex output"
  },
  {
    "objectID": "posts/2015-01-15-the-gap-between-user-requirements-and-software-capabilities-as-technical-debt.html",
    "href": "posts/2015-01-15-the-gap-between-user-requirements-and-software-capabilities-as-technical-debt.html",
    "title": "The Gap Between User Requirements and Software Capabilities as Technical Debt",
    "section": "",
    "text": "One of my favorite graphics is from Al Davis, in 1988. Aside: it is depressing how often we re-invent the wheel in this business.\n\n\n\nAl Davis requirements growth\n\n\nThe nice thing is how one can map various software development concepts to parts of the diagram. I actually think there is another thing you can grab there. Well, two things. One, the environment is not captured in this picture, but only user needs and the specification. In most cases (maybe this is what wasn’t clear in 1988) the user requirements are constrained by the environment, that is itself changing. This is part of our re-definition of the requirements problem of Zave and Jackson.\nTwo, I think you can use this to show how the rate of growth in the gap between needs and system (what Davis calls “inappropriateness”, the shaded area) is also an issue. I think this captures the technical debt problem more succinctly. You will see a growth if, for example, you chose a technology solution that constrains your use of web browser (eg. ActiveX controls mandating IE8). That forces your red line (development/specification/software) to grow slower. Now the question becomes, at what point do you refactor/reengineer so that the rate of adaptability (the slope) increases again?\n(I don’t actually know where I got this – maybe Steve Easterbrook, he likes Comic Sans a LOT – or the original source for this but maybe here.)"
  },
  {
    "objectID": "posts/2017-07-17-7principles-docs.html",
    "href": "posts/2017-07-17-7principles-docs.html",
    "title": "Seven Principles of Effective Documentation",
    "section": "",
    "text": "There has recently been more discussion about software documentation (or perhaps that’s because I only see what I’m interested in… hard to say). At any rate, it seems a lot of discussion inevitably breaks down to “what tool will solve my documentation problems” (e.g., this thread). Others have tried to “fix” UML by proposing new modeling approaches (forgetting, perhaps, that the unified modeling language was spurred by exactly this proliferation of diagram notations).\nI don’t think tools, or formats, or templates, or modeling languages, will ever solve the problem you have. But what will help is to put some people in charge of the project who can think clearly and knowledgeably about what exactly is needed. To that end, the most effective advice (yet perhaps least immediately actionable, as compared to “buy X”) are the principles of effective documentation, originally from the Parnas and Clements paper “A Rational Design Process: How and Why to Fake It” 1. Its more concrete form is published in the SEI text “Documenting Software Architectures”, and is part of the introduction to the course we teach.\nThe other “principle” we mention, but is not part of this list, is “if it isn’t needed, don’t do it”. Documentation (good, up to date documentation certainly) has a cost. Only incur that cost if you are going to realize benefit from it (and naturally, the cost is the upfront cost + maintenance cost).\nI think most of the tooling discussions fall from these principles/rules. For example, Daniel Procida gave a presentation on “4 Elements of Successful Docs”, recommends docs have how-to guides, tutorials, discussions, and reference content. This maps to writing for the reader, and recording rationale.\nIn this perspective, a lot of discussions can be better grounded. For example, “avoid ambiguity” motivates the use of something like UML. The UML is useful at least as the “most common” notation people are aware of (and has many many reference books). Using Markdown to keep things current with your build system can help to keep things current. Confluence or other wikis help with organization and avoiding repetition. And so on.\nAs a good researcher, I should mention this topic greatly interests me. If you want to collaborate, get in touch! I think there’s a lot of room for interesting contributions in making documentation better."
  },
  {
    "objectID": "posts/2017-07-17-7principles-docs.html#footnotes",
    "href": "posts/2017-07-17-7principles-docs.html#footnotes",
    "title": "Seven Principles of Effective Documentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nParnas and Clements, Trans. Software Eng. 12(2), 1986. http://web.engr.oregonstate.edu/~digd/courses/cs361_W15/docs/IEEE86_Parnas_Clement.pdf↩︎"
  },
  {
    "objectID": "posts/2012-04-26-using-github-for-3rd-year-software-engineering.html",
    "href": "posts/2012-04-26-using-github-for-3rd-year-software-engineering.html",
    "title": "Using GitHub for 3rd Year Software Engineering",
    "section": "",
    "text": "This past semester (Winter 2012), I was the instructor for UBC’s CPSC 310: Introduction to Software Engineering. As part of the course, students must complete a large-scale software project in teams of 4–5 in 2 months. This term, I allowed some teams to use GitHub to manage the project."
  },
  {
    "objectID": "posts/2012-04-26-using-github-for-3rd-year-software-engineering.html#reasons",
    "href": "posts/2012-04-26-using-github-for-3rd-year-software-engineering.html#reasons",
    "title": "Using GitHub for 3rd Year Software Engineering",
    "section": "Reasons",
    "text": "Reasons\nAt UBC, we have for some time used IBM’s Rational Team Concert tool, which is free for academic use, as our software collaboration environment. This was the default tool for this term, as well, save for the three groups who applied to use GitHub. The University of Victoria has been using RTC for a similar purpose.\nRTC is, I’m sure, an excellent product for its intended audience, namely, professional software development teams. It is easy to install and maintain for the technical support staff here, has sufficient documentation, and clients for Mac, Linux and Windows. It is also free for us to use as part of the IBM Academic program. However, in course evaluations, it has been the single most complained-about part of the course. It is cumbersome to install for students, and most importantly, always seems to be out-of-date with other software. In our case, RTC 3 is built using Eclipse 3.4, which is “somewhat” incompatible with the libraries and plugins I was looking to use, chiefly the Google Plugin for Eclipse. A significant amount of course time (TA and instructor office hours) was spent getting RTC to work with the other software for the course. And I am just seeing that IBM is now working on RTC 4, which implies more trouble in the future.\nNow, this is partly because students have not had much experience installing commercial development tools on their machines, and that is certainly a learning objective for this course (I am confident it is a pain point for nearly all software developers, new or otherwise). It is also because students run a bewildering array of operating systems and language profiles on their laptops and desktop computers, which makes support a headache.\nThat being said, my sense was that RTC was simply too much tool for what the students needed. As Greg Wilson’s DrProject experiment showed, students simply do not have time, nor inclination, to leverage the more powerful collaboration aspects. Filling out work items, creating documentation, even committing code is something they just do not see the need for. To get them to try it, we must assign marks to those activities. RTC’s terminology (streams, components, etc) is probably great for a developer with multiple projects: for these students (and me!) it is non-standard with most version control concepts and confusing to use.\nSince I’ve personally used GitHub for a while, and git seems to have a lot of developer mindshare, it seemed like a good fit for an experiment."
  },
  {
    "objectID": "posts/2012-04-26-using-github-for-3rd-year-software-engineering.html#experiences",
    "href": "posts/2012-04-26-using-github-for-3rd-year-software-engineering.html#experiences",
    "title": "Using GitHub for 3rd Year Software Engineering",
    "section": "Experiences",
    "text": "Experiences\n\nInstructor\nI emailed Github about the use of their web app for education and received a very prompt affirmative. Github will provide an organization account to the instructor, which includes private repositories for up to 200 people. At the end of the semester, they then require you to either delete the repositories (on Github, obviously not locally) or make them public (free accounts).\nThe Github UI is generally simple, but some of the navigation options are confusing from the team manager point of view (me!). Tracking student performance is pretty easy, since you have ready access to the excellent Github website, including issue tracking, change set tracking, pull requests, etc. Github has excellent graphs that allow the instructor team to check who is doing what. There doesn’t appear to be a good way to email all of the students in the various teams at the same time (we are an “organization” made up of 4–5 person teams).\nGit itself is the main reason to use Github. There are vastly superior tutorials on it, and I like the pure distributed model better than what RTC provides. Finally, and perhaps most important, as the instructor I’ve used Git a fair bit and RTC very little. The disadvantages are that there is no central repository for backup purposes, although being distributed this is presumably less important.\n\n\nStudent\nStudents were generally positive. The alternative, in this course, was to use RTC. Git has the advantage of more widespread adoption (you’re unlikely to use RTC at Microsoft, but MSFT supports Git in various places). And of course, if Github goes down, then students can no longer manage issues. Git itself is complex to learn; I should have provided a short tutorial to those teams on the basics of Git.\nThey also found tools like SourceTree and Tortoise invaluable in understanding what was happening with branches and remotes. For a while, a few teams had multiple, non-merged and conflicting branches for each member, which they could resolve once they saw visually how the branches were happening. The concept of remote repositories and pull requests is a little alien at first.\nIssue tracking in Github is primitive relative to RTC. This is a strength for this course, I feel, but when we go from user stories to tasks it means students had to roll their own classification scheme (e.g., define a product backlog item, then the tasks which compose it).\nThe teams which used Github were much stronger than most other teams in the course, so the results are no doubt skewed. That being said, I don’t think RTC was any simpler to use—in a number of teams, at least one team member never managed to commit code to the shared repository."
  },
  {
    "objectID": "posts/2012-04-26-using-github-for-3rd-year-software-engineering.html#looking-ahead",
    "href": "posts/2012-04-26-using-github-for-3rd-year-software-engineering.html#looking-ahead",
    "title": "Using GitHub for 3rd Year Software Engineering",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe obvious question is, “Would you use Github again”? The answer is yes, and perhaps even “I would like to make everyone use it.”\nIt was confusing to have two separate tools in the course. Partly, this is because marking is complicated by the fact student teams are in tutorial sections, so some teams in a given tutorial were using Github and some RTC. This meant TAs had to mark both tools (and learn both). Exam questions are more complicated, since you must account for some students never having used RTC, if you ask about issue tracking.\nI like the fact that the project collaboration tool was separate from the IDE. I think RTC’s tight Eclipse integration makes it difficult to install the IDE necessary. Some students ran Eclipse 3.7 in conjunction with RTC (Eclipse 3.4) in order to get plugins working. Since git is so popular, it is much easier to find tool support for it than to munge RTC into your work flow. In future, tools like Mylyn would be useful to better integrate issue tracking into the IDE.\nThe big outstanding issue is privacy. In BC, the provincial government is considering laws that prohibit (or seem to, IANAL) student data being anywhere near a US server (despite students happily sending email about their marks from Hotmail or Gmail). While I respect that motivation, I feel there should be some way to give consent, particularly when so many excellent tools are US-based."
  },
  {
    "objectID": "posts/2017-05-11-visual-abs.html",
    "href": "posts/2017-05-11-visual-abs.html",
    "title": "Visual Abstract attempt",
    "section": "",
    "text": "In response to Greg Wilson’s challenge, I did a quick attempt at a Visual Abstract for a recent paper.\n\n\n\nVisual Abstract: Identifying Design Rules in Static Analysis Tools. Evaluated 464 rules, 19% design related, 67% easy to classify.\n\n\nI think it turned out ok; it captures the core findings and presumably will prompt people to look at the paper. I’ve attached the Keynote slide I used in a repo on Github.\nA few comments:\n\nGraphic design is a skill you need to work on (duh). Even with this template, I don’t think it is super compelling. I just used out of the box icons.\nThe footer kinda loses relevance when you don’t have a big name JOURNAL behind you. Something else can go there. I used the conference logo, but maybe some logo for your lab.\nI wanted a URL to point to the full paper, but a bar code might be better. If anyone uses that anymore.\nFor qualitative papers, and maybe software engineering in general, the “outcome data” at the bottom is more difficult to come up with. I don’t know if I can easily pull three nuggets of improvement for each paper (but I hear Greg’s baritone susurration saying “well yes, that’s part of the problem”)\nAs someone on Twitter pointed out, these are intrinsically visual and thus not accessible to the visually impaired. I do think they help the “academicese-impaired”, but each time one of these is used, I would hope a non-visual summary is also presented. Pulling the text together shouldn’t be too hard. I’ve had a go in the “alt” text above.\nDoing a whole batch of these (say for an ICSE track) would be a fair bit of work. Presumably you could pick papers you’ve had to read anyway (and cared about). But summarizing the contributions is not so simple (for me, anyway). Again, perhaps that points to a wider problem.\n\nI’ve noticed more and more papers calling out contributions in special boxes, and bulleted lists in the introduction. I think this is great. One of my pet peeves is a reviewer who points out some trivial English error, but tolerates the total incoherence of the introduction.\nRelated to this is a visual portrayal of the methodology. This happens a lot in medicine, where lots of experiments are conducted and explaining complex cross-over designs is important. But you can see a similar example in Borg et al. 2017, below:\n\n\n\nSample workflow cartoon\n\n\nThis explains how the study was conducted. Again, anything that can explain what is going on for a busy reviewer is helpful. Remember, in 2017 FSE reviewers seem to have reviewed 25-28 papers each. Expecting them to spend more than 30-45 minutes on each one is unrealistic. So make your time with them count!"
  },
  {
    "objectID": "posts/2019-05-17-job-search-canada.html",
    "href": "posts/2019-05-17-job-search-canada.html",
    "title": "Academic Job Searches—A Canadian Perspective",
    "section": "",
    "text": "Academic job interview season is wrapping up, so I thought I’d capture the process from the Canada point of view.\nAcademic CS jobs in Canada follow mostly the same pattern and process as the US (here I am talking about research-focused, tenure-track roles). Hence I think most of the advice from Philip Guo, and Wes Weimer and his academic offspring, are totally applicable (and indeed, were what I relied on in my search). There are a few subtleties I think are useful for applicants to know. Disclaimer: I am relatively junior, and only have limited experience applying in Canada, so these insights are based on my limited sample and from talking to colleagues here. I am also not a legal or immigration expert, so I make no warranty about this advice.\n\nUnderstand why you are interested in Canada\nIn the Weimer/Le Goues job seeker’s guide, they make the point that US candidates tend not to move to Canada. I think it is safe to say that if you have spent your entire master’s/PhD in the States, you have worked in the NSF/DARPA/DOD model of funding, and have family ties there, then the switch to Canada would be a big change. I think this is especially true for smaller schools like ours. We’re a small place but proud (both Canada and Victoria!). So make it clear in your cover letter or interview why you would come. Hopefully reading this guide will help!\n\n\nExplain/communicate the proposed Discovery grant you would win\nIn Canada funding is fairly different than the US. For one thing, Canadian schools have substantially lower tuition for grad students (although that is changing). Faculty research budgets have much lower student stipends as a result, and the grant sizes reflect that. A moderate US grant might be 100k/year; in Canada that would be equivalent to 20k, but support the same research program.\nYour application and interviews should demonstrate you understand this. I suggest reading up at the NSERC page, and also the research services page for the university you apply to.\nThe main grant for new faculty is the Discovery grant. It is a five year grant worth from $20k-50k a year. You are evaluated equally on your ability/experience with highly qualified personnel; your personal ability as a researcher (i.e. CV); and the research proposal. Not holding a Discovery grant is a problem because getting other federal funding depends on this, to some extent. The good news, especially for ECR, is that success rates are relatively high (60-75%). You can expect to prepare this the summer you get hired, for submission by Nov 1.\nYour job talk and your research statement should outline some elements of the five page grant proposal you would write. Departments want to see what you would propose, and how able you are to communicate your vision to external readers. It is a 5 year program, so scope your “future work” to that time frame.\nI think this is broader advice than Canada-only, but one thing I’ve noticed is that applicants who are just finishing a PhD give more narrowly focused talks. Two things to keep in mind if this describes you. 1. You will be competing with people who have 2-4 years of post-doctoral training, and a corresponding breadth of research, more experience training students. Stretch your talk to show how you have the potential to succeed like that. Conversely, one question about post-docs is often “how independent can they be”. This is particularly true if you come from a big lab with a famous PI. 2. Think like a professor. What grant areas will you target? How would you manage 5 masters/phd students? How will you balance teaching load with research? I don’t think you need to feel uncompetitive: we invited you for a reason. But the onsite interview is when we want to see if you are ready for what can be is a very demanding job.\n\n\nFunding\nEngagement with industry The federal government has been a big supporter of industry partnerships recently, although the programs were recently overhauled. This typically means that if you have an industry partner with skin in the game, i.e. financial assistance, you have an excellent chance of obtaining government matching funds. Conversely, if you prefer pure research with no immediate outcomes, finding funds might be more difficult. There are very few large granting agencies. There is no equivalent to DARPA/IARPA, DHS, DOD, DOE funding in Canada; those projects would work with specific people at specific agencies to secure one-off funding. In BC, nearly all grants would come via NSERC programs, or MITACS matching. There are also Networks Centres of Excellence such as MEOPAR that allocate funds in targeted areas (these are being phased out). There is also a recent Defence initiative, IDEaS, to increase Canadian funding for research with defence applications. Finally, there were industry-led superclusters announced, but who/what gets funding is still very unclear. It seems to focus mostly on subsidies for industry-led research.\nIn general, I would say finding funding is much more individualized and distributed than in the States. There are plenty of places to find adequate funding (again, a student probably only costs 20-25k a year), but how to get it is much less clear than a DARPA BAA program. A cynic might say this is because funding announcements are more closely tied to electioneering.\nSummer students and internships. We have a similar program to the REU approach, called USRAs. These are government matching for student research semesters. Again, these are allocated on a per-institution basis (bigger places get more).\nWe have an excellent grid/HPC/cloud computing infrastructure, ComputeCanada. They conduct yearly resource allocation competitions. I don’t know what the success rates are.\nFor large infrastructure, e.g., robots, 3d printers, tabletop displays, quantum computers, the Canadian Foundation for Innovation holds annual competitions for this, but success rates are fairly low.\n\n\nTenure\nWell, more to come from me on this one, but my general sense is that tenure is more collaborative and mentoring than many US places. I don’t think there is the equivalent of “didn’t get the NSF grant, didn’t get tenure”, or “didn’t get 1 million in funding, didn’t get tenure”. That said, standards are just as high as US Tier 1 schools; we just want to help you achieve them. We’re friendly, eh?\n\n\nSpecialty hiring\nCRCs. You may be in the enviable position of applying for a Canada Research Chair. These are a nationwide funding mechanism for research positions. We have Tier 1 (7 years, renewable, senior) and Tier 2 (5 years, renewable, junior/emerging). They typically come with higher salary and teaching relief. Each university gets a quota from the federal government. The approval process is a bit more involved. In addition to approval from (department-faculty/dean-VP academic/provost), you will have your application submitted to the federal government, wherein the case will be made that you are uniquely qualified, amazing, etc. This is almost never turned down, from what I can tell, but could be. In particular, the federal government has a strong desire to see equal allocations of these CRCs to male and female candidates.\n\n\nRequirement for hiring Canadians\nDepartments are usually required to prefer Canadians over non-Canadians, for immigration purposes. This means that of two totally equivalent candidates, the Canadian citizen or permanent resident would be made an offer. If you are a PR/citizen, or applying for PR, that is worth highlighting somewhere.\n\n\nImmigration is easy\nI can’t speak from experience, but my understanding is that immigration to Canada as a permanent resident, and eventual citizenship, is much easier than the US process (with which I do have experience). This is also true for immediate family (spouse/children). In some cases, permanent residency is possible in months, not years.\n\n\nSalary and benefits\nIn general Canada pays less salary. Keep in mind that it is a 1 year salary, not 9 months. Most Canadian schools don’t have the concept of a summer salary. At UVic, we operate on 3 equal semesters, and allocate a research semester where you would like (subject to teaching needs of course).\nThe CRA survey has more useful information. Health care is provincially funded from your taxes, so don’t expect to lose 500-600$ a month to health premiums. From working in the US, even being a well-paid employee at a great employer, there was a significant cost (mental and financial) in understanding yearly plan changes—even without chronic conditions.\nIn most places, faculty are unionized or quasi-unionized. This means you fall into a grid, and your salary increases will be based on a formula in the collective agreement. You can probably look this up online for each institution you visit. Hint: you want to move up the grid as much as possible before you start the job. So Prof. Le Goues’s advice on startup over salary might change, since your salary will be the baseline for future percentage increases.\n\n\nSummary\nI would sum up by saying Canada is an awesome place to do research, and I hope you apply to Canadian universities! Especially mine!\n\n\nResources\n\nMaclean’s Guide to Canadian Universities: This is the Canadian equivalent (in all respects, good and bad) to US News and World Report. It divides universities into medical/doctoral, comprehensive, and primarily undergrad. Canada does not have the same diversity of higher education as the US—for example, there are few private institutions here. The main division for research is whether the school has a medical school or not, as med schools are tightly controlled (public health care dictates number of seats), and med schools tend to accumulate massive amounts of research funding. My school is categorized as a comprehensive, but I wouldn’t say this equates to “more teaching”.\nNSERC: The main engineering funding body, similar to NSF.\nTaulbee Survey: Various stats on academic CS jobs, including some from Canada."
  },
  {
    "objectID": "posts/2014-07-24-cults-of-personality-and-software-process-improvement-2.html",
    "href": "posts/2014-07-24-cults-of-personality-and-software-process-improvement-2.html",
    "title": "Cults of Personality and Software Process Improvement",
    "section": "",
    "text": "I’m a fan of the Cynefin framework. I find it a great tool for understanding what type of problem you are trying to solve. The notion of complex/complicated/simple is quite helpful. You could do worse then to read Dave Snowden’s blog, as he explores each of the domains in the context (most often) of software projects.\nRecently Mr Snowden has been critiquing the Scaled Agile Framework (SAFe) put together by Dean Leffingwell. This attack on SAFe is not unprecedented. It’s hard to take attacks like this too seriously when their proponents don’t put forth data, but merely theory.\nOne of the most difficult parts of doing research in Software Engineering is its inherently uncontrollable, one-off nature. Sure, in some cases—like websites for restaurants, for example—we see repeatability. But the most interesting projects, the ones SAFe is applied to, the complex or perhaps complicated ones, there is no repeatability (by definition). This makes it impossible to say with any degree of accuracy what factors are contributing to the success or failure of the project.1\nIn particular, when you have strong, intelligent, experienced consultants like Mr Snowden, or Mr Leffingwell, or various other graybeards, I don’t think you can control for the ‘personality’ factor. That is, what portion of the success of the initiative (say, applying Sensemaker or SAFe or Scrumban or what have you) is due to the tool/process improvement/methodology, and what portion is due to the smart person effect of the consultant? This is made more difficult when that consultant has a very strong economic incentive to point to the methodology as the distinction, since their business is inextricably tied together with that methodology. Furthermore, just the fact that a company has reached out for help indicates some level of self-awareness.\nMy feeling is that given a successful team, led by an enlightened manager, it wouldn’t matter what methodology they used (which I mentioned previously in the context of tools). And there is some evidence to support this: Capers Jones suggests RUP and TSP have higher quality than Scrum or other approaches. Now that is just one dataset, but it is exactly one more than Mr Snowden has produced, as far as I can tell (the plural of anecdote is not data).\nDoes all this mean it doesn’t matter if we choose RUP, SAFe, Scrum, Kanban, Six Sigma, or Sensemaker? To some extent, I think that is true. I would guess that your measurable outcomes after implementing TSP would be similar to the outcomes after implementing SAFe. But the point is, one cannot measure these things in isolation! You will never know (Heisenberg-like) whether something else would have been better. The local project context is so important that the principles are more important than the specific practices (e.g., the agile manifesto, Cynefin domains, good organizational practices, etc)."
  },
  {
    "objectID": "posts/2014-07-24-cults-of-personality-and-software-process-improvement-2.html#footnotes",
    "href": "posts/2014-07-24-cults-of-personality-and-software-process-improvement-2.html#footnotes",
    "title": "Cults of Personality and Software Process Improvement",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith one exception I am aware of: this paper from Simula in Norway. They paid 4 different companies to develop to the same set of requirements in order to understand the maintainability characteristics of different approaches. But even there, the results are difficult to generalize. Anda, B.C.D.; Sjoberg, D.; Mockus, A, “Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System,” IEEE Transactions on Software Engineering, vol.35, no.3, pp.407,429, May-June 2009 doi: 10.1109/TSE.2008.89 ↩︎↩︎"
  },
  {
    "objectID": "present.html",
    "href": "present.html",
    "title": "Presentations",
    "section": "",
    "text": "Most of my talks can be found at Slideshare or more recently, SpeakerDeck.\nRecordings:\n\nTechnical Debt (TD): Tools + Techniques for Identifying, Fixing, and Managing TD in Research S/W”. A talk at the Canada Research Software conference (CANARIE). July 2021.\nMatrix talk from October, 2018: When Writing it Down is Not Enough: the Era of Computational Notebooks\nMatt’s talk on VAEs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neil Ernst",
    "section": "",
    "text": "Openings 2024 Spring: I am recruiting a post-doc to begin ASAP, and Phd/Masters students.\nI am associate professor of Computer Science at the University of Victoria.\nYou can learn more about my research. Download my short bio.\nI teach courses in software engineering (mostly) for our Software Engineering and Computer Science programs.\nI serve in various roles as workshop and conference organizer, program committees, student supervision, journal reviews, etc. Among others:\n\nSenior Associate editor, Journal of Systems and Software\nregistered reports and open science advocate\nprogram committees - among others, at various times, ICSE, RE, XP, CAISE, ER, MODELS\nAssociate Editor, Journal of Empirical Software Engineering \n\n \n  \n   \n  \n    \n     Mastodon\n  \n  \n    \n     Github"
  },
  {
    "objectID": "index.html#consulting",
    "href": "index.html#consulting",
    "title": "Neil Ernst",
    "section": "Consulting",
    "text": "Consulting\nI occasionally act as speaker, consultant and expert for matters relating to software architecture and software requirements, including architecture analysis, modern software development practices, and analytics for software engineering."
  },
  {
    "objectID": "index.html#previously",
    "href": "index.html#previously",
    "title": "Neil Ernst",
    "section": "Previously",
    "text": "Previously\n\n\nsenior member of the technical staff at the Software Engineering Institute at Carnegie Mellon University.\npostdoc in software engineering at the University of British Columbia.\nPhD student in Computer Science at the University of Toronto.\nmaster’s student in Computer Science at the University of Victoria.\nundergraduate research in geography and GIS at the University of Victoria, including work placements with the BC Ministry of the Environment."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Neil Ernst",
    "section": "Contact",
    "text": "Contact\nPhone: c’mon man email: nernst@uvic.ca  Fax: 250-472-5708 Postal Mail: Department of Computer Science University of Victoria Engineering & Computer Science Building (ECS), Room 504 PO Box 1700 STN CSC Victoria BC V8W 2Y2 Canada"
  },
  {
    "objectID": "prospective.html",
    "href": "prospective.html",
    "title": "Prospective Students",
    "section": "",
    "text": "Do you want to do your Master’s or PhD with me?1\nYou will first need to apply to and be accepted by the University of Victoria. If you are applying for an NSERC or equivalent scholarship/fellowship, please feel free to contact me first.\nYour interests and skills should have some (a lot) of overlap with what I do. For more on that, see my research page, or my blog, or Google Scholar. In general, if you care about software, putting software together, making sure software does what we want, we can probably sort something out.\nWhat will you get out of it? You will be trained in how to conduct research, how to write and communicate your ideas to others, including industry practitioners; you will write software; you will help push forward the boundaries of knowledge in software research. It’s an exciting time to be in research!\nEmail me:\nFor potential MSc students: If you acquired research experience during your undergraduate degree, please tell me about it. Publications in respected conferences or international journals, an undergrad thesis, a Capstone project, an undergrad research assistant position or a directed studies project make you a stronger candidate.\nFor potential PhD students: Tell me about your previous graduate work and include PDFs of your thesis and your best and most recent publications. PhD applicants should have a strong background in computer science or software engineering research (or a related field).\nThank you for your interest in working with me. I receive many emails from prospective students and providing me with all of the requested information will help your email stand out!"
  },
  {
    "objectID": "prospective.html#footnotes",
    "href": "prospective.html#footnotes",
    "title": "Prospective Students",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCribbed largely from the CHISEL group page↩︎"
  }
]